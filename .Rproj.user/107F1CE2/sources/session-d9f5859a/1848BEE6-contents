---
title: "tutorial_[predictions]"
author: "Patrick Kelley"
date:
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

# Introduction
In this tutorial, we will demonstrate some basic model prediction from a Generalized Linear Mixed Model (GLMM), implemented using the ```glmmTMB``` package. An important note here: Many reviewers, committee members, general yahoos, and the like will look at model predictions from a GLMM (or GAMM) and then suggest --or demand-- that you plot the model predictions along with the raw data. Oh boy. Here's mty take. Plotting the raw data (as simple points on a plot) gives the impression that the points are independent (and that the number of points equals the sample size). Obviously, this is not the case for a mixed-effects model, as the effective sample size (discussed in a previous lecture) is some value between the number of groups in the random effect and the number of rows in your data. So, don't fall into that trap! 

That aside, the goal of ths brief tutorial is to teach you a couple of ways of plotting your marginal predictions from mixed-effects models (GLMMs and GAMMs). The approach described here assumes that specific levels of the random effects are not important. Instead, you wish to average across the random effect levels (and thereby generalize to the population as a whole) when examining the singular impacts of specific fixed effects. Typically, this will involve setting all but one fixed effect to the reference level (for categorial terms) and/or the average value for continuous terms. For the focal variable, you predict the response variables for each of its values (from minimum to maximum value). This will give you the data needed for a simple bivariate graph.

## Workspace set-up

```{r}
source("class_r_scripts/load_packages.r")

```

## Load the data
As a reminder of what you did in the tutorial_[glmms_part01], this dataset is on a new species of jackalope. From that tutorial: During a survey of 31 forested study plots (1 square kilometer each) here in Wyoming, scientists discovered a cryptic species of jackalope. Because it is endemic to Wyoming, they named it the Wyoming Jackalope (*Lepusantilocapra wyomingensis*). The researchers quantified their abundance (counts per survey and multiple surveys per plot), community diversity (all *Lepusantilocapra* species in the plots), and also measured several habitat features, including percentage ground vegetation, percentage herb layer, and percent leaf litter. Anyway, these scientists thought the scientific community would not believe their discovery, so they gave the dataset to us. Let's read in these data and save it to an object called "wyoming" and then look at the structure of the data:

```{r}
wyoming <- import("class_data/jackalope_wyomingensis_dataset.csv")
str(wyoming)
```
As you recall, you see several variables and plot information:

* ```plot_id```: values 1-31 for each unique plot
* ```forest_age```: categorical variable describing the age, in years, of each plot
* ```tree_species```: species of dominant tree in each survey
* ```x``` and ```y```: spatial coordinates in UTM (specific projection not important) of each measured survey
* ```abundance```: of *Lepusantilocapra wyomingensis* in each survey
* ```shannon_hlog10```: Shannon diversity index of the *Lepusantilocapra* community in each survey
* ```ground_veg```: Proportion of each survey area covered in ground vegetation
* ```herb_layer```: Proportion of each survey area covered in herbs
* ```leaf_litter```: Proportion of each survey area covered in leaf litter only (no vegetation)
* ```rain_in```: round total precipitation (in inches...because that's what their inexpensive rain gauge collected)

We ran some code to clean the data and then used AICc model selection to identify a best-fit model. Let's run that again here:

```{r}
wyoming <- wyoming |>
  mutate(plot_id = as.factor(plot_id))
mod <- glmmTMB(abundance ~ ground_veg*leaf_litter + herb_layer + rain_in + (1 | plot_id), family=poisson, data=wyoming)
```
From here, we can begin to draw inference and begin to make some predictions from our model. The main results show that the best-supported hypothesis is one where *L. wyomingensis* abundance is impacted by all vegetation variables in various additive and interactive ways. We can get a general idea of what the impacts are from the direction (positive or negative) of the effects, but it's really hard to visualize what these would be without plotting them. 
# How to predict from a GLMM (implemented in the ```glmmTMB``` package)

Let's review the basic definition of marginal and conditional predictions, as related to mixed-effects models:

* ```Marginal predictions```: These represent the expected response when averaging out the random effects. They reflect the population-level effects of the predictors.
* ```Conditional predictions```: These represent the expected response conditioned on specific values of random effects or covariates. They reflect both fixed effects and the influence of random effects.

In most cases, you will want to examine the marginal predictions (i.e. not conditioning the impacts of your fixed terms on specific groups of the random effects). Let's first approach this the long-hand way, so that you can get a sense for what's going on. This also might give you a bit of flexibility if you ever need to convince yourself that some out-of-the-box R solution is working correctly. (You can sense my general pessimism here.) After this example, we will use the ```ggeffects``` package as a short-hand way of achieving the same goal. But, please, always make sure that these solutions are working for you and not against you.

In this example, we want to plot ```abundance``` as a function of ```ground_veg```. And, of course, we want to visualize the effect that comes from our model. First, we create a sequence going from minimum to maximum ```ground_veg``` and make sure there are tiny increments between the values (n=100 in this case); this will ensure that your plot shows a smooth and not jagged line.

```{r}
ground_veg_seq <- seq(min(wyoming$ground_veg), max(wyoming$ground_veg), length.out = 100)
```
We then create a data.frame from this sequence, but we set all of the other variables to their mean values. Here, it is important to include the random effect (```plot_id```) and set it to "NA" so that the random effect is effectively averaged out. 

```{r}
newdata <- data.frame(
  ground_veg = ground_veg_seq,
  leaf_litter = mean(wyoming$leaf_litter), # Set to mean
  herb_layer = mean(wyoming$herb_layer),  # Set to mean
  rain_in = mean(wyoming$rain_in), # Set to mean
  plot_id = NA # Random effect averaged out
)
```

Then, we are ready to send this prediction data.frame into the ```predict()``` function. We need to extract confidence intervals that are asymmetric, so we first must make predictions on the model's link scale and also extract the standard errors of the estimates (also on the link scale). Achieve this by setting ```type=link```.


```{r}
predictions <- predict(mod, newdata = newdata, type = "link", se.fit = TRUE)
predictions 
```

Next, we need to backtransform the central value (average value on the link scale) to the original response scale. And we need to do the same for the confidence intervals. Recall that the confidence intervals are +/- 1.96x the standard error. And, since your link function was a "log", you need to exponentiate your values.

```{r}
# Step 3: Backtransform predictions and calculate confidence intervals
newdata$fit <- exp(predictions$fit)  # Backtransform to response scale
newdata$lower <- exp(predictions$fit - 1.96 * predictions$se.fit)  # Lower CI
newdata$upper <- exp(predictions$fit + 1.96 * predictions$se.fit)  # Upper CI
newdata # view the predicted output
```

You are now ready to plot these model-derived predictions.

```{r}
# Step 4: Plot the predictions using ggplot2
ggplot(newdata, aes(x = ground_veg, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, fill = "blue") +
  labs(
    title = "Marginal Effect of Ground Vegetation on Abundance",
    x = "Ground Vegetation",
    y = "Predicted Abundance"
  ) +
  theme_minimal()
```

For a published work, you would specify in the figure caption how the prediction was made (i.e. setting the other variables to their mean, etc.). You can experiment with other focal variables too. For interactions, consider predicting for three values (low, medium, and high) for one of the interacting terms, and then using the complete range for the other. Note that you will need to use the ```expand.grid``` function within the ```data.frame``` function to produce the prediction data.frame.

## Model predictions using "ggeffects": the easier way!

Now that you have done this the long-hand way, let's streamline everything. Using the relatively new ```ggeffects``` package, you can create marginal predictions using ```type="fixed"``` and specifying the focal variable that you want as your x-axis term. 

```{r}
predictions <- ggpredict(mod, terms = "ground_veg", type = "fixed")  # Marginal predictions
```

That's it. Now plot the predictions...

```{r}
# Plot the predictions
plot(predictions) +
  ggplot2::labs(
    title = "Effect of Ground Vegetation on Abundance",
    x = "Ground Vegetation",
    y = "Predicted Abundance (Marginal)"
  ) +
  ggplot2::theme_minimal()
```

Yes, that was easier by many lines of code. And, convincingly, this produces the same plot as the long-hand (and laborious) method we first ran.

# Predicting from a simple GAM

Now, let's go back to another familiar jackalope dataset that we worked on in the tutorial_[simple_predictions] file. For this example, we will not recalculate and factor in a spatial autocovariate (even though we know it helps our model).

## Import and process data
```{r}
loco <- read_csv("class_data/jackalope_chromoloco_dataset_02.csv")
loco <- as_tibble(loco)
```

```{r}
mod <- gam(loc3 ~ sex + s(pc1, k=4) + s(pc2, k=4) + s(sample_age, k=4), family="betar", data = loco)
summary(mod)
```

Again, let's do this the hard way first!

```{r}
# Create a sequence of values for pc1 (range of interest)
pc1_seq <- seq(min(loco$pc1), max(loco$pc1), length.out = 100) 

# Create a new dataset for predictions, holding other variables constant
newdata <- data.frame(
  pc1 = pc1_seq,
  pc2 = mean(loco$pc2),  # Replace with desired value or reference level
  sample_age = mean(loco$sample_age),
  sex = "female"
  )

loco$sex <- factor(loco$sex)
```

And then make your predictions, backtransform, and plot!

```{r}
# Predict on the link scale
predictions <- predict(mod, newdata = newdata, type = "link", se.fit = TRUE)

# Backtransform predictions and compute confidence intervals
newdata$fit <- plogis(predictions$fit)  # Backtransform to response scale
newdata$lower <- plogis(predictions$fit - 1.96 * predictions$se.fit)  # Lower CI
newdata$upper <- plogis(predictions$fit + 1.96 * predictions$se.fit)  # Upper CI

# Plot the conditional effect
ggplot(newdata, aes(x = pc1, y = fit)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  labs(
    title = "Conditional Effect of PC1 on LOC3",
    x = "PC1",
    y = "LOC3 (Response Scale)"
  ) +
  theme_minimal()
```

Now, if you've picked up anything in this course, you'll immediately be skeptical if the confidence intervals are symmetrical or not. They are indeed asymmetrical (and therefore correct). 

Now, let's try the easy way!

```{r}
predictions <- ggpredict(mod, terms = "pc1", type = "fixed")

plot(predictions) +
  ggplot2::labs(
    title = "Effect of PC1 on Loc3",
    x = "PC1",
    y = "Predicted Loc3 (Marginal)"
  ) +
  ggplot2::theme_minimal()
```

Yes, that's annoying simple. 

THE END