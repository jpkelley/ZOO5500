[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "",
    "text": "Essential Course Information",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Information</span>"
    ]
  },
  {
    "objectID": "index.html#essential-course-information",
    "href": "index.html#essential-course-information",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "",
    "text": "Instructor\nPatrick Kelley\n\n\n\n\nEmail\njkelle24@uwyo.edu\n\n\nSchedule\nTu / Th, 11:00–12:15 MT\n\n\nDates\n20-Jan-2026 to 08-May-2026\n\n\nMeeting Type\nsynchronous remote\n\n\nLocation\nJoin via Zoom\n\n\nCommunication\nSlack (private link sent to your email)",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Information</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html",
    "href": "draft_sections/syllabus.html",
    "title": "2  Syllabus",
    "section": "",
    "text": "2.1 Why this course?\nThe benefit of this course is to not only teach you about statistics (and some statistical traps), but to have you working with your peers as you delve into the issues that inevitably arise with your own data analysis. Learning how to navigate data analysis issues is a skill that is critical for developing researchers. Please try to make this class as useful as possible for you and include most or all of the analyses that you expect will go into one of your dissertation chapters. You can just include one analysis if that is all that you need or you can include 4-5 analyses if that is what you need to answer your question(s). This is an opportunity for you to get more help with data analysis, so take advantage.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#course-objectives",
    "href": "draft_sections/syllabus.html#course-objectives",
    "title": "2  Syllabus",
    "section": "2.2 Course Objectives",
    "text": "2.2 Course Objectives\nBy the end of the course you should:\n\nFeel comfortable using the statistics modeling approaches taught in course and know how to approach analyzing your own data\nFeel comfortable writing methods and results section in a manuscript\nMake significant progress with some element of your own analysis, with the recognition that some people’s analyses are more or less simple than others’.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#office-hours",
    "href": "draft_sections/syllabus.html#office-hours",
    "title": "2  Syllabus",
    "section": "2.3 Office hours",
    "text": "2.3 Office hours\nDue to the nature of the course, I will not have specific office hours for the course. I have extended class times (after our weekly meetings) to allow more time to ask questions and for one-on-one work. These are optional and may be canceled in some weeks depending on your interest in meeting.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#location-and-meeting-times",
    "href": "draft_sections/syllabus.html#location-and-meeting-times",
    "title": "2  Syllabus",
    "section": "2.4 Location and meeting times",
    "text": "2.4 Location and meeting times\nTuesday/Thursday on Zoom; 11:00AM - 12:15PM (Mountain Time) Spring term 2026",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#prerequisites",
    "href": "draft_sections/syllabus.html#prerequisites",
    "title": "2  Syllabus",
    "section": "2.5 Prerequisites",
    "text": "2.5 Prerequisites\nTo ensure your success in this course, the following are required: - You must have a dataset to be analyzed this semester. This is very important. This class will only cover data reformatting; we will not cover data processing (except as necessary in specific cases). - Your data analysis must not be used in another (past or present). - You must be at least in your second year of graduate school. - You must have some exposure to using the Program R (tidyverse preferred, but base R is nice too). - You must have taken a statistics course in the last five years.\nAssessment and Grading Standards This course is graded as Pass or Fail. To pass the course you need to do the following: ● Participate at least 80% of the class meetings. Simply inform me of your absences (for health reasons, field research, etc.), and then do what you can to catch up with the work. ● Turn in all assignments on their due dates (see Course Outline below). This is critical to your success and the flow of the course.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#attendanceparticipation-policy",
    "href": "draft_sections/syllabus.html#attendanceparticipation-policy",
    "title": "2  Syllabus",
    "section": "2.6 Attendance/Participation Policy:",
    "text": "2.6 Attendance/Participation Policy:\nThis is a graduate level course, and you are here for your own benefit. That being said, I expect you to come to class, stay engaged with the material, and not only learn how to do your own analysis but understand other types of data and analyses by working with your group members. If you do this, you should have analyzed your own data by the end of the semester and have part of a manuscript completed. Please email me ahead of time when you will be unable to attend class for a valid reason.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "href": "draft_sections/syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "title": "2  Syllabus",
    "section": "2.7 How to succeed in the course (beyond your wildest and funniest dreams)",
    "text": "2.7 How to succeed in the course (beyond your wildest and funniest dreams)\nGraduate school can be considerably challenging, as everyone is attempting to juggle research, teaching, classes, health, and family, all while coping with unexpected stressors. Course information is flying at you from every direction; there are many specific terms and concepts that you need to learn and operationalize. So, here are some reminders for you (even though I know you don’t need these):\n\nAsk questions! Even though there are no exams, take copious notes and work collaboratively to build course notes.\nDon’t be afraid to redirect the flow of the course. I am here for you. I want to give you time to think about and discuss the material. This is supposed to be fun (while simultaneously transforming you into analytical gurus)! So, just talk to me about how I can help!\nRead all the material in this Course Guidebook. Many online courses require much more reading; this one does not.\nShow up to as many of the synchronous (Zoom) discussions as you can. When we meet together online, our goal will be to solidify everyone’s understanding of different concepts and how they are linked. These concepts will be useful as you navigate your own analysis project.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#expectations",
    "href": "draft_sections/syllabus.html#expectations",
    "title": "2  Syllabus",
    "section": "2.8 Expectations",
    "text": "2.8 Expectations\nAs your instructor, you should expect me to:\n\nTry my very hardest to make the course go smoothly (the reason you now have this nice new online course guide); but please be prepared for the inevitable hiccups.\nRespond to questions within 24 hours during the work week. However, I likely will not respond during the weekend (unless there is an urgent matter).\nRespect you not only as a learner but as a colleague.\nUnderstand that these are strange and sometimes unforgiving times. We all have varying levels of tolerance and resistance to stress. If you are having a hard time for whatever reason, please communicate with me. I not good at judging, but I can do a hell of a job listening and working with you to solve a problem.\n\nAs a student, you are expected to:\n\nBe respectful of everyone in the class, including me.\nAsk for help if needed.\nTreat your presence in the classroom and your enrollment in this course as you would a job; Act professionally, arrive on time, pay attention, complete your work in a timely and professional manner, and treat your learning seriously.\nUnderstand that everyone is going through different things (family events, etc.), and be understanding of each other.\nBe engaged in the course.\nBe engaged within your assigned groups and help each other learn. Teaching another group member something you know solidifies your own knowledge and also sets you up to be a great future colleague.\n\n\n\n\n\n\n\nNoteA note on knowledge-sharing\n\n\n\n\n\nOur classroom is a shared intellectual space. Questions, mistakes, and partial understanding are part of learning. Please remember that supporting one another’s intellectual growth matters more than performative corrections or demonstrations of expertise.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#r-code-readings-and-discussion-sections",
    "href": "draft_sections/syllabus.html#r-code-readings-and-discussion-sections",
    "title": "2  Syllabus",
    "section": "2.9 R Code, readings, and discussion sections:",
    "text": "2.9 R Code, readings, and discussion sections:\nAll R code for the course is available within this Course Guidebook. Unlike past versions of this course, the present iteration no longer has traditional lectures. That said, I may respond to your questions by creating a presentation for you. I will share such material immediately.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#further-details-on-assignments-milestones",
    "href": "draft_sections/syllabus.html#further-details-on-assignments-milestones",
    "title": "2  Syllabus",
    "section": "2.10 Further details on assignments (Milestones):",
    "text": "2.10 Further details on assignments (Milestones):\nDetails about the final deliverable: The paper should be single-spaced (or no more than 1.15 line spacing), Times New Roman 11 point font (or similar), and one-inch margins. Lengths can be much less depending upon your questions. I just want to make sure you make this as concise as possible. If you had significant issues during the term with some component of your analysis (and you’ve communicated these with me), your final deliverable should focus on documenting your statistical issues and how you approached them. The following sections should be included (adhering to the specified length restrictions):\n\nOne paragraph introduction (max. ¼ page): includes why subject is important and question(s) and predictions\nMethods section (max. 2 pages): complete methods section, including statistics section\nResults section (max. 2 pages): detail of the results of all analyses described in the Methods section\nOne paragraph discussion (max. ¼ page): conclusions/summary of your results and what they mean\nAppendix/Supplemental Information (as long as you want): detailed statistics section and other details not included in the main methods. Most published articles do not want a long statistics section in the main manuscript. This will go into the Appendix. I want to see all of the details of your analysis in the Appendix, so that I can determine whether it makes sense.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#classroom-climate-and-conduct",
    "href": "draft_sections/syllabus.html#classroom-climate-and-conduct",
    "title": "2  Syllabus",
    "section": "2.11 Classroom Climate and Conduct:",
    "text": "2.11 Classroom Climate and Conduct:",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#classroom-behavior-policy",
    "href": "draft_sections/syllabus.html#classroom-behavior-policy",
    "title": "2  Syllabus",
    "section": "2.12 Classroom Behavior Policy:",
    "text": "2.12 Classroom Behavior Policy:",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#classroom-statement-on-diversity",
    "href": "draft_sections/syllabus.html#classroom-statement-on-diversity",
    "title": "2  Syllabus",
    "section": "2.13 Classroom Statement on Diversity:",
    "text": "2.13 Classroom Statement on Diversity:",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#duty-to-report",
    "href": "draft_sections/syllabus.html#duty-to-report",
    "title": "2  Syllabus",
    "section": "2.14 Duty to Report:",
    "text": "2.14 Duty to Report:",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#disability-statement",
    "href": "draft_sections/syllabus.html#disability-statement",
    "title": "2  Syllabus",
    "section": "2.15 Disability Statement:",
    "text": "2.15 Disability Statement:\nIf you have a physical, learning, sensory or psychological disability and require accommodations, please let me know as soon as possible. You will need to register with, and provide documentation of your disability to University Disability Support Services (UDSS) in SEO, room 330 Knight Hall.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/syllabus.html#academic-honesty",
    "href": "draft_sections/syllabus.html#academic-honesty",
    "title": "2  Syllabus",
    "section": "2.16 Academic Honesty:",
    "text": "2.16 Academic Honesty:\nThe University of Wyoming is built upon a strong foundation of integrity, respect and trust. All members of the university community have a responsibility to be honest and the right to expect honesty from others. Any form of academic dishonesty is unacceptable to our community and will not be tolerated [from the University Catalog]. Teachers and students should report suspected violations of standards of academic honesty to the instructor, department head, or dean. Other University regulations can be found here",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_welcome.html",
    "href": "draft_sections/01_welcome.html",
    "title": "2  Course Introduction",
    "section": "",
    "text": "2.1 The Core Goal of the Course",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_welcome.html#the-core-goal-of-the-course",
    "href": "draft_sections/01_welcome.html#the-core-goal-of-the-course",
    "title": "2  Course Introduction",
    "section": "",
    "text": "Messy field or lab data → processed data → models → inference\n\n\n\n\n\n\n\n\n\nNoteA note on Bayesian methods\n\n\n\n\n\nBayesian approaches are increasingly common and powerful, and this may change over the next 5–10 years. However, the foundational ideas about data structure, scale, and modeling assumptions are shared across frameworks.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_welcome.html#meetings-and-expectations",
    "href": "draft_sections/01_welcome.html#meetings-and-expectations",
    "title": "2  Course Introduction",
    "section": "2.3 Meetings and Expectations",
    "text": "2.3 Meetings and Expectations\nOn our first in-person meeting, we will: - do brief class introductions\n- discuss statistical philosophy\n- go through example problems\n- talk about common analytical pitfalls to avoid\nThis assumes you have watched or skimmed the asynchronous videos beforehand.\nThis course is designed to be cumulative and interactive—preparation matters.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/naming_conventions.html",
    "href": "draft_sections/naming_conventions.html",
    "title": "6  Naming conventions",
    "section": "",
    "text": "6.1 Part I: Naming Conventions and Data Organization\n[Based primarily on the Naming Conventions lecture :contentReferenceoaicite:0]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "draft_sections/naming_conventions.html#why-naming-conventions-matter",
    "href": "draft_sections/naming_conventions.html#why-naming-conventions-matter",
    "title": "6  Naming conventions",
    "section": "6.2 Why Naming Conventions Matter",
    "text": "6.2 Why Naming Conventions Matter\n\n[Explain why variable names, file names, and coding style influence collaboration, reproducibility, and long-term scalability]\n[Discuss how unconventional naming can cause downstream problems in analysis]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "draft_sections/naming_conventions.html#common-naming-conventions",
    "href": "draft_sections/naming_conventions.html#common-naming-conventions",
    "title": "6  Naming conventions",
    "section": "6.3 Common Naming Conventions",
    "text": "6.3 Common Naming Conventions\n\n[Describe camelCase, PascalCase, and snake_case]\n[Explain why snake_case is typically preferred in R and data science]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "draft_sections/naming_conventions.html#general-rules-for-naming-data-objects",
    "href": "draft_sections/naming_conventions.html#general-rules-for-naming-data-objects",
    "title": "6  Naming conventions",
    "section": "6.4 General Rules for Naming Data Objects",
    "text": "6.4 General Rules for Naming Data Objects\n\n[Outline rules about avoiding spaces, special characters, leading numbers, and inconsistent capitalization]\n[Describe best practices for handling dates, missing values, and metadata]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "draft_sections/naming_conventions.html#naming-variables-vs.-naming-functions",
    "href": "draft_sections/naming_conventions.html#naming-variables-vs.-naming-functions",
    "title": "6  Naming conventions",
    "section": "6.5 Naming Variables vs. Naming Functions",
    "text": "6.5 Naming Variables vs. Naming Functions\n\n[Explain the noun/verb distinction for objects and functions]\n[Provide guidance on clarity versus brevity]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "draft_sections/naming_conventions.html#coding-style-and-the-tidyverse",
    "href": "draft_sections/naming_conventions.html#coding-style-and-the-tidyverse",
    "title": "6  Naming conventions",
    "section": "6.6 Coding Style and the Tidyverse",
    "text": "6.6 Coding Style and the Tidyverse\n\n[Introduce the tidyverse style guide and its role in standardization]\n[Describe tools such as lintr and styler and what problems they solve]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "draft_sections/naming_conventions.html#data-file-formats-and-dataset-management",
    "href": "draft_sections/naming_conventions.html#data-file-formats-and-dataset-management",
    "title": "6  Naming conventions",
    "section": "6.7 Data File Formats and Dataset Management",
    "text": "6.7 Data File Formats and Dataset Management\n\n[Compare acceptable formats for small datasets (CSV, TSV)]\n[Introduce efficient formats for larger datasets (e.g., parquet) and why they matter]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "draft_sections/naming_conventions.html#becoming-a-better-statistical-programmer",
    "href": "draft_sections/naming_conventions.html#becoming-a-better-statistical-programmer",
    "title": "6  Naming conventions",
    "section": "6.8 Becoming a Better Statistical Programmer",
    "text": "6.8 Becoming a Better Statistical Programmer\n\n[Encourage iterative improvement of existing projects]\n[Discuss professional responsibility in collaborative data management]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html",
    "href": "draft_sections/data_exploration.html",
    "title": "8  Data exploration",
    "section": "",
    "text": "8.1 Data Exploration\nYou have already collected data. Now we focus on: - understanding what those data actually are\n- identifying patterns\n- building appropriate models\n- validating and selecting those models\n- making inference and generating new hypotheses\nWe will take a frequentist approach in this course. We will not cover Bayesian modeling in detail, though many ideas—especially model structure and philosophy—transfer directly.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#distinguishing-measurand-observed-data-and-estimated-value",
    "href": "draft_sections/data_exploration.html#distinguishing-measurand-observed-data-and-estimated-value",
    "title": "8  Data exploration",
    "section": "8.2 Distinguishing measurand, observed data, and estimated value",
    "text": "8.2 Distinguishing measurand, observed data, and estimated value\n\n\n\n\n\n\nTipPopulation abundance is changing over time\n\n\n\nPrompt: [What is the measurand? What are the observed data? What does the model return?]\n\n\nReveal\n\n\nMeasurand: True population abundance within a defined area/time window\n\nObserved: Counts/detections/encounters\n\nEstimated: Abundance available for detection during sampling, conditional on closure/availability",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#why-data-exploration-comes-first",
    "href": "draft_sections/data_exploration.html#why-data-exploration-comes-first",
    "title": "8  Data exploration",
    "section": "8.3 Why Data Exploration Comes First",
    "text": "8.3 Why Data Exploration Comes First\n\n[Explain the risks of skipping exploratory steps]\n[Frame exploration as hypothesis protection, not data dredging]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#step-1-identifying-outliers",
    "href": "draft_sections/data_exploration.html#step-1-identifying-outliers",
    "title": "8  Data exploration",
    "section": "8.4 Step 1: Identifying Outliers",
    "text": "8.4 Step 1: Identifying Outliers\n\n[Define univariate vs. multivariate outliers]\n[Discuss different sources of outliers: error, biology, rare events]\n\n\n8.4.1 Graphical Identification of Univariate Outliers\n\n[Describe Cleveland plots, Tukey boxplots, and conditional boxplots]\n[Explain strengths and weaknesses of each]\n\n\n\n8.4.2 Statistical vs. Expert Judgment\n\n[Contrast formal statistical rules with biological or domain expertise]\n[Discuss risks of authority-based decisions]\n\n\n\n8.4.3 Multivariate Outliers\n\n[Introduce Mahalanobis distance and robust alternatives]\n[Explain why univariate methods can fail]\n\n\n\n8.4.4 What to Do When You Find Outliers\n\n[Outline options: removal, parallel analyses, transformation, or no action]\n[Emphasize transparency and justification]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#step-2-examining-zeroes-in-the-data",
    "href": "draft_sections/data_exploration.html#step-2-examining-zeroes-in-the-data",
    "title": "8  Data exploration",
    "section": "8.5 Step 2: Examining Zeroes in the Data",
    "text": "8.5 Step 2: Examining Zeroes in the Data\n\n[Explain zero inflation and why it matters]\n[Describe when excess zeroes signal a modeling problem versus a biological pattern]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#step-3-collinearity-among-predictors",
    "href": "draft_sections/data_exploration.html#step-3-collinearity-among-predictors",
    "title": "8  Data exploration",
    "section": "8.6 Step 3: Collinearity Among Predictors",
    "text": "8.6 Step 3: Collinearity Among Predictors\n\n[Define collinearity and why it destabilizes models]\n[Contrast implications for frequentist vs. Bayesian models]\n\n\n8.6.1 Detecting Collinearity\n\n[Describe scatterplots, correlation coefficients, and VIFs/GVIFs]\n[Explain interpretation thresholds (e.g., VIF &lt; 3)]\n\n\n\n8.6.2 Dealing with Collinearity\n\n[Describe selective variable removal]\n[Introduce PCA as a dimensionality-reduction approach]\n[Discuss biological reasoning in deciding what to retain]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#step-4-considering-interaction-terms",
    "href": "draft_sections/data_exploration.html#step-4-considering-interaction-terms",
    "title": "8  Data exploration",
    "section": "8.7 Step 4: Considering Interaction Terms",
    "text": "8.7 Step 4: Considering Interaction Terms\n\n[Explain what interaction terms represent biologically]\n[Discuss sample size constraints and interpretability]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#step-5-standardizing-scaling-or-leaving-covariates-alone",
    "href": "draft_sections/data_exploration.html#step-5-standardizing-scaling-or-leaving-covariates-alone",
    "title": "8  Data exploration",
    "section": "8.8 Step 5: Standardizing, Scaling, or Leaving Covariates Alone",
    "text": "8.8 Step 5: Standardizing, Scaling, or Leaving Covariates Alone\n\n[Define standardization and centering]\n[Explain how these choices affect interpretation of coefficients and intercepts]\n[Provide guidance on when each approach is appropriate]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#step-6-violations-of-homogeneity",
    "href": "draft_sections/data_exploration.html#step-6-violations-of-homogeneity",
    "title": "8  Data exploration",
    "section": "8.9 Step 6: Violations of Homogeneity",
    "text": "8.9 Step 6: Violations of Homogeneity\n\n[Introduce homoscedasticity and heteroscedasticity]\n[Explain why residuals—not raw data—are the focus]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_roadmap.html",
    "href": "draft_sections/02_course_roadmap.html",
    "title": "3  Course Roadmap",
    "section": "",
    "text": "3.1 Course Roadmap\nThe course progresses as follows:\nGiven the scope of the entire course, we may only scratch the surface of some topics. Even our cursory treatment will convince you why these subjects matter for scientists who have messy data.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_roadmap.html#course-roadmap",
    "href": "draft_sections/02_course_roadmap.html#course-roadmap",
    "title": "3  Course Roadmap",
    "section": "",
    "text": "Metrology\nData exploration, outliers, visual inspection\nData heterogeneity\nGeneralized Linear Models (GLMs)\nGeneral principles of model validation\nGeneralized Linear Mixed Models (GLMMs)\nInformation Theory of Model Selection\nGeneralized Additive (Mixed) Models (GAMMs)\nSpatial and Temporal autocorrelation\nStructural Causal Models (but still not Bayesian)",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_roadmap.html#course-roadmap-1",
    "href": "draft_sections/02_course_roadmap.html#course-roadmap-1",
    "title": "3  course_roadmap",
    "section": "3.2 Course Roadmap",
    "text": "3.2 Course Roadmap\nThe course progresses as follows: - Metrology - Data exploration, outliers, visual inspection - Data heterogeneity - Generalized Linear Models (GLMs) - General principles of model validation - Generalized Linear Mixed Models (GLMMs) - Information Theory of Model Selection - Generalized Additive (Mixed) Models (GAMMs) - Spatial and Temporal autocorrelation - Structural Causal Models (but still not Bayesian)\nGiven the scope of the entire course, we may only scratch the surface of some topics. Even our cursory treatment will convince you why these subjects matter for scientists who have messy data.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>course_roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_roadmap.html#how-this-course-is-structured",
    "href": "draft_sections/02_course_roadmap.html#how-this-course-is-structured",
    "title": "3  Course Roadmap",
    "section": "3.2 How This Course Is Structured",
    "text": "3.2 How This Course Is Structured\nWe will meet twice weekly via Zoom. work in small groups, and you will also work extensively with your own data. By early October (around October 4), your dataset should be in working order—not just technically usable, but conceptually ready to explain to others.\n\n\n\n\n\n\nNote\n\n\n\nWorking order does not mean “a single spreadsheet.”\nA dataset is only complete if it includes metadata. Metadata describe: - what the variables are\n- how the data were collected\n- under what conditions\n- with what assumptions and limitations\nIf you have questions about metadata, I have a dedicated lecture on metadata structures, and I strongly encourage you to reach out.\n\n\n\n3.2.1 What to expect next\n\n[Describe what will happen in the remainder of the first class]\n[Outline upcoming assignments or activities]",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_roadmap.html#getting-started",
    "href": "draft_sections/02_course_roadmap.html#getting-started",
    "title": "3  Course Roadmap",
    "section": "3.3 Getting Started",
    "text": "3.3 Getting Started\n\n3.3.1 Who are you?\n\n[Prompt students to introduce themselves]\n[Encourage sharing of disciplinary background]\n\n\n\n3.3.2 Your data\n\n[Ask students to describe the data they work with or hope to work with]\n[Surface common challenges and anxieties]\n\n\n\n3.3.3 Your concerns and expectations\n\n[Invite discussion of fears, gaps, or uncertainties]",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html",
    "href": "draft_sections/03_syllabus.html",
    "title": "4  Location and meeting times",
    "section": "",
    "text": "4.1 Office hours\nTuesday/Thursday on Zoom; 11:00AM - 12:15PM (Mountain Time) Spring term 2026\nDue to the nature of the course, I will not have specific office hours for the course. I have extended class times (after our weekly meetings) to allow more time to ask questions and for one-on-one work. These are optional and may be canceled in some weeks depending on your interest in meeting.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#course-objectives",
    "href": "draft_sections/03_syllabus.html#course-objectives",
    "title": "4  Syllabus",
    "section": "",
    "text": "Think quantitatively about messy ecological data. You should be on your way to developing a habit of translating real-world ecological questions into quantitative frameworks, while explicitly acknowledging uncertainty and bias in estimation and measurement.\nDistinguish between questions/hypotheses, data, and inferences You should be able to (1) clearly separate ecological hypotheses, the data actually collected, and the quantities being estimated, and (2) understand why these distinctions matter for interpretation and decision-making.\nUnderstand the processes that generate data You should be able to recognize how different study designs, variation in detection, observer effects, instrument error, and data processing shape and constrain the structure of ecological datasets.\nSelect, build, and critique statistical models as scientific tools You should be comfortable choosing and using statistical models not as black boxes, but as explicit representations of assumptions about ecological processes, variation, and causal structure. You should never blindly choose a statistical model again!\nInterpret results in ecological —not just statistical— terms You should be able to conceptually move beyond p-values and coefficients to clearly articulate what your results mean biologically, mechanistically, and practically.\nBe comfortable applying causal –and not just correlational– reasoning You should be able to use causal thinking (conceptual models and directed acyclic graphs) to evaluate what can —and ehat cannot— be inferred from observational and experimental data.\nCommunicate quantitative results clearly, transparently, and honestly Present analyses, figures, and conclusions in ways that are transparent, reproducible, and appropriate for scientific audiences.\nDevelop durable, reproducible analytical workflows You should be increasingly comfortable with good data-science practices that support clarity, versioning (even though we will not delve into GitHub this term), and reusability of analyses.\nDevelop new confidence working with unfamiliar, complex, imperfect datasets You should leave this course better prepared to engage with real ecological data —without expecting it to be clean, complete, or simple.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#office-hours",
    "href": "draft_sections/03_syllabus.html#office-hours",
    "title": "4  Syllabus",
    "section": "4.2 Office hours",
    "text": "4.2 Office hours\nDue to the nature of the course, I will not have specific office hours for the course. I have extended class times (after our weekly meetings) to allow more time to ask questions and for one-on-one work. These are optional and may be canceled in some weeks depending on your interest in meeting.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#location-and-meeting-times",
    "href": "draft_sections/03_syllabus.html#location-and-meeting-times",
    "title": "4  Syllabus",
    "section": "4.2 Location and meeting times",
    "text": "4.2 Location and meeting times\nTuesday/Thursday on Zoom; 11:00AM - 12:15PM (Mountain Time) Spring term 2026",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#prerequisites",
    "href": "draft_sections/03_syllabus.html#prerequisites",
    "title": "4  Location and meeting times",
    "section": "4.2 Prerequisites",
    "text": "4.2 Prerequisites\nTo ensure your success in this course, the following are required: - You must have a dataset to be analyzed this semester. This is very important. This class will only cover data reformatting; we will not cover data processing (except as necessary in specific cases). - Your data analysis must not be used in another (past or present). - You must be at least in your second year of graduate school. - You must have some exposure to using the Program R (tidyverse preferred, but base R is nice too). - You must have taken a statistics course in the last five years.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#attendanceparticipation-policy",
    "href": "draft_sections/03_syllabus.html#attendanceparticipation-policy",
    "title": "4  Location and meeting times",
    "section": "4.4 Attendance/Participation Policy:",
    "text": "4.4 Attendance/Participation Policy:\nThis is a graduate level course, and you are here for your own benefit. That being said, I expect you to come to class, stay engaged with the material, and not only learn how to do your own analysis but understand other types of data and analyses by working with your group members. If you do this, you should have analyzed your own data by the end of the semester and have part of a manuscript completed. Please email me ahead of time when you will be unable to attend class for a valid reason.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "href": "draft_sections/03_syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "title": "4  Location and meeting times",
    "section": "4.5 How to succeed in the course (beyond your wildest and funniest dreams)",
    "text": "4.5 How to succeed in the course (beyond your wildest and funniest dreams)\nGraduate school can be considerably challenging, as everyone is attempting to juggle research, teaching, classes, health, and family, all while coping with unexpected stressors. Course information is flying at you from every direction; there are many specific terms and concepts that you need to learn and operationalize. So, here are some reminders for you (even though I know you don’t need these):\n\nAsk questions! Even though there are no exams, take copious notes and work collaboratively to build course notes.\nDon’t be afraid to redirect the flow of the course. 5000-level courses should be flexible and fun. I want to give you time to think about and discuss the material. I’m willing to alter the pace of the course, change the order of topics, or devise new exercises for you. This is intended to be fun (while simultaneously transforming you into analytical gurus)! So, just talk to me about how I can help!\nRead all the material in this course guidebook. Many online courses require much more reading; this one does not.\nShow up to as many of the synchronous (Zoom) discussions as you can. When we meet together online, our goal will be to solidify everyone’s understanding of different concepts and how they are linked. These concepts will be useful as you navigate your own analysis project.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#expectations",
    "href": "draft_sections/03_syllabus.html#expectations",
    "title": "4  Location and meeting times",
    "section": "4.6 Expectations",
    "text": "4.6 Expectations\nAs your instructor, you should expect me to:\n\nTry my very hardest to make the course go smoothly (the reason you now have this nice new online course guide); but please be prepared for the inevitable hiccups. No matter how hard we all try, there always seem to be a few obstacles (like internet going down for a couple of hours when we’re on Zoom).\nRespond to questions within 24 hours during the work week. However, I likely will not respond during the weekend (unless there is an urgent matter).\nRespect you not only as a learner but as a colleague.\nUnderstand that these are strange and sometimes unforgiving times. We all have varying levels of tolerance and resistance to stress. If you are having a hard time for whatever reason, please communicate with me. I suck at judging, but I can do a hell of a job listening and working with you to solve a problem.\n\nAs a student, you are expected to:\n\nBe respectful of everyone in the class, including me.\nAsk for help if needed.\nTreat your presence in the classroom and your enrollment in this course as you would a job; Act professionally, arrive on time, pay attention, complete your work in a timely and professional manner, and treat your learning seriously.\nUnderstand that everyone is going through different things (family events, etc.), and be understanding of each other.\nBe engaged in the course.\nBe engaged within your assigned groups and help each other learn. Teaching another group member something you know solidifies your own knowledge and also sets you up to be a great future colleague.\n\n\n\n\n\n\n\nNoteA note on knowledge-sharing\n\n\n\n\n\nOur classroom is a shared intellectual space. Questions, mistakes, and partial understanding are part of learning. Please remember that supporting one another’s intellectual growth matters more than performative corrections or demonstrations of expertise.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#r-code-readings-and-discussion-sections",
    "href": "draft_sections/03_syllabus.html#r-code-readings-and-discussion-sections",
    "title": "4  Location and meeting times",
    "section": "4.7 R Code, readings, and discussion sections:",
    "text": "4.7 R Code, readings, and discussion sections:\nAll R code required for both instruction and hands-on exercises is available within this course book. Unlike past versions of this course, the present iteration no longer has traditional lectures. That said, I may respond to your questions by creating mini-presentations for you. I will share such material immediately after our discussions.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#further-details-on-assignments-milestones",
    "href": "draft_sections/03_syllabus.html#further-details-on-assignments-milestones",
    "title": "5  Syllabus",
    "section": "5.9 Further details on assignments (Milestones):",
    "text": "5.9 Further details on assignments (Milestones):\nDetails about the final deliverable: The paper should be single-spaced (or no more than 1.15 line spacing), Times New Roman 11 point font (or similar), and one-inch margins. Lengths can be much less depending upon your questions. I just want to make sure you make this as concise as possible. If you had significant issues during the term with some component of your analysis (and you’ve communicated these with me), your final deliverable should focus on documenting your statistical issues and how you approached them. The following sections should be included (adhering to the specified length restrictions):\n\nOne paragraph introduction (max. ¼ page): includes why subject is important and question(s) and predictions\nMethods section (max. 2 pages): complete methods section, including statistics section\nResults section (max. 2 pages): detail of the results of all analyses described in the Methods section\nOne paragraph discussion (max. ¼ page): conclusions/summary of your results and what they mean\nAppendix/Supplemental Information (as long as you want): detailed statistics section and other details not included in the main methods. Most published articles do not want a long statistics section in the main manuscript. This will go into the Appendix. I want to see all of the details of your analysis in the Appendix, so that I can determine whether it makes sense.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#classroom-climate-and-conduct",
    "href": "draft_sections/03_syllabus.html#classroom-climate-and-conduct",
    "title": "4  Location and meeting times",
    "section": "4.8 Classroom Climate and Conduct:",
    "text": "4.8 Classroom Climate and Conduct:\nAgain, you will be respectful towards your classmates and your instructors. Spirited debate and disagreement are to be expected in any classroom, and all perspectives will be heard, but we will behave civilly and with respect towards one another. Personal attacks, offensive language, name-calling, and dismissive gestures (eye-rolling, saying “whatever”, etc.) are not warranted in a learning atmosphere. Plus, in my opinion, such behavior shows an ability to problem-solve, which is counter to the mission of any university. As your instructor, I have the right to dismiss you from the classroom if you engage in disrespectful or disruptive behavior. Lastly, for the privacy of your fellow students, please do not record the lectures (unless with permission of the instructor).",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#classroom-behavior-policy",
    "href": "draft_sections/03_syllabus.html#classroom-behavior-policy",
    "title": "5  Syllabus",
    "section": "5.11 Classroom Behavior Policy:",
    "text": "5.11 Classroom Behavior Policy:",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#classroom-statement-on-diversity",
    "href": "draft_sections/03_syllabus.html#classroom-statement-on-diversity",
    "title": "4  Location and meeting times",
    "section": "4.9 Classroom Statement on Diversity:",
    "text": "4.9 Classroom Statement on Diversity:\nThe University of Wyoming values an educational environment that is diverse, equitable, and inclusive. The diversity that students and faculty bring to class, including age, country of origin, culture, disability, economic class, ethnicity, gender identity, immigration status, linguistic, political affiliation, race, religion, sexual orientation, veteran status, worldview, and other social and cultural diversity is valued, respected, and considered a resource for learning. we understand that our UW community members represent a rich variety of backgrounds and perspectives. We are committed to providing an atmosphere for learning that respects diversity of all types. While working together to build this community, we ask all members–from students to staff to faculty–to:\n\nBe transparent about pre-existing biases and beliefs.\nDo not hesitate to share their unique experiences and perspectives.\nBe open to the views of others.\nHonor the uniqueness of their colleagues.\nAppreciate the opportunity that we have to learn from each other in this community.\nValue each other’s opinions and communicate in a respectful manner.\nKeep confidential any discussions of a personal (or professional) nature.\nUse this opportunity together to discuss ways in which we can create an inclusive environment in this course and across the University community.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#duty-to-report",
    "href": "draft_sections/03_syllabus.html#duty-to-report",
    "title": "4  Location and meeting times",
    "section": "4.10 Duty to Report:",
    "text": "4.10 Duty to Report:\nUW faculty are committed to supporting students and upholding the University’s non-discrimination policy. Under Title IX, discrimination based upon sex and gender is prohibited. If you experience an incident of sex- or gender-based discrimination, we encourage you to report it. While you may talk to a faculty member, understand that as a “Responsible Employee” of the University, the faculty member is required to report information you share about the incident to the University’s Title IX Coordinator (you may choose whether you or anyone involved is identified by name). If you would like to speak with someone who may be able to afford you privacy or confidentiality, there are people who can meet with you. Faculty can help direct you or you may find info about UW policy and resources at http://www.uwyo.edu/reportit. While we want you to feel comfortable coming to us with issues you may be struggling with or concerns you may be having, please be aware that we have some reporting requirements that are part of our job requirements at UW. You do not have to go through the experience alone. For example, if you inform us of an issue of sexual harassment, sexual assault, or discrimination we will keep the information as private as we can, but we am required to bring it to the attention of the institution’s Title IX Coordinator. If you would like to talk to those offices directly, you can contact Equal Opportunity Report and Response (Bureau of Mines Room 319, 766-5200, report-it@uwyo.edu, www.uwyo.edu/reportit). Additionally, you can also report incidents or complaints to the UW Police Department. You can also get support at the STOP Violence program (stopviolence@uwyo.edu, www.uwyo.edu/stop, 766-3296) (or SAFE Project (www.safeproject.org, campus@safeproject.org, 766-3434, 24-Hour hotline: 745-3556). Assistance and resources are available, and you are not required to make a formal complaint or participate in an investigation to access them. Another common example is if you are struggling with an issue that may be traumatic, or under unusual stress. We will likely inform the Dean of Students Office or Counseling Center. If you would like to reach out directly to them for assistance, you can contact them by going to www.uwyo.edu/dos/uwyocares.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#disability-statement",
    "href": "draft_sections/03_syllabus.html#disability-statement",
    "title": "4  Location and meeting times",
    "section": "4.11 Disability Statement:",
    "text": "4.11 Disability Statement:\nIf you have a physical, learning, sensory or psychological disability and require accommodations, please let me know as soon as possible. You will need to register with, and provide documentation of your disability to University Disability Support Services (UDSS) in SEO, room 330 Knight Hall.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#academic-honesty",
    "href": "draft_sections/03_syllabus.html#academic-honesty",
    "title": "4  Location and meeting times",
    "section": "4.12 Academic Honesty:",
    "text": "4.12 Academic Honesty:\nThe University of Wyoming is built upon a strong foundation of integrity, respect and trust. All members of the university community have a responsibility to be honest and the right to expect honesty from others. Any form of academic dishonesty is unacceptable to our community and will not be tolerated [from the University Catalog]. Teachers and students should report suspected violations of standards of academic honesty to the instructor, department head, or dean. Other University regulations can be found here",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/statistical_aphorisms.html",
    "href": "draft_sections/statistical_aphorisms.html",
    "title": "5  Statistical Aphorisms",
    "section": "",
    "text": "5.1 Three Statistical Aphorisms (to guide you through this course)",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "draft_sections/statistical_aphorisms.html#three-statistical-aphorisms-to-guide-you",
    "href": "draft_sections/statistical_aphorisms.html#three-statistical-aphorisms-to-guide-you",
    "title": "5  Statistical Aphorisms",
    "section": "",
    "text": "NoteAphorism 1: For progress, embrace imperfection\n\n\n\nBeware perfection. In data analysis, one can be paralyzed by “perfect” solutions. After all, perfection is a moving target. What does “perfection” really mean in practice? Does it mean that an analysis looks complex enough to impress your colleagues or instructors, even if it’s not appropriate for your questions? Is it an analysis that works flawlessly the first time, right out of the box? Or is it one that is quickly built and deployed but that also may hide critical assumptions or produce error-filled predictions? Or perhaps one that a senior colleague or advisor has deemed “the correct” approach (without justification)?\nThe danger lies in waiting for an ideal solution that may never arrive, or may arrive too late. In doing so, your and your collaborators’ progress stalls, opportunities are missed, and, perhaps most importantly, learning is sorely delayed. This idea is captured poetically in the words of Robert Watson-Watt, the inventor of radar:\n\nGive them the third best to go on with; the second best comes too late, the best never comes.\n\nWatson-Watt’s advice should remind us that, in dynamic and complex fields such as radar development or our own messy statistical modeling, timely action often outweighs our vision of perfection. A solution that is simply good enough today allows you to:\n\nLearn through deployment: Practical application often reveals insights that you cannot anticipate.\nIterate and improve: Real-world feedback reveals issues far faster than endless refinement in isolation!\nTransparently communicate analytical limitations: Collaborators and stakeholders can better understand assumptions, uncertainties, and the scope of your work.\n\nIn short (as as you know), perfection is often the enemy of good progress. By consciously and purposefully adopting a culture of imperfection, you will be able to better learn, iterate, and produce work that is timely, practical, and ultimately more impactful.\n\n\n\n\n\n\n\n\nNoteAphorism #2: Beware of Armadillo Burrows.\n\n\n\nDon’t fall into someone else’s statistical trap!\n\n\n\n\n\n\n\n\nNoteAphorism #3: Beware paralysis of analysis.\n\n\n\nCamponotus leonardi infected by a “zombie fungus” (genus Ophiocordyceps)",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "draft_sections/statistical_aphorisms.html#analytical-workflow",
    "href": "draft_sections/statistical_aphorisms.html#analytical-workflow",
    "title": "5  Statistical Aphorisms",
    "section": "5.2 Analytical Workflow",
    "text": "5.2 Analytical Workflow",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "draft_sections/statistical_aphorisms.html#defining-analytical-workflow",
    "href": "draft_sections/statistical_aphorisms.html#defining-analytical-workflow",
    "title": "5  Statistical Aphorisms",
    "section": "5.3 Defining “analytical workflow”",
    "text": "5.3 Defining “analytical workflow”\nWhat do we mean by analytical workflow? First, it is the data infrastructure that supports efficient analysis. It turns raw data into action.\nThere should also be analysis of the workflow itself. Is it efficient in terms of computational load or time? Does it allow for reproducibility?",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "draft_sections/statistical_aphorisms.html#two-thoughts-as-you-develop-your-analytical-workflow",
    "href": "draft_sections/statistical_aphorisms.html#two-thoughts-as-you-develop-your-analytical-workflow",
    "title": "5  Statistical Aphorisms",
    "section": "5.4 Two thoughts as you develop your analytical workflow",
    "text": "5.4 Two thoughts as you develop your analytical workflow\n\nThere are a huge &gt;\n\nHere’s my wisdom for your use, as I learned it when the moose And the reindeer roamed where Paris roars to-night: “There are nine and sixty ways of constructing tribal lays, And—every—single—one—of—them—is—right! –From In the Neolithic Age (Rudyard Kipling)\na core tenet of collaborative, creative, and productive data science Let me that for you How many of you have experience with prompt engineering? Your goals as a scientist… Use this course to develop your own philosophical workflow (that reduces bias) Stick to this philosophy until you learn something new and improved",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "draft_sections/statistical_aphorisms.html#statistical-philosophy",
    "href": "draft_sections/statistical_aphorisms.html#statistical-philosophy",
    "title": "5  Statistical Aphorisms",
    "section": "5.5 Statistical Philosophy",
    "text": "5.5 Statistical Philosophy\nMore than anything else, I want you to use this course to develop a personal statistical philosophy.\nA good philosophy: - reduces bias\n- prevents analytical wandering\n- promotes clarity and efficiency\n- evolves as you learn more\nI will present a set of guiding principles and aphorisms early in the course. You should modify them, reject them, or replace them—but you should have something guiding your decisions.\nThis is how you become consistent, thoughtful, and credible as a scientist.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_welcome.html#why-this-course",
    "href": "draft_sections/01_welcome.html#why-this-course",
    "title": "2  Course Introduction",
    "section": "2.2 Why this course?",
    "text": "2.2 Why this course?\nThe benefit of this course is to not only teach you about statistics (and some statistical traps), but to have you working with your peers as you delve into the issues that inevitably arise with your own data analysis. Learning how to navigate data analysis issues is a skill that is critical for developing researchers. Please try to make this class as useful as possible for you and include most or all of the analyses that you expect will go into one of your dissertation chapters. You can just include one analysis if that is all that you need or you can include 4-5 analyses if that is what you need to answer your question(s). This is an opportunity for you to get more help with data analysis, so take advantage.\nThe central goal of this course is to guide you through the transition:\n\nMessy field or lab data → processed data → models → inference\n\nYou have already collected data. Now we focus on: - understanding what those data actually are\n- identifying patterns\n- building appropriate models\n- validating and selecting those models\n- making inference and generating new hypotheses\nWe will take a frequentist approach in this course. We will not cover Bayesian modeling in detail, though many ideas—especially model structure and philosophy—transfer directly.\n\n\n\n\n\n\nNoteA note on Bayesian methods\n\n\n\n\n\nBayesian approaches are increasingly common and powerful, and this may change over the next 5–10 years. However, the foundational ideas about data structure, scale, and modeling assumptions are shared across frameworks.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/statistical_aphorisms.html#three-statistical-aphorisms-to-guide-you-through-this-course",
    "href": "draft_sections/statistical_aphorisms.html#three-statistical-aphorisms-to-guide-you-through-this-course",
    "title": "5  Statistical Aphorisms",
    "section": "5.2 Analytical Workflow",
    "text": "NoteAphorism 1: Embrace imperfection\n\n\n\nBeware perfection. In data analysis, one can be paralyzed by “perfect” solutions. After all, perfection is a moving target. What does “perfection” really mean in practice? Does it mean that an analysis looks complex enough to impress your colleagues or instructors, even if it’s not appropriate for your questions? Is it an analysis that works flawlessly the first time, right out of the box? Or is it one that is quickly built and deployed but that also may hide critical assumptions or produce error-filled predictions? Or perhaps one that a senior colleague or advisor has deemed “the correct” approach (without justification)?\nThe danger lies in waiting for an ideal solution that may never arrive, or may arrive too late. In doing so, your and your collaborators’ progress stalls, opportunities are missed, and, perhaps most importantly, learning is sorely delayed. This idea is captured poetically in the words of Robert Watson-Watt, the inventor of radar:\n\nGive them the third best to go on with; the second best comes too late, the best never comes.\n\nWatson-Watt’s advice should remind us that, in dynamic and complex fields such as radar development or our own messy statistical modeling, timely action often outweighs our vision of perfection. A solution that is simply good enough today allows you to:\n\nLearn through deployment: Practical application often reveals insights that you cannot anticipate.\nIterate and improve: Real-world feedback reveals issues far faster than endless refinement in isolation!\nTransparently communicate analytical limitations: Collaborators and stakeholders can better understand assumptions, uncertainties, and the scope of your work.\n\nIn short (as as you know), perfection is often the enemy of good progress. By consciously and purposefully adopting a culture of imperfection, you will be able to better learn, iterate, and produce work that is timely, practical, and ultimately more impactful.\n\n\n\n\n\n\n\n\nNoteAphorism 2: Embrace complexity.\n\n\n\nBeware over-simplicity.\n\n\n\n\n\n\nNoteAphorism #3: Beware of Armadillo Burrows.\n\n\n\nDon’t fall into someone else’s statistical trap!\n\n\n\n\n\n\n\n\nNoteAphorism #: Beware paralysis of analysis.\n\n\n\nCamponotus leonardi infected by a “zombie fungus” (genus Ophiocordyceps)\n\n\n\n5.2 Analytical Workflow\n\n\n5.3 Defining “analytical workflow”\nWhat do we mean by analytical workflow? First, it is the data infrastructure that supports efficient analysis. It turns raw data into action.\nThere should also be analysis of the workflow itself. Is it efficient in terms of computational load or time? Does it allow for reproducibility?\n\n\n5.4 Two thoughts as you develop your analytical workflow\n\nThere are a huge &gt;\n\nHere’s my wisdom for your use, as I learned it when the moose And the reindeer roamed where Paris roars to-night: “There are nine and sixty ways of constructing tribal lays, And—every—single—one—of—them—is—right! –From In the Neolithic Age (Rudyard Kipling)\na core tenet of collaborative, creative, and productive data science Let me that for you How many of you have experience with prompt engineering? Your goals as a scientist… Use this course to develop your own philosophical workflow (that reduces bias) Stick to this philosophy until you learn something new and improved\n\n\n5.5 Statistical Philosophy\nMore than anything else, I want you to use this course to develop a personal statistical philosophy.\nA good philosophy: - reduces bias\n- prevents analytical wandering\n- promotes clarity and efficiency\n- evolves as you learn more\nI will present a set of guiding principles and aphorisms early in the course. You should modify them, reject them, or replace them—but you should have something guiding your decisions.\nThis is how you become consistent, thoughtful, and credible as a scientist.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "index.html#daily-resources",
    "href": "index.html#daily-resources",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "1.1 Daily resources",
    "text": "1.1 Daily resources\nGoogle Sheet with analysis resources Wikipedia page with troubleshooting",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Information & Calendar</span>"
    ]
  },
  {
    "objectID": "index.html#course-calendar",
    "href": "index.html#course-calendar",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "1.3 Course Calendar",
    "text": "1.3 Course Calendar\nThis calendar may change slightly as we progress through the term. You will receive immediate notification of any and all changes.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nIn-class exercise\nMilestone assignment\nStress level\nGrade weight (%)\n\n\n\n\n1 (19-Jan-2026)\nCourse intro; RStudio setup; AI guardrails\nProject scoping discussion\n—\nVery low\n5\n\n\n2 (26-Jan-2026)\nFile structure; reproducible workflows; AI logs\nFile audit walkthrough\nAnalysis Concept Note\nLow\n5\n\n\n3 (02-Feb-2026)\nData exploration; measurement & uncertainty\nExploratory data analysis walkthrough\n—\nVery low\n0\n\n\n4 (09-Feb-2026)\nGLMs (theory + practice)\nGLM exercise\nData Readiness Note\nLow\n10\n\n\n5 (16-Feb-2026)\nGLMs; AIC & information theory\nModel comparison exercise\n—\nLow\n5\n\n\n6 (23-Feb-2026)\nGLMMs; effective sample size\nGLMM exercise\n—\nModerate\n15\n\n\n7 (02-Mar-2026)\nGAMs\nGAM exercise\nWorking Model (draft)\nModerate\n0\n\n\n8 (09-Mar-2026)\nGAMMs; spatial & temporal heterogeneity\nModel refinement exercise\nWorking Model (final lock)\nModerate\n15\n\n\n9 (16-Mar-2026)\nStudent Spring Break\n—\n—\nNone\n0\n\n\n10 (23-Mar-2026)\nStructural Causal Modeling (SCM)\nCausal diagram critique\n—\nLow\n10\n\n\n11 (30-Mar-2026)\nInstructor Spring Break\n—\n—\nNone\n0\n\n\n12 (06-Apr-2026)\nPrediction & uncertainty\nPrediction checks\nInterpretation Memo\nLow\n10\n\n\n13 (13-Apr-2026)\nSynthesis & justification\nPeer + AI review\n—\nModerate\n0\n\n\n14 (20-Apr-2026)\nTables, figures, reporting standards\nTable/figure workshop\nResults Section\nLow\n10\n\n\n15 (27-Apr-2026)\nWriting Results sections\nDraft clinic\nFull Draft\nModerate\n15\n\n\n16 (04-May-2026)\nReflection & closure\nCourse wrap-up\nRevision Plan (not executed)\nNone\n5",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Information & Calendar</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html",
    "href": "draft_sections/preprocessing.html",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "",
    "text": "9.1 Overview\nWelcome back. This marks the beginning of the data exploration phase of the course. In this video, we focus on basic exploratory steps that should always be done before formal modeling.\nThe goal here is not to perform inference, but to identify potential problems early—before they complicate or derail the analysis phase. In the next video, we will move on to coping with heterogeneity in the data, particularly through variance structures.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#the-data-exploration-toolkit",
    "href": "draft_sections/preprocessing.html#the-data-exploration-toolkit",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.2 The Data Exploration Toolkit",
    "text": "9.2 The Data Exploration Toolkit\nAcross this section of the course, we will work through five core data exploration steps:\n\nIdentifying outliers (univariate and multivariate)\n\nIdentifying extra zeros in the data\n\nAssessing multicollinearity\n\nThinking carefully about interactions\n\nDeciding whether to standardize covariates\n\nThese steps are meant to give you a practical toolkit for diagnosing issues early. Skipping them often leads to paralysis during the modeling phase. Many of these are mistakes I have made myself over the years—and learned from the hard way.\nIn this video, we focus on Steps 1 and 2: outliers and extra zeros.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#types-of-outliers",
    "href": "draft_sections/preprocessing.html#types-of-outliers",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.3 Types of Outliers",
    "text": "9.3 Types of Outliers\nOutliers arise for many reasons, and not all outliers are errors. It is useful to think about different types:\n\n9.3.1 Univariate Outliers\nThese occur in one dimension only—a single variable. For example, a body size measurement that is larger than expected relative to the rest of the population.\nThese values may be: - Slightly larger or smaller than expected - Rare but biologically plausible - Statistically extreme, depending on assumptions\n\n\n\n9.3.2 Multivariate Outliers\nMultivariate outliers occur when a data point is unusual in combination across variables, even if it is not extreme in any single variable.\nThese are extremely common in real datasets and often more important than univariate outliers.\n\n\n\n9.3.3 Influential Observations\nThese are data points that exert disproportionate influence on a regression model. They are typically identified after modeling (e.g., via Cook’s distance).\nThis approach is common, but it is not blind to inference and can introduce bias if used carelessly. Decisions about data inclusion should not be driven by p-values.\n\n\n\n9.3.4 Measurement and Processing Errors\nOutliers may arise from:\n\nData entry errors\nSpreadsheet mistakes\nMiscommunication during field measurements\nInstrument or observer error\n\nThis is why rigorous data quality control is essential.\n\n\n\n9.3.5 Natural Oddities (“Black Swans”)\nSome outliers are real and biologically meaningful. Rare events—such as extreme climatic years or unusual individuals—may have disproportionate ecological importance.\nAutomatically removing such observations risks losing genuine scientific insight.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#identifying-univariate-outliers",
    "href": "draft_sections/preprocessing.html#identifying-univariate-outliers",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.4 Identifying Univariate Outliers",
    "text": "9.4 Identifying Univariate Outliers\nA classic definition (Moore & McCabe):\n\nAn outlier is an observation that lies outside the overall pattern of a distribution.\n\nThis definition is inherently univariate. There are two broad approaches to identifying outliers:\n\n9.4.1 Statistical Identification\nThis depends on: - Sample size - Assumptions about the underlying distribution\nWith small sample sizes, this approach can be fragile and misleading.\n\n\n\n9.4.2 Expert Judgment\nThis relies on domain knowledge, but must be applied cautiously. Appeals to authority are not substitutes for statistical reasoning, though in some cases expert knowledge is indispensable.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#graphical-approaches-to-univariate-outliers",
    "href": "draft_sections/preprocessing.html#graphical-approaches-to-univariate-outliers",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.5 Graphical Approaches to Univariate Outliers",
    "text": "9.5 Graphical Approaches to Univariate Outliers\nBefore modeling, graphical inspection provides a useful first approximation.\n\n9.5.1 Cleveland Plots (Dot Charts)\nCleveland plots rank observations and display them visually. They make no distributional assumptions and are purely exploratory.\nThey are subjective, but effective for spotting values that warrant further attention.\n\n\n\n9.5.2 Tukey Box Plots\nTukey-style box plots are widely used and based on quantiles, not normality.\nKey components:\n\nThe box spans the interquartile range (IQR):\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\nWhiskers extend to \\(1.5 \\times \\text{IQR}\\)\nPoints beyond this threshold are flagged as outliers\nPoints beyond \\(3 \\times \\text{IQR}\\) may be flagged as extreme outliers\n\nThese thresholds are arbitrary but conventional. They provide a consistent rule of thumb rather than a strict statistical test.\n\n\n\n9.5.3 Conditional Box Plots\nConditional box plots split the data by a grouping factor (e.g., month, treatment, site).\nAdvantages: - Can scale box width by sample size - Highlight where outliers originate - Useful for diagnosing heterogeneity across groups",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#caveats-with-univariate-outliers",
    "href": "draft_sections/preprocessing.html#caveats-with-univariate-outliers",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.6 Caveats with Univariate Outliers",
    "text": "9.6 Caveats with Univariate Outliers\nBe especially cautious with small sample sizes.\nFor example, data drawn from a Gamma distribution (bounded at zero, right-skewed) may produce values flagged as outliers by Tukey’s rule—even when they are perfectly consistent with the true distribution.\nOutliers identified by a rule are not necessarily statistical anomalies.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#multivariate-outliers-1",
    "href": "draft_sections/preprocessing.html#multivariate-outliers-1",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.7 Multivariate Outliers",
    "text": "9.7 Multivariate Outliers\nMost datasets are multivariate. A data point may appear normal in each variable individually, yet be extreme in multivariate space.\nThis is where univariate methods fail.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#mahalanobis-distance",
    "href": "draft_sections/preprocessing.html#mahalanobis-distance",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.8 Mahalanobis Distance",
    "text": "9.8 Mahalanobis Distance\nA powerful method for identifying multivariate outliers is Mahalanobis distance, which accounts for covariance among variables.\nConceptually, it measures how far a point is from the multivariate centroid:\n\\[\nD^2 = (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\n\\]\nIn practice, this can be implemented using existing R packages (e.g., psych) and works well for moderate-dimensional datasets.\n\n\n9.8.1 Extensions: Minimum Covariance Determinant (MCD)\nA newer variant uses a subset (e.g., 75%) of the data to estimate covariance robustly, then flags points outside that structure.\nAdvantages: - Improves model convergence - Produces stable coefficient estimates\nDisadvantages: - Can be overly aggressive in flagging outliers - Requires careful justification\nThis approach is promising but still evolving.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#what-to-do-if-you-identify-an-outlier",
    "href": "draft_sections/preprocessing.html#what-to-do-if-you-identify-an-outlier",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.9 What to Do If You Identify an Outlier",
    "text": "9.9 What to Do If You Identify an Outlier\nThere is no single correct response. Options include:\n\n9.9.1 1. Remove the Outlier (With Justification)\nRemoval must be justified using multiple lines of evidence, such as: - Field notes - Known anomalies (e.g., storms, injuries) - Instrument failure\n\n\n\n9.9.2 2. Conduct Concurrent Analyses\nRun models: - With the outlier included - With the outlier removed\nReport both results transparently. If conclusions are unchanged, the outlier has little influence.\n\n\n\n9.9.3 3. Do Nothing (Often the Best Choice)\nWith adequate sample size, individual outliers rarely matter. If one point drastically changes results, the real issue is often insufficient data.\n\n\n\n9.9.4 4. Do Not Transform the Data\nData transformation is largely obsolete in modern statistical practice. It is unnecessary for most models and can actively distort inference.\nTransformations often fail to achieve their stated goals and introduce new problems.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#ethical-and-biological-considerations",
    "href": "draft_sections/preprocessing.html#ethical-and-biological-considerations",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.10 Ethical and Biological Considerations",
    "text": "9.10 Ethical and Biological Considerations\nDo not remove outliers that are biologically meaningful:\n\nExtreme climate years\nRare but dominant individuals\nUnusual but real ecological events\n\nThese often generate the most interesting hypotheses.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#extra-zeros-in-the-data",
    "href": "draft_sections/preprocessing.html#extra-zeros-in-the-data",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.11 Extra Zeros in the Data",
    "text": "9.11 Extra Zeros in the Data\nThe second focus of this video is identifying excess zeros, which can cause serious modeling issues.\nWe are not trying to force data into normality. Instead, we want to understand whether the data reasonably approximate a distribution we intend to model.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#why-extra-zeros-matter",
    "href": "draft_sections/preprocessing.html#why-extra-zeros-matter",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.12 Why Extra Zeros Matter",
    "text": "9.12 Why Extra Zeros Matter\nExcess zeros can:\n\nPrevent model convergence\nProduce unstable or misleading coefficient estimates\n\nIf the likelihood surface is poorly defined, the estimation algorithm may fail or settle on spurious solutions.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#visual-inspection-for-zero-inflation",
    "href": "draft_sections/preprocessing.html#visual-inspection-for-zero-inflation",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.13 Visual Inspection for Zero Inflation",
    "text": "9.13 Visual Inspection for Zero Inflation\nA practical approach:\n\nPlot a histogram\nIncrease the number of bins\nZoom into the lower range (e.g., 0–10)\n\nIf the frequency at zero is much higher than expected under a Poisson or Gamma distribution, zero inflation may be present.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#quantifying-zero-inflation",
    "href": "draft_sections/preprocessing.html#quantifying-zero-inflation",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.14 Quantifying Zero Inflation",
    "text": "9.14 Quantifying Zero Inflation\nA simple diagnostic:\n\nCalculate the proportion of zeros\nIf more than ~50% of observations are zero, standard GLMs may struggle\n\nImportantly, zeros are not bad data. They often represent real biological states (e.g., non-breeders, absence).\nThey simply require the appropriate model, which we will cover later.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/preprocessing.html#wrap-up",
    "href": "draft_sections/preprocessing.html#wrap-up",
    "title": "9  Data Exploration I: Outliers and Extra Zeros",
    "section": "9.15 Wrap-Up",
    "text": "9.15 Wrap-Up\nIn this video, we covered:\n\nTypes of outliers\nGraphical and statistical tools for identifying them\nEthical considerations for handling outliers\nIdentification of excess zeros and why they matter\n\nThese steps should be completed before formal modeling. They prevent bias, reduce frustration, and lead to more defensible inference.\nIn the next section, we will move on to heterogeneity and variance structures.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "draft_sections/intro_modeling.html",
    "href": "draft_sections/intro_modeling.html",
    "title": "10  modeling",
    "section": "",
    "text": "10.1 Statistical Philosophy and Practice",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>modeling</span>"
    ]
  },
  {
    "objectID": "draft_sections/intro_modeling.html#statistical-philosophy-and-practice",
    "href": "draft_sections/intro_modeling.html#statistical-philosophy-and-practice",
    "title": "10  modeling",
    "section": "",
    "text": "10.1.1 A culture of the imperfect\n\n[Introduce aphorisms about imperfection and pragmatism]\n[Discuss why waiting for the “perfect” analysis is often counterproductive]\n\n\n\n10.1.2 Avoiding statistical traps\n\n[Describe common analytical pitfalls]\n[Emphasize critical reading of others’ analyses]\n\n\n\n10.1.3 Multiple valid paths\n\n[Introduce the idea that many analytical approaches can be defensible]\n[Connect to collaboration and intellectual humility]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>modeling</span>"
    ]
  },
  {
    "objectID": "draft_sections/intro_modeling.html#patterns-models-and-inference",
    "href": "draft_sections/intro_modeling.html#patterns-models-and-inference",
    "title": "10  modeling",
    "section": "10.2 Patterns, Models, and Inference",
    "text": "10.2 Patterns, Models, and Inference\n\n10.2.1 Detecting patterns\n\n[Describe visualization and exploratory analysis as pattern discovery]\n[Emphasize skepticism and iteration]\n\n\n\n10.2.2 Models as tools, not truths\n\n[Explain what models are (and are not)]\n[Discuss assumptions, simplifications, and abstraction]\n\n\n\n10.2.3 Inference and uncertainty\n\n[Describe uncertainty, variability, and confidence]\n[Distinguish statistical significance from scientific importance]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>modeling</span>"
    ]
  },
  {
    "objectID": "draft_sections/intro_modeling.html#limits-of-conventional-modeling",
    "href": "draft_sections/intro_modeling.html#limits-of-conventional-modeling",
    "title": "10  modeling",
    "section": "10.3 Limits of Conventional Modeling",
    "text": "10.3 Limits of Conventional Modeling\n\n10.3.1 Standard models you already know\n\n[Briefly list GLMs, GAMs, GLMMs, etc.]\n[Acknowledge their usefulness]\n\n\n\n10.3.2 Why these models are often insufficient\n\n[Discuss complexity, nonlinearity, dependence, and scale]\n[Explain mismatch between data-generating processes and model assumptions]\n\n\n\n10.3.3 What the “ideal” might look like\n\n[Pose the question of ideal inference without answering it fully]\n[Frame this as a motivating tension for the course]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>modeling</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html",
    "href": "draft_sections/glms.html",
    "title": "11  glms",
    "section": "",
    "text": "12 Chapter Overview\n[Briefly describe how this chapter moves from data hygiene and naming conventions, through exploratory data analysis, to formal statistical modeling with GLMs. Emphasize workflow and scientific reasoning.]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#learning-objectives",
    "href": "draft_sections/glms.html#learning-objectives",
    "title": "11  glms",
    "section": "12.1 Learning Objectives",
    "text": "12.1 Learning Objectives\n\n[Describe what students should be able to do after completing this chapter, spanning data management, exploration, and modeling]\n[Connect coding practices to reproducibility and statistical inference]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#why-we-need-glms",
    "href": "draft_sections/glms.html#why-we-need-glms",
    "title": "11  glms",
    "section": "13.1 Why We Need GLMs",
    "text": "13.1 Why We Need GLMs\n\n[Explain limitations of classical linear models for non-normal data]\n[Position GLMs as a unifying statistical framework]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#what-are-generalized-linear-models",
    "href": "draft_sections/glms.html#what-are-generalized-linear-models",
    "title": "11  glms",
    "section": "13.2 What Are Generalized Linear Models?",
    "text": "13.2 What Are Generalized Linear Models?\n\n[Describe the three core components: random component, systematic component, link function]\n[Explain how GLMs subsume t-tests and ANOVA]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#advantages-of-glms-in-ecological-and-field-data",
    "href": "draft_sections/glms.html#advantages-of-glms-in-ecological-and-field-data",
    "title": "11  glms",
    "section": "13.3 Advantages of GLMs in Ecological and Field Data",
    "text": "13.3 Advantages of GLMs in Ecological and Field Data\n\n[Discuss unbalanced data, natural boundaries, and autocorrelation]\n[Introduce extensions (GLMMs, GAMs, GAMMs)]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#glms-vs.-traditional-tests",
    "href": "draft_sections/glms.html#glms-vs.-traditional-tests",
    "title": "11  glms",
    "section": "13.4 GLMs vs. Traditional Tests",
    "text": "13.4 GLMs vs. Traditional Tests\n\n[Conceptually compare t-tests, ANOVA, and GLMs]\n[Emphasize equivalence under Gaussian assumptions]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#model-assumptions-revisited",
    "href": "draft_sections/glms.html#model-assumptions-revisited",
    "title": "11  glms",
    "section": "13.5 Model Assumptions Revisited",
    "text": "13.5 Model Assumptions Revisited\n\n[Clarify assumptions about residuals rather than raw data]\n[Discuss independence, linearity, and variance structure]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#choosing-the-right-distribution",
    "href": "draft_sections/glms.html#choosing-the-right-distribution",
    "title": "11  glms",
    "section": "13.6 Choosing the Right Distribution",
    "text": "13.6 Choosing the Right Distribution\n\n[Outline how response type (count, binary, proportion, continuous) drives distribution choice]\n[Introduce overdispersion and zero inflation]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#link-functions-and-interpretation",
    "href": "draft_sections/glms.html#link-functions-and-interpretation",
    "title": "11  glms",
    "section": "13.7 Link Functions and Interpretation",
    "text": "13.7 Link Functions and Interpretation\n\n[Explain the role of link functions in connecting predictors to the mean response]\n[Describe common links (log, logit, etc.) and when they are used]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#building-a-glm-initial-workflow",
    "href": "draft_sections/glms.html#building-a-glm-initial-workflow",
    "title": "11  glms",
    "section": "13.8 Building a GLM: Initial Workflow",
    "text": "13.8 Building a GLM: Initial Workflow\n\n[Describe steps from distribution choice to model specification]\n[Emphasize exploratory analysis as a prerequisite]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#assessing-normality-and-model-fit",
    "href": "draft_sections/glms.html#assessing-normality-and-model-fit",
    "title": "11  glms",
    "section": "13.9 Assessing Normality and Model Fit",
    "text": "13.9 Assessing Normality and Model Fit\n\n[Explain visual diagnostics (histograms, Q-Q plots)]\n[Introduce simulation-based diagnostics using DHARMa]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glms.html#take-home-messages-for-glms",
    "href": "draft_sections/glms.html#take-home-messages-for-glms",
    "title": "11  glms",
    "section": "13.10 Take-Home Messages for GLMs",
    "text": "13.10 Take-Home Messages for GLMs\n\n[Reinforce the idea that good models start with good questions]\n[Encourage model-based thinking over test-based thinking]",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>glms</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html",
    "href": "draft_sections/glm_poisson.html",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "",
    "text": "12.1 Overview\nWelcome back to Day 3. This is the second video on generalized linear models (GLMs), focusing specifically on models for non-normal data. In this lecture, we work through count data as a concrete case study to build intuition about model choice, diagnostics, prediction, and interpretation.\nThese ideas will be reinforced through hands-on practice in class.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#poisson-glms-for-count-data",
    "href": "draft_sections/glm_poisson.html#poisson-glms-for-count-data",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.2 Poisson GLMs for Count Data",
    "text": "12.2 Poisson GLMs for Count Data\nCount data are extremely common in ecological surveys and field studies. Whenever your response variable is a count (e.g., number of individuals, detections, or events), the Poisson distribution is often the natural starting point.\nA defining property of the Poisson distribution is:\n\nThe mean equals the variance.\n\nFormally,\n\\[\n\\mathbb{E}(Y) = \\mathrm{Var}(Y)\n\\]\nThis means the distribution is fully specified by a single parameter (the mean). Other distributions often require additional parameters to describe dispersion.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#link-functions-for-the-poisson-distribution",
    "href": "draft_sections/glm_poisson.html#link-functions-for-the-poisson-distribution",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.3 Link Functions for the Poisson Distribution",
    "text": "12.3 Link Functions for the Poisson Distribution\nSeveral link functions are available for Poisson GLMs:\n\nIdentity\n\nSquare root\n\nLog (default and most commonly used)\n\nThe log link is generally preferred because it ensures that fitted values are strictly positive, which is required for count data.\nWith a log link:\n\nThe linear predictor is on the log scale\nThe inverse link is the exponential function\nCoefficients are interpreted multiplicatively after back-transformation\n\nIf \\(\\eta\\) is the linear predictor, then the mean response is:\n\\[\n\\mu = \\exp(\\eta)\n\\]\nBack-transforming model output is critical for interpretation and prediction.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#assumptions-of-the-poisson-glm",
    "href": "draft_sections/glm_poisson.html#assumptions-of-the-poisson-glm",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.4 Assumptions of the Poisson GLM",
    "text": "12.4 Assumptions of the Poisson GLM\nFor a Poisson GLM to be appropriate, the response must satisfy:\n\\[\n\\mathrm{Var}(Y) \\approx \\mathbb{E}(Y)\n\\]\nTo evaluate this assumption, we calculate an overdispersion parameter. Ideally, this value should be close to 1.\n\nDispersion \\(&gt; 1\\): Overdispersion\nDispersion \\(&lt; 1\\): Underdispersion\n\nIn practice, overdispersion is far more common than underdispersion.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#calculating-the-dispersion-statistic",
    "href": "draft_sections/glm_poisson.html#calculating-the-dispersion-statistic",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.5 Calculating the Dispersion Statistic",
    "text": "12.5 Calculating the Dispersion Statistic\nIf dispersion is not reported automatically, it can be calculated as:\n\\[\n\\hat{c} = \\frac{\\sum r_i^2}{n - p}\n\\]\nwhere:\n\n\\(r_i\\) are Pearson residuals\n\n\\(n\\) is the number of observations\n\n\\(p\\) is the number of estimated parameters (including the intercept)\n\nFor mixed models, defining \\(p\\) becomes more complicated because the effective sample size lies between the number of observations and the number of random-effect levels.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#when-overdispersion-is-a-symptom",
    "href": "draft_sections/glm_poisson.html#when-overdispersion-is-a-symptom",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.6 When Overdispersion Is a Symptom",
    "text": "12.6 When Overdispersion Is a Symptom\nBefore assuming overdispersion is the primary issue, consider other common causes:\n\nModel misspecification (missing predictors)\nInfluential observations or outliers\nMissing interaction terms\nPredictors on inappropriate scales\nUnmodeled nonlinear relationships\nZero inflation\nUnaccounted dependency structure\n\nA standard GLM assumes independent observations with no spatial, temporal, or hierarchical structure.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#strategies-for-handling-overdispersion",
    "href": "draft_sections/glm_poisson.html#strategies-for-handling-overdispersion",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.7 Strategies for Handling Overdispersion",
    "text": "12.7 Strategies for Handling Overdispersion\n\n12.7.1 Quasi-Poisson Models\nQuasi-Poisson models adjust standard errors to account for overdispersion while leaving the mean structure unchanged. This is a technical fix and can work in some cases, but it is not always ideal.\n\n\n12.7.2 Negative Binomial Models (Preferred)\nThe negative binomial distribution includes two parameters:\n\nMean\nDispersion parameter\n\nThis allows the variance to exceed the mean:\n\\[\n\\mathrm{Var}(Y) = \\mu + \\alpha \\mu^2\n\\]\nwhere \\(\\alpha\\) controls overdispersion. This makes negative binomial GLMs a robust choice for overdispersed count data.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#why-distribution-choice-matters",
    "href": "draft_sections/glm_poisson.html#why-distribution-choice-matters",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.8 Why Distribution Choice Matters",
    "text": "12.8 Why Distribution Choice Matters\nAt large mean values, a Poisson distribution can visually resemble a normal distribution. However, the underlying assumptions remain different.\nIf variance is mischaracterized:\n\nStandard errors will be incorrect\nConfidence intervals will be misleading\np-values may be invalid\n\nA good decision workflow is:\n\nIs the response discrete or continuous?\nWhat are the bounds of the data?\nDoes variance increase with the mean?",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#example-dataset-fish-abundance-and-water-depth",
    "href": "draft_sections/glm_poisson.html#example-dataset-fish-abundance-and-water-depth",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.9 Example Dataset: Fish Abundance and Water Depth",
    "text": "12.9 Example Dataset: Fish Abundance and Water Depth\nWe now consider a dataset designed to address the question:\n\nHas the relationship between water depth and total fish abundance changed over time?\n\nBefore modeling, we:\n\nInspect variable structure\nRemove a spatial outlier (for teaching purposes)\nRescale depth for interpretability\n\nAlways examine your data first, but avoid making inferential claims from exploratory plots alone.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#ordinary-least-squares-as-a-baseline",
    "href": "draft_sections/glm_poisson.html#ordinary-least-squares-as-a-baseline",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.10 Ordinary Least Squares as a Baseline",
    "text": "12.10 Ordinary Least Squares as a Baseline\nWe begin with a Gaussian linear model:\n\\[\n\\text{Total abundance} \\sim \\text{Mean depth}\n\\]\nThe model reports a strong effect of depth. However, residual diagnostics reveal major violations:\n\nHeteroskedasticity\nSkewed residuals\nNon-normality\nInfluential observations\n\nQ–Q plots confirm these violations. This model is not appropriate for count data.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#residuals-in-glms",
    "href": "draft_sections/glm_poisson.html#residuals-in-glms",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.11 Residuals in GLMs",
    "text": "12.11 Residuals in GLMs\nIn GLMs, raw residuals (observed minus fitted) are not useful. Instead, use:\n\nPearson residuals\nDeviance residuals\n\nThese should be plotted against:\n\nFitted values\nEach predictor\nTime\nSpace\n\nResiduals should show no systematic structure across any variable.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#fitting-a-poisson-glm",
    "href": "draft_sections/glm_poisson.html#fitting-a-poisson-glm",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.12 Fitting a Poisson GLM",
    "text": "12.12 Fitting a Poisson GLM\nWe refit the model using a Poisson GLM with a log link:\n\\[\n\\log(\\mu_i) = \\beta_0 + \\beta_1 \\text{Depth}_i\n\\]\nThe model explains approximately 43% of the deviance:\n\\[\nR^2_{\\text{pseudo}} = \\frac{D_\\text{null} - D_\\text{residual}}{D_\\text{null}}\n\\]\nHowever, diagnostics reveal extreme overdispersion.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#explicit-overdispersion-diagnosis",
    "href": "draft_sections/glm_poisson.html#explicit-overdispersion-diagnosis",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.13 Explicit Overdispersion Diagnosis",
    "text": "12.13 Explicit Overdispersion Diagnosis\nUsing Pearson residuals, we compute:\n\\[\n\\hat{c} \\approx 115\n\\]\nThis indicates severe overdispersion and confirms that the Poisson model is mis-specified.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#model-refinement-attempts",
    "href": "draft_sections/glm_poisson.html#model-refinement-attempts",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.14 Model Refinement Attempts",
    "text": "12.14 Model Refinement Attempts\nWe explore several refinements:\n\nCook’s distance reveals many influential points\nAdding sampling period as a covariate improves fit marginally\nAdding an offset for sampling effort worsens dispersion\n\nEach step provides diagnostic information but does not resolve the issue.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#negative-binomial-glm",
    "href": "draft_sections/glm_poisson.html#negative-binomial-glm",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.15 Negative Binomial GLM",
    "text": "12.15 Negative Binomial GLM\nWe refit the same model using a negative binomial GLM:\n\\[\n\\log(\\mu_i) = \\beta_0 + \\beta_1 \\text{Depth}_i + \\beta_2 \\text{Period}_i\n\\]\nThis dramatically improves the model:\n\nDispersion \\(\\approx 1\\)\nResiduals stabilize\nCook’s distance values drop\nAIC strongly favors the negative binomial model",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#interpreting-the-final-model",
    "href": "draft_sections/glm_poisson.html#interpreting-the-final-model",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.16 Interpreting the Final Model",
    "text": "12.16 Interpreting the Final Model\nThe depth–abundance relationship is similar across periods. However, claims about similarity require explicit comparison between:\n\nAdditive models\nInteraction models\n\nOnly after comparing these models can we infer whether relationships truly differ.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#prediction-and-back-transformation",
    "href": "draft_sections/glm_poisson.html#prediction-and-back-transformation",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.17 Prediction and Back-Transformation",
    "text": "12.17 Prediction and Back-Transformation\nWhen predicting from GLMs:\n\nAlways use type = \"link\"\nRequest standard errors\nCompute confidence intervals on the link scale\nBack-transform using the inverse link\n\nFor a log link, predictions are back-transformed as:\n\\[\n\\hat{\\mu} = \\exp(\\hat{\\eta})\n\\]\nConfidence intervals are:\n\\[\n\\exp(\\hat{\\eta} \\pm 1.96 \\cdot \\text{SE})\n\\]\nThis produces asymmetric intervals appropriate for count data.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#a-cautionary-example",
    "href": "draft_sections/glm_poisson.html#a-cautionary-example",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.18 A Cautionary Example",
    "text": "12.18 A Cautionary Example\nIn a published study of reproductive success, predictions were plotted on the link scale, leading to biologically impossible negative values. The model itself was correct—the error occurred during prediction and visualization.\nThis mistake is common and avoidable.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#key-take-home-messages",
    "href": "draft_sections/glm_poisson.html#key-take-home-messages",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.19 Key Take-Home Messages",
    "text": "12.19 Key Take-Home Messages\n\nUse Poisson GLMs for count data whenever appropriate\nAlways check for zero inflation and overdispersion\nPrefer negative binomial models when overdispersion is present\nNever extrapolate beyond observed data ranges\nDefault to type = \"link\" when predicting\nAlways back-transform predictions and confidence intervals",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/glm_poisson.html#wrap-up",
    "href": "draft_sections/glm_poisson.html#wrap-up",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.20 Wrap-Up",
    "text": "12.20 Wrap-Up\nThis concludes Day 3. In Day 4, we will continue building on these ideas and extend them to more complex modeling frameworks.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "draft_sections/housekeeping.html",
    "href": "draft_sections/housekeeping.html",
    "title": "3  housekeeping",
    "section": "",
    "text": "3.1 Goals this week\nThis week, you will be exposed to (or reminded about?!) some ways of organizing file structures for creating a reproducible analysis. Note that other courses here at UW cover the concepts of reproducible analytical workflows –i.e. the use of GitHub, formalized workflows, metadata, and repositories– in great detail. This course abbreviates some of those concepts and best practices, doing just enough to allow us to focus on statistical modeling. Feel free to apply what you have learned in other courses.\nDownload and install R and RStudio: This is just a reminder to make sure everything is working. Set up a project. You can do this on your own (in RStudio, create new project in an existing directory). Set up a file structure in your RStudio project. Import your dataset: Just as a reminder, you should already have an organized dataset (either one or more files). If these files are already in .csv format, keep them as such, as this is a widely recognized file format that ensures data long-term stability (.xlsx and such formats are not). These files will be placed in your Google Drive analysis folder. Data should be long (rather than wide) format, but don’t worry about this distinction if you don’t want to bother with it at this point.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>housekeeping</span>"
    ]
  },
  {
    "objectID": "draft_sections/housekeeping.html#goals-this-week",
    "href": "draft_sections/housekeeping.html#goals-this-week",
    "title": "3  housekeeping",
    "section": "",
    "text": "Week\nTopics\nIn-class exercise\nMilestone assignment\nStress level\nGrade weight (%)\n\n\n\n\n1\nCourse intro; RStudio setup; AI guardrails\nProject scoping discussion\nAnalysis Concept Note\nVery low\n5\n\n\n2\nFile structure; reproducible workflows; AI logs\nFile audit walkthrough\nAnalysis Concept Note\nLow\n5\n\n\n3\nData exploration; measurement & uncertainty\nEDA sketching\nData Readiness Note\nVery low\n0\n\n\n4\nGLMs (theory + practice)\nGLM exercise\nData Readiness Note\nLow\n10\n\n\n5\nGLMs; AIC & information theory\nModel comparison exercise\n—\nLow\n5\n\n\n6\nGLMMs; effective sample size\nHierarchical model exercise\nWorking Model\nModerate\n15\n\n\n7\nGAMs\nSmooths & diagnostics exercise\nWorking Model\nModerate\n0\n\n\n8\nGAMMs; spatial & temporal heterogeneity\nModel refinement\nWorking Model (final lock)\nModerate\n15\n\n\n9\nStudent Spring Break\n—\n—\nNone\n0\n\n\n10\nStructural Causal Modeling (SCM)\nCausal diagram critique\nInterpretation Memo\nLow\n10\n\n\n11\nInstructor Spring Break\n—\n—\nNone\n0\n\n\n12\nPrediction & uncertainty\nPrediction checks\nInterpretation Memo\nLow\n10\n\n\n13\nSynthesis & justification\nPeer + AI review\nResults Section\nModerate\n0\n\n\n14\nTables, figures, reporting standards\nTable/figure workshop\nResults Section\nLow\n10\n\n\n15\nWriting Results sections\nDraft clinic\nFull Draft\nModerate\n15\n\n\n16\nReflection & closure\nCourse wrap-up\nRevision Plan (not executed)\nNone\n5",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>housekeeping</span>"
    ]
  },
  {
    "objectID": "draft_sections/housekeeping.html#course-calendar",
    "href": "draft_sections/housekeeping.html#course-calendar",
    "title": "3  housekeeping",
    "section": "3.2 Course Calendar",
    "text": "3.2 Course Calendar\n\n\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nIn-class exercise\nMilestone assignment\nStress level\nGrade weight (%)\n\n\n\n\n1\nCourse intro; RStudio setup; AI guardrails\nProject scoping discussion\nAnalysis Concept Note\nVery low\n5\n\n\n2\nFile structure; reproducible workflows; AI logs\nFile audit walkthrough\nAnalysis Concept Note\nLow\n5\n\n\n3\nData exploration; measurement & uncertainty\nEDA sketching\nData Readiness Note\nVery low\n0\n\n\n4\nGLMs (theory + practice)\nGLM exercise\nData Readiness Note\nLow\n10\n\n\n5\nGLMs; AIC & information theory\nModel comparison exercise\n—\nLow\n5\n\n\n6\nGLMMs; effective sample size\nHierarchical model exercise\nWorking Model\nModerate\n15\n\n\n7\nGAMs\nSmooths & diagnostics exercise\nWorking Model\nModerate\n0\n\n\n8\nGAMMs; spatial & temporal heterogeneity\nModel refinement\nWorking Model (final lock)\nModerate\n15\n\n\n9\nStudent Spring Break\n—\n—\nNone\n0\n\n\n10\nStructural Causal Modeling (SCM)\nCausal diagram critique\nInterpretation Memo\nLow\n10\n\n\n11\nInstructor Spring Break\n—\n—\nNone\n0\n\n\n12\nPrediction & uncertainty\nPrediction checks\nInterpretation Memo\nLow\n10\n\n\n13\nSynthesis & justification\nPeer + AI review\nResults Section\nModerate\n0\n\n\n14\nTables, figures, reporting standards\nTable/figure workshop\nResults Section\nLow\n10\n\n\n15\nWriting Results sections\nDraft clinic\nFull Draft\nModerate\n15\n\n\n16\nReflection & closure\nCourse wrap-up\nRevision Plan (not executed)\nNone\n5",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>housekeeping</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_course_intro.html",
    "href": "draft_sections/01_course_intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Welcome to Quantitative Analysis of (Messy) Field Data!\nThe central goal of this course is to guide you through the transition:\nWorking through this challenging analytical landscape will not only teach you about statistics (and some statistical traps), but it will give you a concrete opportunityto work with your peers as you delve into the myriad issues that inevitably arise in data analysis. Learning how to navigate unfamiliar analytical territory is a skill that is critical for every developing researchers.\nThat said, we must acknowledge that everyone assimilates new material and produces syntheses in varied ways. All I ask is that each of you make use of this class to make significant progress towards completion of one of your thesis or dissertation chapters. You can choose a single analysis if that is all that you need or you can include 4-5 shorter analyses if that is what you need to make progress. This is an focused opportunity for you to get more help with data analysis, so take advantage.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_course_intro.html#why-this-course",
    "href": "draft_sections/01_course_intro.html#why-this-course",
    "title": "2  Course Introduction",
    "section": "2.2 Why this course?",
    "text": "2.2 Why this course?\nThe benefit of this course is to not only teach you about statistics (and some statistical traps), but to have you working with your peers as you delve into the issues that inevitably arise with your own data analysis. Learning how to navigate data analysis issues is a skill that is critical for developing researchers. Please try to make this class as useful as possible for you and include most or all of the analyses that you expect will go into one of your dissertation chapters. You can just include one analysis if that is all that you need or you can include 4-5 analyses if that is what you need to answer your question(s). This is an opportunity for you to get more help with data analysis, so take advantage.\nThe central goal of this course is to guide you through the transition:\n\nMessy field or lab data → processed data → models → inference",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_course_intro.html#meetings-and-expectations",
    "href": "draft_sections/01_course_intro.html#meetings-and-expectations",
    "title": "2  Course Introduction",
    "section": "2.3 Meetings and Expectations",
    "text": "2.3 Meetings and Expectations\nOn our first in-person meeting, we will: - do brief class introductions\n- discuss statistical philosophy\n- go through example problems\n- talk about common analytical pitfalls to avoid\nThis assumes you have watched or skimmed the asynchronous videos beforehand.\nThis course is designed to be cumulative and interactive—preparation matters.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_calendar.html",
    "href": "draft_sections/04_calendar.html",
    "title": "5  calendar",
    "section": "",
    "text": "5.1 Course Calendar",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>calendar</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_calendar.html#course-calendar",
    "href": "draft_sections/04_calendar.html#course-calendar",
    "title": "5  calendar",
    "section": "",
    "text": "Week\nTopics\nIn-class exercise\nMilestone assignment\nStress level\nGrade weight (%)\n\n\n\n\n1\nCourse intro; RStudio setup; AI guardrails\nProject scoping discussion\nAnalysis Concept Note\nVery low\n5\n\n\n2\nFile structure; reproducible workflows; AI logs\nFile audit walkthrough\nAnalysis Concept Note\nLow\n5\n\n\n3\nData exploration; measurement & uncertainty\nEDA sketching\nData Readiness Note\nVery low\n0\n\n\n4\nGLMs (theory + practice)\nGLM exercise\nData Readiness Note\nLow\n10\n\n\n5\nGLMs; AIC & information theory\nModel comparison exercise\n—\nLow\n5\n\n\n6\nGLMMs; effective sample size\nHierarchical model exercise\nWorking Model\nModerate\n15\n\n\n7\nGAMs\nSmooths & diagnostics exercise\nWorking Model\nModerate\n0\n\n\n8\nGAMMs; spatial & temporal heterogeneity\nModel refinement\nWorking Model (final lock)\nModerate\n15\n\n\n9\nStudent Spring Break\n—\n—\nNone\n0\n\n\n10\nStructural Causal Modeling (SCM)\nCausal diagram critique\nInterpretation Memo\nLow\n10\n\n\n11\nInstructor Spring Break\n—\n—\nNone\n0\n\n\n12\nPrediction & uncertainty\nPrediction checks\nInterpretation Memo\nLow\n10\n\n\n13\nSynthesis & justification\nPeer + AI review\nResults Section\nModerate\n0\n\n\n14\nTables, figures, reporting standards\nTable/figure workshop\nResults Section\nLow\n10\n\n\n15\nWriting Results sections\nDraft clinic\nFull Draft\nModerate\n15\n\n\n16\nReflection & closure\nCourse wrap-up\nRevision Plan (not executed)\nNone\n5",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>calendar</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_course_intro.html#course-objectives",
    "href": "draft_sections/01_course_intro.html#course-objectives",
    "title": "2  Introduction",
    "section": "2.2 Course Objectives",
    "text": "2.2 Course Objectives\nBy the end of this course, you should be able to:\n\nThink quantitatively about messy ecological data. You should be on your way to developing a habit of translating real-world ecological questions into quantitative frameworks, while explicitly acknowledging uncertainty and bias in estimation and measurement.\nDistinguish between questions/hypotheses, data, and inferences. You should be able to (1) clearly separate ecological hypotheses, the data actually collected, and the quantities being estimated, and (2) understand why these distinctions matter for interpretation and decision-making.\nUnderstand the processes that generate data. You should be able to recognize how different study designs, variation in detection, observer effects, instrument error, and data processing shape and constrain the structure of ecological datasets.\nSelect, build, and critique statistical models as scientific tools. You should be comfortable choosing and using statistical models not as black boxes, but as explicit representations of assumptions about ecological processes, variation, and causal structure. You should never blindly choose a statistical model again!\nInterpret results in ecological —not just statistical— terms. You should be able to conceptually move beyond p-values and coefficients to clearly articulate what your results mean biologically, mechanistically, and practically.\nBe comfortable applying causal –and not just correlational– reasoning. You should be able to use causal thinking (conceptual models and directed acyclic graphs) to evaluate what can —and ehat cannot— be inferred from observational and experimental data.\nCommunicate quantitative results clearly, transparently, and honestl.y Present analyses, figures, and conclusions in ways that are transparent, reproducible, and appropriate for scientific audiences.\nDevelop durable, reproducible analytical workflows. You should be increasingly comfortable with good data-science practices that support clarity, versioning (even though we will not delve into GitHub this term), and reusability of analyses.\nDevelop new confidence working with unfamiliar, complex, imperfect datasets. You should leave this course better prepared to engage with real ecological data —without expecting it to be clean, complete, or simple.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/data_exploration.html#data-exploration",
    "href": "draft_sections/data_exploration.html#data-exploration",
    "title": "8  Data exploration",
    "section": "",
    "text": "NoteA note on Bayesian methods\n\n\n\n\n\nBayesian approaches are increasingly common and powerful, and this may change over the next 5–10 years. However, the foundational ideas about data structure, scale, and modeling assumptions are shared across frameworks.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_course_intro.html#welcome-to-quantitative-analysis-of-messy-field-data",
    "href": "draft_sections/01_course_intro.html#welcome-to-quantitative-analysis-of-messy-field-data",
    "title": "2  Introduction",
    "section": "",
    "text": "Messy field or lab data → processed data → models → inference",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/01_course_intro.html#what-is-new-this-term-spring-2026",
    "href": "draft_sections/01_course_intro.html#what-is-new-this-term-spring-2026",
    "title": "2  Introduction",
    "section": "2.3 What is new this term (Spring 2026)?",
    "text": "2.3 What is new this term (Spring 2026)?\nFor Spring 2026, I have added attention to new topics that improve both efficiency of analysis workflows and our ability to understand causality at a deeper level. Specifically, I have chosen to add course components on:\n\nUsing large-language models (LLMs) –via ChatGPT– to improve analytical workflows. This term, I have integrated ChatGPT into the course as a way to support the process of analysis rather than to automate the analysis itself. Many of the hardest parts of quantitative work happen outside of writing code: figuring out why R code isn’t behaving as expected, checking whether an interpretation actually follows from a model, troubleshooting an obscure error message, or getting feedback on how tp clearly document a decision. Used carefully, ChatGPT can act like a sounding board for troubleshooting, sanity-checking, and refining explanations without taking over your critical thinking. Throughout the course, I have deliberately constrained ChatGPT’s role and scope and have tailored its guardrails to match the course progression. In other words, ChatGPT is used to help improve analytical workflow and decision-making, not to generate results or write code on a student’s behalf. We will work together to assess the utility of this tool as the term advances.\nUsing large-language models (LLMs) to improve teaching at scale. What does this mean? In past terms, between 10-15 students have been enrolled in this course. This term, I have allowed 26 students to enroll. Without a Teaching Assistant, this spawns a significant issue of scale. This is solvable if we use AI as a helpful tool. So, after spending some time as an LLM-consultant and tester, I decided to explicitly integrate chatGPT’s LLMs into my teaching/grading workflow. This means that students will use chatGPT to run checks on their work prior to submission; this will solve some of the “tuning” issues (with codes, grammar, clarity, etc.) that I have seen in student submissions in previous years.\nStructural Causal Models (SCMs) I added Structural Causal Models (SCMs) at the end of the course to give us a way to think more clearly about complex systems where multiple variables influence each other at the same time. In the first part of the course, students will see the limits of fitting separate models (e.g. GLMs, GLMMs, GAMMs) for singular response variables; those models can work individually, but they often miss shared drivers, indirect effects, or how pieces of the complex system of exogenous and endogenous factors fit together. SCMs give us a way to lay out those relationships explicitly and analyze them as a connected structure (as is present in natural systems) rather than as a collection of isolated models. The goal isn’t to replace the modeling approaches we’ve already used but to improve inference by making our assumptions about the system clear and coherent.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html",
    "href": "draft_sections/ai_analytica.html",
    "title": "5  GenAI in Science",
    "section": "",
    "text": "5.1 Responsible use of generative artificial intelligence (GenAI) in scientific analysis: a framework for guard rails and documentation",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#purpose",
    "href": "draft_sections/ai_analytica.html#purpose",
    "title": "5  GenAI in Science",
    "section": "5.3 Purpose",
    "text": "5.3 Purpose\nThis document establishes principles, guard rails, and documentation practices for the responsible use of generative artificial intelligence (GenAI) tools in scientific research and analysis. One such GenAI tool that most of us are familiar with is ChatGPT (Generative Pre-trained Transformer), a GenAI model that can detect (and arguably understand) and generate human-like text by predicting what comes next in a sentence. Large Language Models (LLMs) are very advanced GenAI systems trained on absolutely massive amounts of text to answer complex questions, write in certain rhetorical tones, summarize information, or have conversations in natural language. Given this tool’s widespread utility, it can certainly be misused. Therefore, the guidelines below attempt to ensure that GenAI augments human reasoning without compromising scientific validity, reproducibility, or accountability. In all aspects of the present academic work, GenAI tools like ChatGPT are therefore treated as assistive tools –comparable to statistical software (like R) or calculators– not as autonomous analysts or skilled authors.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#core-principles",
    "href": "draft_sections/ai_analytica.html#core-principles",
    "title": "5  GenAI in Science",
    "section": "5.3 Core Principles",
    "text": "5.3 Core Principles\n\nAccountability: The human researcher retains full responsibility for –and is held accountable for– all analytical decisions, interpretations, text and code, and inference. The use of GenAI does not relieve any burdens of authorship, responsibility, or liability. Importantly, this means that the human researcher is allowed –and, in most cases, is encouraged– to use AI-produced code, as long as the research has vetted and error-checked. This likewise assumes that the researcher accepts responsibility for any and all errors arising from AI assistance. That is, GenAI may accelerate work, but it may not replace understanding. If a researcher cannot defend a decision without the GenAI present, the decision is invalid.\nScientific Primacy: Scientific reasoning precedes and constrains GenAI use. Hypotheses, data-generating assumptions, and model structures must be defined by the researcher. GenAI may clarify or critique these decisions but may not create them from scratch without human justification.\nTransparency: All GenAI use must be documented in a way that allows another scientist to understand how GenAI influenced the work. GenAI-assisted reasoning must be distinguishable from original analysis.\nReproducibility: All results must be reproducible without access to GenAI tools. Data, code, and associated documentation must be sufficient to reproduce results independently. GenAI may assist development but must not be a hidden dependency.\nProportionality: The level of documentation naturally should be proportional to the influence of GenAI. Minor stylistic assistance requires minimal logging, while conceptual or analytical assistance requires a larger volume of explicit documentation and justification.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#transparency",
    "href": "draft_sections/ai_analytica.html#transparency",
    "title": "5  Science GPTs",
    "section": "5.4 Transparency",
    "text": "5.4 Transparency\nAll AI use must be documented in a way that allows another scientist to understand how AI influenced the work. AI-assisted reasoning must be distinguishable from original analysis.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#reproducibility",
    "href": "draft_sections/ai_analytica.html#reproducibility",
    "title": "5  Science GPTs",
    "section": "5.5 Reproducibility",
    "text": "5.5 Reproducibility\nAll results must be reproducible without access to AI tools. Data, code, and associated documentation must be sufficient to reproduce results independently. AI may assist development but must not be a hidden dependency.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#proportionality",
    "href": "draft_sections/ai_analytica.html#proportionality",
    "title": "5  Science GPTs",
    "section": "5.6 Proportionality",
    "text": "5.6 Proportionality\nThe level of documentation naturally should be proportional to the influence of AI. Minor stylistic assistance requires minimal logging, while conceptual or analytical assistance requires a larger volume of explicit documentation and justification.\n\n5.6.1 Permitted Uses of AI in Scientific Work\n\nConceptual Clarification: AI may be used to explain statistical concepts, examine modeling assumptions, and interpret model diagnostics (at a broad level).\nPlanning and Reflection: AI may assist with refining research questions, generating assumption checklists, stress-testing interpretations, and identifying alternative explanations that are then vetted for ecological sanity by the human researcher.\nWriting Support: AI may be used to improve clarity, organization, and tone, and to identify points of ambiguity or inferential overreach. AI absolutely must not invent methods, results, or citations.\nCode Understanding: AI may explain what existing code does, diagnose warnings or errors conceptually, and suggest stylistic or reproducibility improvements. AI may not replace independent code comprehension.\n\n\n\nClick to expand\nThis is a green collapsible box.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#permitted-uses-of-ai-in-scientific-work",
    "href": "draft_sections/ai_analytica.html#permitted-uses-of-ai-in-scientific-work",
    "title": "5  Science GPTs",
    "section": "5.7 Permitted Uses of AI in Scientific Work",
    "text": "5.7 Permitted Uses of AI in Scientific Work",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#conceptual-clarification",
    "href": "draft_sections/ai_analytica.html#conceptual-clarification",
    "title": "5  Science GPTs",
    "section": "5.8 Conceptual Clarification",
    "text": "5.8 Conceptual Clarification\nAI may be used to explain statistical concepts, examine modeling assumptions, interpret model diagnostics at a high level, and identify missing assumptions or caveats.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#planning-and-reflection",
    "href": "draft_sections/ai_analytica.html#planning-and-reflection",
    "title": "5  Science GPTs",
    "section": "5.9 Planning and Reflection",
    "text": "5.9 Planning and Reflection\nAI may assist with refining research questions, generating assumption checklists, stress-testing interpretations, and identifying alternative explanations that are then vetted for ecological sanity by the human researcher.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#writing-support",
    "href": "draft_sections/ai_analytica.html#writing-support",
    "title": "5  Science GPTs",
    "section": "5.10 Writing Support",
    "text": "5.10 Writing Support\nAI may be used to improve clarity, organization, and tone, and to identify points of ambiguity or inferential overreach. AI absolutely must not invent methods, results, or citations.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#code-understanding",
    "href": "draft_sections/ai_analytica.html#code-understanding",
    "title": "5  Science GPTs",
    "section": "5.11 Code Understanding",
    "text": "5.11 Code Understanding\nAI may explain what existing code does, diagnose warnings or errors conceptually, and suggest stylistic or reproducibility improvements. AI may not replace independent code comprehension.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#replacement-of-scientific-judgment",
    "href": "draft_sections/ai_analytica.html#replacement-of-scientific-judgment",
    "title": "5  Science GPTs",
    "section": "6.1 Replacement of Scientific Judgment",
    "text": "6.1 Replacement of Scientific Judgment\nAI must not be used to select models, error distributions, priors, or random-effects structures without independent human justification, nor to interpret results without verification.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#undocumented-analysis-generation",
    "href": "draft_sections/ai_analytica.html#undocumented-analysis-generation",
    "title": "5  Science GPTs",
    "section": "6.2 Undocumented Analysis Generation",
    "text": "6.2 Undocumented Analysis Generation\nAI must not generate full end-to-end analytical pipelines without explanation, or produce black-box code whose logic is not understood by the researcher.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#fabrication",
    "href": "draft_sections/ai_analytica.html#fabrication",
    "title": "5  Science GPTs",
    "section": "6.3 Fabrication",
    "text": "6.3 Fabrication\nAI must not be used to invent data, methods, results, or citations, or to create post-hoc justifications unsupported by the analysis.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#outcome-optimization",
    "href": "draft_sections/ai_analytica.html#outcome-optimization",
    "title": "5  Science GPTs",
    "section": "6.4 Outcome Optimization",
    "text": "6.4 Outcome Optimization\nAI must not be used to iteratively prompt for statistically significant results, improved AIC values, or exaggerated certainty.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Science GPTs</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#purpose-of-the-log",
    "href": "draft_sections/ai_analytica.html#purpose-of-the-log",
    "title": "5  GenAI in Science",
    "section": "5.6 Purpose of the Log",
    "text": "5.6 Purpose of the Log\nResearchers maintain an AI Interaction Log alongside their analysis notebook to document meaningful AI involvement in the research process.\nRequired Information: Entries record the date, GenAI tool used, purpose of interaction, nature of assistance provided, key takeaway, and the verification step performed.\nWhen Logging Is Required: Logging is required when GenAI influences model choice, interpretation, methodological justification, or scientific claims. Minor stylistic or grammatical use does not require detailed logging.\nModel Defense and Decision Documentation: For each major analytical decision, the researcher documents the decision made, the scientific rationale, alternative options considered, the reason alternatives were rejected, and whether GenAI input was used. This documentation may appear in a Methods section, lab notebook, or decision appendix.\nVerification Obligations: Any GenAI-suggested content must be independently verified using primary literature, software documentation, diagnostic checks, or independent reasoning. Unverified AI output must not appear in final analyses.\nAuthorship and Attribution: GenAI tools are not authors and do not receive citation credit. If required by journals or funders, GenAI use may be acknowledged in a neutral Methods or Acknowledgments statement.\nEthical Safeguards: GenAI use must not obscure uncertainty, inflate confidence, reduce methodological transparency, or disadvantage collaborators or students with limited access to AI tools.\nPeriodic Review: This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#required-information",
    "href": "draft_sections/ai_analytica.html#required-information",
    "title": "5  GenAI in Science",
    "section": "6.2 Required Information",
    "text": "6.2 Required Information\nEntries record the date, AI tool used, purpose of interaction, nature of assistance provided, key takeaway, and the verification step performed.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#when-logging-is-required",
    "href": "draft_sections/ai_analytica.html#when-logging-is-required",
    "title": "5  GenAI in Science",
    "section": "6.3 When Logging Is Required",
    "text": "6.3 When Logging Is Required\nLogging is required when AI influences model choice, interpretation, methodological justification, or scientific claims. Minor stylistic or grammatical use does not require detailed logging.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#model-defense-and-decision-documentation",
    "href": "draft_sections/ai_analytica.html#model-defense-and-decision-documentation",
    "title": "5  GenAI in Science",
    "section": "6.4 Model Defense and Decision Documentation",
    "text": "6.4 Model Defense and Decision Documentation\nFor each major analytical decision, the researcher documents the decision made, the scientific rationale, alternative options considered, the reason alternatives were rejected, and whether AI input was used. This documentation may appear in a Methods section, lab notebook, or decision appendix.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#verification-obligations",
    "href": "draft_sections/ai_analytica.html#verification-obligations",
    "title": "5  GenAI in Science",
    "section": "6.5 Verification Obligations",
    "text": "6.5 Verification Obligations\nAny AI-suggested content must be independently verified using primary literature, software documentation, diagnostic checks, or independent reasoning. Unverified AI output must not appear in final analyses.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#what-is-allowed-and-what-is-not-allowed",
    "href": "draft_sections/ai_analytica.html#what-is-allowed-and-what-is-not-allowed",
    "title": "5  GenAI in Science",
    "section": "5.5 What is allowed, and what is not allowed?",
    "text": "5.5 What is allowed, and what is not allowed?\n\nPermitted Uses of GPTs in Scientific Work\n\nConceptual Clarification: GenAI may be used to explain statistical concepts, examine modeling assumptions, and interpret model diagnostics (at a broad level).\nPlanning and Reflection: GenAI may assist with refining research questions, generating assumption checklists, stress-testing interpretations, and identifying alternative explanations that are then vetted for ecological sanity by the human researcher.\nWriting Support: GenAI may be used to improve clarity, organization, and tone, and to identify points of ambiguity or inferential overreach. AI absolutely must not invent methods, results, or citations.\nCode Understanding: GenAI may explain what existing code does, diagnose warnings or errors conceptually, and suggest stylistic or reproducibility improvements. AI may not replace independent code comprehension.\n\n\n\n\nProhibited or Restricted Uses of GenAIin Scientific Work\n\nReplacement of Scientific Judgment: GenAI must not be used to select models, error distributions, priors, or random-effects structures without independent human justification, nor to interpret results without verification.\nUndocumented Analysis Generation: GenAI must not generate full end-to-end analytical pipelines without explanation, or produce black-box code whose logic is not understood by the researcher.\nFabrication: GenAI must not be used to invent data, methods, results, or citations, or to create post-hoc justifications unsupported by the analysis.\nOutcome Optimization: GenAI must not be used to iteratively prompt for statistically significant results, improved AIC values, or exaggerated certainty.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#core-principles-of-responsible-use-of-genai",
    "href": "draft_sections/ai_analytica.html#core-principles-of-responsible-use-of-genai",
    "title": "5  GenAI in Science",
    "section": "5.4 Core principles of responsible use of GenAI",
    "text": "5.4 Core principles of responsible use of GenAI\n\nAccountability: The human researcher retains full responsibility for –and is held accountable for– all analytical decisions, interpretations, text and code, and inference. The use of GenAI does not relieve any burdens of authorship, responsibility, or liability. Importantly, this means that the human researcher is allowed –and, in most cases, is encouraged– to use AI-produced code, as long as the research has vetted and error-checked. This likewise assumes that the researcher accepts responsibility for any and all errors arising from AI assistance. That is, GenAI may accelerate work, but it may not replace understanding. If a researcher cannot defend a decision without the GenAI present, the decision is invalid.\nScientific Primacy: Scientific reasoning precedes and constrains GenAI use. Hypotheses, data-generating assumptions, and model structures must be defined by the researcher. GenAI may clarify or critique these decisions but may not create them from scratch without human justification.\nTransparency: All GenAI use must be documented in a way that allows another scientist to understand how GenAI influenced the work. GenAI-assisted reasoning must be distinguishable from original analysis.\nReproducibility: All results must be reproducible without access to GenAI tools. Data, code, and associated documentation must be sufficient to reproduce results independently. GenAI may assist development but must not be a hidden dependency.\nProportionality: The level of documentation naturally should be proportional to the influence of GenAI. Minor stylistic assistance requires minimal logging, while conceptual or analytical assistance requires a larger volume of explicit documentation and justification.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#genai-interaction-log",
    "href": "draft_sections/ai_analytica.html#genai-interaction-log",
    "title": "5  GenAI in Science",
    "section": "5.6 GenAI Interaction Log",
    "text": "5.6 GenAI Interaction Log\nWhat is a GenAI Interaction Log?\n\n5.6.1 Purpose of the Log\nResearchers maintain an AI Interaction Log alongside their analysis notebook to document meaningful AI involvement in the research process.\nRequired Information: Entries record the date, GenAI tool used, purpose of interaction, nature of assistance provided, key takeaway, and the verification step performed.\nWhen Logging Is Required: Logging is required when GenAI influences model choice, interpretation, methodological justification, or scientific claims. Minor stylistic or grammatical use does not require detailed logging.\nVerification Obligations: Any GenAI-suggested content must be independently verified using primary literature, software documentation, diagnostic checks, or independent reasoning. Unverified AI output must not appear in final analyses.\nAuthorship and Attribution: GenAI tools are not authors and do not receive citation credit. If required by journals or funders, GenAI use may be acknowledged in a neutral Methods or Acknowledgments statement.\nEthical Safeguards: GenAI use must not obscure uncertainty, inflate confidence, reduce methodological transparency, or disadvantage collaborators or students with limited access to AI tools.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_syllabus.html#assessment-and-grading-standards",
    "href": "draft_sections/03_syllabus.html#assessment-and-grading-standards",
    "title": "4  Location and meeting times",
    "section": "4.3 Assessment and Grading Standards",
    "text": "4.3 Assessment and Grading Standards\nThis course is graded as Pass or Fail (technically “Satisfactory” or “Unsatisfactory”). To pass the course you need to do the following:\n\nParticipate at least 80% of the class meetings. Simply inform me of your absences (for health reasons, field research, etc.), and then do what you can to catch up with the work.\nTurn in all assignments on their due dates (see Course Outline below). This is especially critical until Spring Break, after which there will be more flexibility in your schedule.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Location and meeting times</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#how-do-i-log-my-use-of-genai",
    "href": "draft_sections/ai_analytica.html#how-do-i-log-my-use-of-genai",
    "title": "5  GenAI in Science",
    "section": "5.7 How do I log my use of GenAI?",
    "text": "5.7 How do I log my use of GenAI?\nWell, it should be as simple as putting an entry in my field notebook.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/ai_analytica.html#timeline-this-documented-was-updated-on-06-january-2026.-this-document-should-be-reviewed-periodically-particularly-when-adopting-new-genai-tools-or-when-disciplinary-norms-evolve-while-resisting-harmful-shifting-baselines.",
    "href": "draft_sections/ai_analytica.html#timeline-this-documented-was-updated-on-06-january-2026.-this-document-should-be-reviewed-periodically-particularly-when-adopting-new-genai-tools-or-when-disciplinary-norms-evolve-while-resisting-harmful-shifting-baselines.",
    "title": "5  GenAI in Science",
    "section": "5.2 Timeline: This documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "text": "5.2 Timeline: This documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_intro.html",
    "href": "draft_sections/02_course_intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Welcome to Quantitative Analysis of (Messy) Field Data!\nThe central goal of this course is to guide you through the transition:\nWorking through this challenging analytical landscape will not only teach you about statistics (and some statistical traps), but it will give you a concrete opportunityto work with your peers as you delve into the myriad issues that inevitably arise in data analysis. Learning how to navigate unfamiliar analytical territory is a skill that is critical for every developing researchers.\nThat said, we must acknowledge that everyone assimilates new material and produces syntheses in varied ways. All I ask is that each of you make use of this class to make significant progress towards completion of one of your thesis or dissertation chapters. You can choose a single analysis if that is all that you need or you can include 4-5 shorter analyses if that is what you need to make progress. This is an focused opportunity for you to get more help with data analysis, so take advantage.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_intro.html#welcome-to-quantitative-analysis-of-messy-field-data",
    "href": "draft_sections/02_course_intro.html#welcome-to-quantitative-analysis-of-messy-field-data",
    "title": "2  Introduction",
    "section": "",
    "text": "Messy field or lab data → processed data → models → inference",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_intro.html#course-objectives",
    "href": "draft_sections/02_course_intro.html#course-objectives",
    "title": "2  Introduction",
    "section": "2.2 Course Objectives",
    "text": "2.2 Course Objectives\nBy the end of this course, you should be able to:\n\nThink quantitatively about messy ecological data. You should be on your way to developing a habit of translating real-world ecological questions into quantitative frameworks, while explicitly acknowledging uncertainty and bias in estimation and measurement.\nDistinguish between questions/hypotheses, data, and inferences. You should be able to (1) clearly separate ecological hypotheses, the data actually collected, and the quantities being estimated, and (2) understand why these distinctions matter for interpretation and decision-making.\nUnderstand the processes that generate data. You should be able to recognize how different study designs, variation in detection, observer effects, instrument error, and data processing shape and constrain the structure of ecological datasets.\nSelect, build, and critique statistical models as scientific tools. You should be comfortable choosing and using statistical models not as black boxes, but as explicit representations of assumptions about ecological processes, variation, and causal structure. You should never blindly choose a statistical model again!\nInterpret results in ecological —not just statistical— terms. You should be able to conceptually move beyond p-values and coefficients to clearly articulate what your results mean biologically, mechanistically, and practically.\nBe comfortable applying causal –and not just correlational– reasoning. You should be able to use causal thinking (conceptual models and directed acyclic graphs) to evaluate what can —and ehat cannot— be inferred from observational and experimental data.\nCommunicate quantitative results clearly, transparently, and honestl.y Present analyses, figures, and conclusions in ways that are transparent, reproducible, and appropriate for scientific audiences.\nDevelop durable, reproducible analytical workflows. You should be increasingly comfortable with good data-science practices that support clarity, versioning (even though we will not delve into GitHub this term), and reusability of analyses.\nDevelop new confidence working with unfamiliar, complex, imperfect datasets. You should leave this course better prepared to engage with real ecological data —without expecting it to be clean, complete, or simple.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/02_course_intro.html#what-is-new-this-term-spring-2026",
    "href": "draft_sections/02_course_intro.html#what-is-new-this-term-spring-2026",
    "title": "2  Introduction",
    "section": "2.3 What is new this term (Spring 2026)?",
    "text": "2.3 What is new this term (Spring 2026)?\nFor Spring 2026, I have added attention to new topics that improve both efficiency of analysis workflows and our ability to understand causality at a deeper level. Specifically, I have chosen to add course components on:\n\nUsing large-language models (LLMs) –via ChatGPT– to improve analytical workflows. This term, I have integrated ChatGPT into the course as a way to support the process of analysis rather than to automate the analysis itself. Many of the hardest parts of quantitative work happen outside of writing code: figuring out why R code isn’t behaving as expected, checking whether an interpretation actually follows from a model, troubleshooting an obscure error message, or getting feedback on how tp clearly document a decision. Used carefully, ChatGPT can act like a sounding board for troubleshooting, sanity-checking, and refining explanations without taking over your critical thinking. Throughout the course, I have deliberately constrained ChatGPT’s role and scope and have tailored its guardrails to match the course progression. In other words, ChatGPT is used to help improve analytical workflow and decision-making, not to generate results or write code on a student’s behalf. We will work together to assess the utility of this tool as the term advances.\nUsing large-language models (LLMs) to improve teaching at scale. What does this mean? In past terms, between 10-15 students have been enrolled in this course. This term, I have allowed 26 students to enroll. Without a Teaching Assistant, this spawns a significant issue of scale. This is solvable if we use AI as a helpful tool. So, after spending some time as an LLM-consultant and tester, I decided to explicitly integrate chatGPT’s LLMs into my teaching/grading workflow. This means that students will use chatGPT to run checks on their work prior to submission; this will solve some of the “tuning” issues (with codes, grammar, clarity, etc.) that I have seen in student submissions in previous years.\nStructural Causal Models (SCMs) I added Structural Causal Models (SCMs) at the end of the course to give us a way to think more clearly about complex systems where multiple variables influence each other at the same time. In the first part of the course, students will see the limits of fitting separate models (e.g. GLMs, GLMMs, GAMMs) for singular response variables; those models can work individually, but they often miss shared drivers, indirect effects, or how pieces of the complex system of exogenous and endogenous factors fit together. SCMs give us a way to lay out those relationships explicitly and analyze them as a connected structure (as is present in natural systems) rather than as a collection of isolated models. The goal isn’t to replace the modeling approaches we’ve already used but to improve inference by making our assumptions about the system clear and coherent.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_course_roadmap.html",
    "href": "draft_sections/03_course_roadmap.html",
    "title": "3  Course Roadmap",
    "section": "",
    "text": "3.1 Course Roadmap\nThe course progresses as follows:\nGiven the scope of the entire course, we may only scratch the surface of some topics. Even our cursory treatment will convince you why these subjects matter for scientists who have messy data.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_course_roadmap.html#course-roadmap",
    "href": "draft_sections/03_course_roadmap.html#course-roadmap",
    "title": "3  Course Roadmap",
    "section": "",
    "text": "Metrology\nData exploration, outliers, visual inspection\nData heterogeneity\nGeneralized Linear Models (GLMs)\nGeneral principles of model validation\nGeneralized Linear Mixed Models (GLMMs)\nInformation Theory of Model Selection\nGeneralized Additive (Mixed) Models (GAMMs)\nSpatial and Temporal autocorrelation\nStructural Causal Models (but still not Bayesian)",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_course_roadmap.html#how-this-course-is-structured",
    "href": "draft_sections/03_course_roadmap.html#how-this-course-is-structured",
    "title": "3  Course Roadmap",
    "section": "3.2 How This Course Is Structured",
    "text": "3.2 How This Course Is Structured\nWe will meet twice weekly via Zoom. work in small groups, and you will also work extensively with your own data. By early October (around October 4), your dataset should be in working order—not just technically usable, but conceptually ready to explain to others.\n\n\n\n\n\n\nNote\n\n\n\nWorking order does not mean “a single spreadsheet.”\nA dataset is only complete if it includes metadata. Metadata describe: - what the variables are\n- how the data were collected\n- under what conditions\n- with what assumptions and limitations\nIf you have questions about metadata, I have a dedicated lecture on metadata structures, and I strongly encourage you to reach out.\n\n\n\n3.2.1 What to expect next\n\n[Describe what will happen in the remainder of the first class]\n[Outline upcoming assignments or activities]",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/03_course_roadmap.html#getting-started",
    "href": "draft_sections/03_course_roadmap.html#getting-started",
    "title": "3  Course Roadmap",
    "section": "3.3 Getting Started",
    "text": "3.3 Getting Started\n\n3.3.1 Who are you?\n\n[Prompt students to introduce themselves]\n[Encourage sharing of disciplinary background]\n\n\n\n3.3.2 Your data\n\n[Ask students to describe the data they work with or hope to work with]\n[Surface common challenges and anxieties]\n\n\n\n3.3.3 Your concerns and expectations\n\n[Invite discussion of fears, gaps, or uncertainties]",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html",
    "href": "draft_sections/04_syllabus.html",
    "title": "4  Syllabus",
    "section": "",
    "text": "4.1 Syllabus",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#location-and-meeting-times",
    "href": "draft_sections/04_syllabus.html#location-and-meeting-times",
    "title": "4  Syllabus",
    "section": "4.2 Location and meeting times",
    "text": "4.2 Location and meeting times\nTuesday/Thursday on Zoom; 11:00AM - 12:15PM (Mountain Time) Spring term 2026",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#office-hours",
    "href": "draft_sections/04_syllabus.html#office-hours",
    "title": "4  Syllabus",
    "section": "4.3 Office hours",
    "text": "4.3 Office hours\nDue to the nature of the course, I will not have specific office hours for the course. I have extended class times (after our weekly meetings) to allow more time to ask questions and for one-on-one work. These are optional and may be canceled in some weeks depending on your interest in meeting.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#prerequisites",
    "href": "draft_sections/04_syllabus.html#prerequisites",
    "title": "4  Syllabus",
    "section": "4.4 Prerequisites",
    "text": "4.4 Prerequisites\nTo ensure your success in this course, the following are required: - You must have a dataset to be analyzed this semester. This is very important. This class will only cover data reformatting; we will not cover data processing (except as necessary in specific cases). - Your data analysis must not be used in another (past or present). - You must be at least in your second year of graduate school. - You must have some exposure to using the Program R (tidyverse preferred, but base R is nice too). - You must have taken a statistics course in the last five years.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#assessment-and-grading-standards",
    "href": "draft_sections/04_syllabus.html#assessment-and-grading-standards",
    "title": "4  Syllabus",
    "section": "4.5 Assessment and Grading Standards",
    "text": "4.5 Assessment and Grading Standards\nThis course is graded as Pass or Fail (technically “Satisfactory” or “Unsatisfactory”). To pass the course you need to do the following:\n\nParticipate at least 80% of the class meetings. Simply inform me of your absences (for health reasons, field research, etc.), and then do what you can to catch up with the work.\nTurn in all assignments on their due dates (see Course Outline below). This is especially critical until Spring Break, after which there will be more flexibility in your schedule.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#attendanceparticipation-policy",
    "href": "draft_sections/04_syllabus.html#attendanceparticipation-policy",
    "title": "4  Syllabus",
    "section": "4.6 Attendance/Participation Policy:",
    "text": "4.6 Attendance/Participation Policy:\nThis is a graduate level course, and you are here for your own benefit. That being said, I expect you to come to class, stay engaged with the material, and not only learn how to do your own analysis but understand other types of data and analyses by working with your group members. If you do this, you should have analyzed your own data by the end of the semester and have part of a manuscript completed. Please email me ahead of time when you will be unable to attend class for a valid reason.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "href": "draft_sections/04_syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "title": "4  Syllabus",
    "section": "4.7 How to succeed in the course (beyond your wildest and funniest dreams)",
    "text": "4.7 How to succeed in the course (beyond your wildest and funniest dreams)\nGraduate school can be considerably challenging, as everyone is attempting to juggle research, teaching, classes, health, and family, all while coping with unexpected stressors. Course information is flying at you from every direction; there are many specific terms and concepts that you need to learn and operationalize. So, here are some reminders for you (even though I know you don’t need these):\n\nAsk questions! Even though there are no exams, take copious notes and work collaboratively to build course notes.\nDon’t be afraid to redirect the flow of the course. 5000-level courses should be flexible and fun. I want to give you time to think about and discuss the material. I’m willing to alter the pace of the course, change the order of topics, or devise new exercises for you. This is intended to be fun (while simultaneously transforming you into analytical gurus)! So, just talk to me about how I can help!\nRead all the material in this course guidebook. Many online courses require much more reading; this one does not.\nShow up to as many of the synchronous (Zoom) discussions as you can. When we meet together online, our goal will be to solidify everyone’s understanding of different concepts and how they are linked. These concepts will be useful as you navigate your own analysis project.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#expectations",
    "href": "draft_sections/04_syllabus.html#expectations",
    "title": "4  Syllabus",
    "section": "4.8 Expectations",
    "text": "4.8 Expectations\nAs your instructor, you should expect me to:\n\nTry my very hardest to make the course go smoothly (the reason you now have this nice new online course guide); but please be prepared for the inevitable hiccups. No matter how hard we all try, there always seem to be a few obstacles (like internet going down for a couple of hours when we’re on Zoom).\nRespond to questions within 24 hours during the work week. However, I likely will not respond during the weekend (unless there is an urgent matter).\nRespect you not only as a learner but as a colleague.\nUnderstand that these are strange and sometimes unforgiving times. We all have varying levels of tolerance and resistance to stress. If you are having a hard time for whatever reason, please communicate with me. I suck at judging, but I can do a hell of a job listening and working with you to solve a problem.\n\nAs a student, you are expected to:\n\nBe respectful of everyone in the class, including me.\nAsk for help if needed.\nTreat your presence in the classroom and your enrollment in this course as you would a job; Act professionally, arrive on time, pay attention, complete your work in a timely and professional manner, and treat your learning seriously.\nUnderstand that everyone is going through different things (family events, etc.), and be understanding of each other.\nBe engaged in the course.\nBe engaged within your assigned groups and help each other learn. Teaching another group member something you know solidifies your own knowledge and also sets you up to be a great future colleague.\n\n\n\n\n\n\n\nNoteA note on knowledge-sharing\n\n\n\n\n\nOur classroom is a shared intellectual space. Questions, mistakes, and partial understanding are part of learning. Please remember that supporting one another’s intellectual growth matters more than performative corrections or demonstrations of expertise.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#r-code-readings-and-discussion-sections",
    "href": "draft_sections/04_syllabus.html#r-code-readings-and-discussion-sections",
    "title": "4  Syllabus",
    "section": "4.9 R Code, readings, and discussion sections:",
    "text": "4.9 R Code, readings, and discussion sections:\nAll R code required for both instruction and hands-on exercises is available within this course book. Unlike past versions of this course, the present iteration no longer has traditional lectures. That said, I may respond to your questions by creating mini-presentations for you. I will share such material immediately after our discussions.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#classroom-climate-and-conduct",
    "href": "draft_sections/04_syllabus.html#classroom-climate-and-conduct",
    "title": "4  Syllabus",
    "section": "4.10 Classroom Climate and Conduct:",
    "text": "4.10 Classroom Climate and Conduct:\nAgain, you will be respectful towards your classmates and your instructors. Spirited debate and disagreement are to be expected in any classroom, and all perspectives will be heard, but we will behave civilly and with respect towards one another. Personal attacks, offensive language, name-calling, and dismissive gestures (eye-rolling, saying “whatever”, etc.) are not warranted in a learning atmosphere. Plus, in my opinion, such behavior shows an ability to problem-solve, which is counter to the mission of any university. As your instructor, I have the right to dismiss you from the classroom if you engage in disrespectful or disruptive behavior. Lastly, for the privacy of your fellow students, please do not record the lectures (unless with permission of the instructor).",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#classroom-statement-on-diversity",
    "href": "draft_sections/04_syllabus.html#classroom-statement-on-diversity",
    "title": "4  Syllabus",
    "section": "4.11 Classroom Statement on Diversity:",
    "text": "4.11 Classroom Statement on Diversity:\nThe University of Wyoming values an educational environment that is diverse, equitable, and inclusive. The diversity that students and faculty bring to class, including age, country of origin, culture, disability, economic class, ethnicity, gender identity, immigration status, linguistic, political affiliation, race, religion, sexual orientation, veteran status, worldview, and other social and cultural diversity is valued, respected, and considered a resource for learning. we understand that our UW community members represent a rich variety of backgrounds and perspectives. We are committed to providing an atmosphere for learning that respects diversity of all types. While working together to build this community, we ask all members–from students to staff to faculty–to:\n\nBe transparent about pre-existing biases and beliefs.\nDo not hesitate to share their unique experiences and perspectives.\nBe open to the views of others.\nHonor the uniqueness of their colleagues.\nAppreciate the opportunity that we have to learn from each other in this community.\nValue each other’s opinions and communicate in a respectful manner.\nKeep confidential any discussions of a personal (or professional) nature.\nUse this opportunity together to discuss ways in which we can create an inclusive environment in this course and across the University community.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#duty-to-report",
    "href": "draft_sections/04_syllabus.html#duty-to-report",
    "title": "4  Syllabus",
    "section": "4.12 Duty to Report:",
    "text": "4.12 Duty to Report:\nUW faculty are committed to supporting students and upholding the University’s non-discrimination policy. Under Title IX, discrimination based upon sex and gender is prohibited. If you experience an incident of sex- or gender-based discrimination, we encourage you to report it. While you may talk to a faculty member, understand that as a “Responsible Employee” of the University, the faculty member is required to report information you share about the incident to the University’s Title IX Coordinator (you may choose whether you or anyone involved is identified by name). If you would like to speak with someone who may be able to afford you privacy or confidentiality, there are people who can meet with you. Faculty can help direct you or you may find info about UW policy and resources at http://www.uwyo.edu/reportit. While we want you to feel comfortable coming to us with issues you may be struggling with or concerns you may be having, please be aware that we have some reporting requirements that are part of our job requirements at UW. You do not have to go through the experience alone. For example, if you inform us of an issue of sexual harassment, sexual assault, or discrimination we will keep the information as private as we can, but we am required to bring it to the attention of the institution’s Title IX Coordinator. If you would like to talk to those offices directly, you can contact Equal Opportunity Report and Response (Bureau of Mines Room 319, 766-5200, report-it@uwyo.edu, www.uwyo.edu/reportit). Additionally, you can also report incidents or complaints to the UW Police Department. You can also get support at the STOP Violence program (stopviolence@uwyo.edu, www.uwyo.edu/stop, 766-3296) (or SAFE Project (www.safeproject.org, campus@safeproject.org, 766-3434, 24-Hour hotline: 745-3556). Assistance and resources are available, and you are not required to make a formal complaint or participate in an investigation to access them. Another common example is if you are struggling with an issue that may be traumatic, or under unusual stress. We will likely inform the Dean of Students Office or Counseling Center. If you would like to reach out directly to them for assistance, you can contact them by going to www.uwyo.edu/dos/uwyocares.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#disability-statement",
    "href": "draft_sections/04_syllabus.html#disability-statement",
    "title": "4  Syllabus",
    "section": "4.13 Disability Statement:",
    "text": "4.13 Disability Statement:\nIf you have a physical, learning, sensory or psychological disability and require accommodations, please let me know as soon as possible. You will need to register with, and provide documentation of your disability to University Disability Support Services (UDSS) in SEO, room 330 Knight Hall.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/04_syllabus.html#academic-honesty",
    "href": "draft_sections/04_syllabus.html#academic-honesty",
    "title": "4  Syllabus",
    "section": "4.14 Academic Honesty:",
    "text": "4.14 Academic Honesty:\nThe University of Wyoming is built upon a strong foundation of integrity, respect and trust. All members of the university community have a responsibility to be honest and the right to expect honesty from others. Any form of academic dishonesty is unacceptable to our community and will not be tolerated [from the University Catalog]. Teachers and students should report suspected violations of standards of academic honesty to the instructor, department head, or dean. Other University regulations can be found here",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "draft_sections/05_ai.html",
    "href": "draft_sections/05_ai.html",
    "title": "5  GenAI in Science",
    "section": "",
    "text": "5.1 Responsible use of generative artificial intelligence (GenAI) in scientific analysis: a framework for guard rails and documentation",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/05_ai.html#timeline-this-documented-was-updated-on-06-january-2026.-this-document-should-be-reviewed-periodically-particularly-when-adopting-new-genai-tools-or-when-disciplinary-norms-evolve-while-resisting-harmful-shifting-baselines.",
    "href": "draft_sections/05_ai.html#timeline-this-documented-was-updated-on-06-january-2026.-this-document-should-be-reviewed-periodically-particularly-when-adopting-new-genai-tools-or-when-disciplinary-norms-evolve-while-resisting-harmful-shifting-baselines.",
    "title": "5  GenAI in Science",
    "section": "5.2 Timeline: This documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "text": "5.2 Timeline: This documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/05_ai.html#purpose",
    "href": "draft_sections/05_ai.html#purpose",
    "title": "5  GenAI in Science",
    "section": "5.3 Purpose",
    "text": "5.3 Purpose\nThis document establishes principles, guard rails, and documentation practices for the responsible use of generative artificial intelligence (GenAI) tools in scientific research and analysis. One such GenAI tool that most of us are familiar with is ChatGPT (Generative Pre-trained Transformer), a GenAI model that can detect (and arguably understand) and generate human-like text by predicting what comes next in a sentence. Large Language Models (LLMs) are very advanced GenAI systems trained on absolutely massive amounts of text to answer complex questions, write in certain rhetorical tones, summarize information, or have conversations in natural language. Given this tool’s widespread utility, it can certainly be misused. Therefore, the guidelines below attempt to ensure that GenAI augments human reasoning without compromising scientific validity, reproducibility, or accountability. In all aspects of the present academic work, GenAI tools like ChatGPT are therefore treated as assistive tools –comparable to statistical software (like R) or calculators– not as autonomous analysts or skilled authors.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/05_ai.html#core-principles-of-responsible-use-of-genai",
    "href": "draft_sections/05_ai.html#core-principles-of-responsible-use-of-genai",
    "title": "5  GenAI in Science",
    "section": "5.4 Core principles of responsible use of GenAI",
    "text": "5.4 Core principles of responsible use of GenAI\n\nAccountability: The human researcher retains full responsibility for –and is held accountable for– all analytical decisions, interpretations, text and code, and inference. The use of GenAI does not relieve any burdens of authorship, responsibility, or liability. Importantly, this means that the human researcher is allowed –and, in most cases, is encouraged– to use AI-produced code, as long as the research has vetted and error-checked. This likewise assumes that the researcher accepts responsibility for any and all errors arising from AI assistance. That is, GenAI may accelerate work, but it may not replace understanding. If a researcher cannot defend a decision without the GenAI present, the decision is invalid.\nScientific Primacy: Scientific reasoning precedes and constrains GenAI use. Hypotheses, data-generating assumptions, and model structures must be defined by the researcher. GenAI may clarify or critique these decisions but may not create them from scratch without human justification.\nTransparency: All GenAI use must be documented in a way that allows another scientist to understand how GenAI influenced the work. GenAI-assisted reasoning must be distinguishable from original analysis.\nReproducibility: All results must be reproducible without access to GenAI tools. Data, code, and associated documentation must be sufficient to reproduce results independently. GenAI may assist development but must not be a hidden dependency.\nProportionality: The level of documentation naturally should be proportional to the influence of GenAI. Minor stylistic assistance requires minimal logging, while conceptual or analytical assistance requires a larger volume of explicit documentation and justification.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/05_ai.html#what-is-allowed-and-what-is-not-allowed",
    "href": "draft_sections/05_ai.html#what-is-allowed-and-what-is-not-allowed",
    "title": "5  GenAI in Science",
    "section": "5.5 What is allowed, and what is not allowed?",
    "text": "5.5 What is allowed, and what is not allowed?\n\nPermitted Uses of GPTs in Scientific Work\n\nConceptual Clarification: GenAI may be used to explain statistical concepts, examine modeling assumptions, and interpret model diagnostics (at a broad level).\nPlanning and Reflection: GenAI may assist with refining research questions, generating assumption checklists, stress-testing interpretations, and identifying alternative explanations that are then vetted for ecological sanity by the human researcher.\nWriting Support: GenAI may be used to improve clarity, organization, and tone, and to identify points of ambiguity or inferential overreach. AI absolutely must not invent methods, results, or citations.\nCode Understanding: GenAI may explain what existing code does, diagnose warnings or errors conceptually, and suggest stylistic or reproducibility improvements. AI may not replace independent code comprehension.\n\n\n\n\nProhibited or Restricted Uses of GenAIin Scientific Work\n\nReplacement of Scientific Judgment: GenAI must not be used to select models, error distributions, priors, or random-effects structures without independent human justification, nor to interpret results without verification.\nUndocumented Analysis Generation: GenAI must not generate full end-to-end analytical pipelines without explanation, or produce black-box code whose logic is not understood by the researcher.\nFabrication: GenAI must not be used to invent data, methods, results, or citations, or to create post-hoc justifications unsupported by the analysis.\nOutcome Optimization: GenAI must not be used to iteratively prompt for statistically significant results, improved AIC values, or exaggerated certainty.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/05_ai.html#genai-interaction-log",
    "href": "draft_sections/05_ai.html#genai-interaction-log",
    "title": "5  GenAI in Science",
    "section": "5.6 GenAI Interaction Log",
    "text": "5.6 GenAI Interaction Log\nWhat is a GenAI Interaction Log?\n\n5.6.1 Purpose of the Log\nResearchers maintain an AI Interaction Log alongside their analysis notebook to document meaningful AI involvement in the research process.\nRequired Information: Entries record the date, GenAI tool used, purpose of interaction, nature of assistance provided, key takeaway, and the verification step performed.\nWhen Logging Is Required: Logging is required when GenAI influences model choice, interpretation, methodological justification, or scientific claims. Minor stylistic or grammatical use does not require detailed logging.\nVerification Obligations: Any GenAI-suggested content must be independently verified using primary literature, software documentation, diagnostic checks, or independent reasoning. Unverified AI output must not appear in final analyses.\nAuthorship and Attribution: GenAI tools are not authors and do not receive citation credit. If required by journals or funders, GenAI use may be acknowledged in a neutral Methods or Acknowledgments statement.\nEthical Safeguards: GenAI use must not obscure uncertainty, inflate confidence, reduce methodological transparency, or disadvantage collaborators or students with limited access to AI tools.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "draft_sections/05_ai.html#how-do-i-log-my-use-of-genai",
    "href": "draft_sections/05_ai.html#how-do-i-log-my-use-of-genai",
    "title": "5  GenAI in Science",
    "section": "5.7 How do I log my use of GenAI?",
    "text": "5.7 How do I log my use of GenAI?\nWell, it should be as simple as putting an entry in my field notebook.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "chapters/statistical_aphorisms.html",
    "href": "chapters/statistical_aphorisms.html",
    "title": "8  Statistical Aphorisms",
    "section": "",
    "text": "8.1 Three Statistical Aphorisms (to guide you through this course)",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "chapters/statistical_aphorisms.html#three-statistical-aphorisms-to-guide-you-through-this-course",
    "href": "chapters/statistical_aphorisms.html#three-statistical-aphorisms-to-guide-you-through-this-course",
    "title": "8  Statistical Aphorisms",
    "section": "8.2 Analytical Workflow",
    "text": "NoteAphorism 1: Embrace imperfection\n\n\n\nBeware perfection. In data analysis, one can be paralyzed by “perfect” solutions. After all, perfection is a moving target. What does “perfection” really mean in practice? Does it mean that an analysis looks complex enough to impress your colleagues or instructors, even if it’s not appropriate for your questions? Is it an analysis that works flawlessly the first time, right out of the box? Or is it one that is quickly built and deployed but that also may hide critical assumptions or produce error-filled predictions? Or perhaps one that a senior colleague or advisor has deemed “the correct” approach (without justification)?\nThe danger lies in waiting for an ideal solution that may never arrive, or may arrive too late. In doing so, your and your collaborators’ progress stalls, opportunities are missed, and, perhaps most importantly, learning is sorely delayed. This idea is captured poetically in the words of Robert Watson-Watt, the inventor of radar:\n\nGive them the third best to go on with; the second best comes too late, the best never comes.\n\nWatson-Watt’s advice should remind us that, in dynamic and complex fields such as radar development or our own messy statistical modeling, timely action often outweighs our vision of perfection. A solution that is simply good enough today allows you to:\n\nLearn through deployment: Practical application often reveals insights that you cannot anticipate.\nIterate and improve: Real-world feedback reveals issues far faster than endless refinement in isolation!\nTransparently communicate analytical limitations: Collaborators and stakeholders can better understand assumptions, uncertainties, and the scope of your work.\n\nIn short (as as you know), perfection is often the enemy of good progress. By consciously and purposefully adopting a culture of imperfection, you will be able to better learn, iterate, and produce work that is timely, practical, and ultimately more impactful.\n\n\n\n\n\n\n\n\nNoteAphorism 2: Embrace complexity.\n\n\n\nBeware over-simplicity.\n\n\n\n\n\n\nNoteAphorism #3: Beware of Armadillo Burrows.\n\n\n\nDon’t fall into someone else’s statistical trap!\n\n\n\n\n\n\n\n\nNoteAphorism #: Beware paralysis of analysis.\n\n\n\nCamponotus leonardi infected by a “zombie fungus” (genus Ophiocordyceps)\n\n\n\n8.2 Analytical Workflow\n\n\n8.3 Defining “analytical workflow”\nWhat do we mean by analytical workflow? First, it is the data infrastructure that supports efficient analysis. It turns raw data into action.\nThere should also be analysis of the workflow itself. Is it efficient in terms of computational load or time? Does it allow for reproducibility?\n\n\n8.4 Two thoughts as you develop your analytical workflow\n\nThere are a huge &gt;\n\nHere’s my wisdom for your use, as I learned it when the moose And the reindeer roamed where Paris roars to-night: “There are nine and sixty ways of constructing tribal lays, And—every—single—one—of—them—is—right! –From In the Neolithic Age (Rudyard Kipling)\na core tenet of collaborative, creative, and productive data science Let me that for you How many of you have experience with prompt engineering? Your goals as a scientist… Use this course to develop your own philosophical workflow (that reduces bias) Stick to this philosophy until you learn something new and improved\n\n\n8.5 Statistical Philosophy\nMore than anything else, I want you to use this course to develop a personal statistical philosophy.\nA good philosophy: - reduces bias\n- prevents analytical wandering\n- promotes clarity and efficiency\n- evolves as you learn more\nI will present a set of guiding principles and aphorisms early in the course. You should modify them, reject them, or replace them—but you should have something guiding your decisions.\nThis is how you become consistent, thoughtful, and credible as a scientist.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "chapters/statistical_aphorisms.html#defining-analytical-workflow",
    "href": "chapters/statistical_aphorisms.html#defining-analytical-workflow",
    "title": "8  Statistical Aphorisms",
    "section": "8.3 Defining “analytical workflow”",
    "text": "8.3 Defining “analytical workflow”\nWhat do we mean by analytical workflow? First, it is the data infrastructure that supports efficient analysis. It turns raw data into action.\nThere should also be analysis of the workflow itself. Is it efficient in terms of computational load or time? Does it allow for reproducibility?",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "chapters/statistical_aphorisms.html#two-thoughts-as-you-develop-your-analytical-workflow",
    "href": "chapters/statistical_aphorisms.html#two-thoughts-as-you-develop-your-analytical-workflow",
    "title": "8  Statistical Aphorisms",
    "section": "8.4 Two thoughts as you develop your analytical workflow",
    "text": "8.4 Two thoughts as you develop your analytical workflow\n\nThere are a huge &gt;\n\nHere’s my wisdom for your use, as I learned it when the moose And the reindeer roamed where Paris roars to-night: “There are nine and sixty ways of constructing tribal lays, And—every—single—one—of—them—is—right! –From In the Neolithic Age (Rudyard Kipling)\na core tenet of collaborative, creative, and productive data science Let me that for you How many of you have experience with prompt engineering? Your goals as a scientist… Use this course to develop your own philosophical workflow (that reduces bias) Stick to this philosophy until you learn something new and improved",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "chapters/statistical_aphorisms.html#statistical-philosophy",
    "href": "chapters/statistical_aphorisms.html#statistical-philosophy",
    "title": "8  Statistical Aphorisms",
    "section": "8.5 Statistical Philosophy",
    "text": "8.5 Statistical Philosophy\nMore than anything else, I want you to use this course to develop a personal statistical philosophy.\nA good philosophy: - reduces bias\n- prevents analytical wandering\n- promotes clarity and efficiency\n- evolves as you learn more\nI will present a set of guiding principles and aphorisms early in the course. You should modify them, reject them, or replace them—but you should have something guiding your decisions.\nThis is how you become consistent, thoughtful, and credible as a scientist.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Aphorisms</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html",
    "href": "chapters/naming_conventions.html",
    "title": "9  Naming conventions",
    "section": "",
    "text": "9.1 Part I: Naming Conventions and Data Organization\n[Based primarily on the Naming Conventions lecture :contentReferenceoaicite:0]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#why-naming-conventions-matter",
    "href": "chapters/naming_conventions.html#why-naming-conventions-matter",
    "title": "9  Naming conventions",
    "section": "9.2 Why Naming Conventions Matter",
    "text": "9.2 Why Naming Conventions Matter\n\n[Explain why variable names, file names, and coding style influence collaboration, reproducibility, and long-term scalability]\n[Discuss how unconventional naming can cause downstream problems in analysis]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#common-naming-conventions",
    "href": "chapters/naming_conventions.html#common-naming-conventions",
    "title": "9  Naming conventions",
    "section": "9.3 Common Naming Conventions",
    "text": "9.3 Common Naming Conventions\n\n[Describe camelCase, PascalCase, and snake_case]\n[Explain why snake_case is typically preferred in R and data science]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#general-rules-for-naming-data-objects",
    "href": "chapters/naming_conventions.html#general-rules-for-naming-data-objects",
    "title": "9  Naming conventions",
    "section": "9.4 General Rules for Naming Data Objects",
    "text": "9.4 General Rules for Naming Data Objects\n\n[Outline rules about avoiding spaces, special characters, leading numbers, and inconsistent capitalization]\n[Describe best practices for handling dates, missing values, and metadata]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#naming-variables-vs.-naming-functions",
    "href": "chapters/naming_conventions.html#naming-variables-vs.-naming-functions",
    "title": "9  Naming conventions",
    "section": "9.5 Naming Variables vs. Naming Functions",
    "text": "9.5 Naming Variables vs. Naming Functions\n\n[Explain the noun/verb distinction for objects and functions]\n[Provide guidance on clarity versus brevity]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#coding-style-and-the-tidyverse",
    "href": "chapters/naming_conventions.html#coding-style-and-the-tidyverse",
    "title": "9  Naming conventions",
    "section": "9.6 Coding Style and the Tidyverse",
    "text": "9.6 Coding Style and the Tidyverse\n\n[Introduce the tidyverse style guide and its role in standardization]\n[Describe tools such as lintr and styler and what problems they solve]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#data-file-formats-and-dataset-management",
    "href": "chapters/naming_conventions.html#data-file-formats-and-dataset-management",
    "title": "9  Naming conventions",
    "section": "9.7 Data File Formats and Dataset Management",
    "text": "9.7 Data File Formats and Dataset Management\n\n[Compare acceptable formats for small datasets (CSV, TSV)]\n[Introduce efficient formats for larger datasets (e.g., parquet) and why they matter]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#becoming-a-better-statistical-programmer",
    "href": "chapters/naming_conventions.html#becoming-a-better-statistical-programmer",
    "title": "9  Naming conventions",
    "section": "9.8 Becoming a Better Statistical Programmer",
    "text": "9.8 Becoming a Better Statistical Programmer\n\n[Encourage iterative improvement of existing projects]\n[Discuss professional responsibility in collaborative data management]",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html",
    "href": "chapters/data_exploration.html",
    "title": "9  Data exploration",
    "section": "",
    "text": "9.1 Data Exploration\nYou have already collected data. Now we focus on: - understanding what those data actually are\n- identifying patterns\n- building appropriate models\n- validating and selecting those models\n- making inference and generating new hypotheses\nWe will take a frequentist approach in this course. We will not cover Bayesian modeling in detail, though many ideas—especially model structure and philosophy—transfer directly.",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#data-exploration",
    "href": "chapters/data_exploration.html#data-exploration",
    "title": "9  Data exploration",
    "section": "",
    "text": "NoteA note on Bayesian methods\n\n\n\n\n\nBayesian approaches are increasingly common and powerful, and this may change over the next 5–10 years. However, the foundational ideas about data structure, scale, and modeling assumptions are shared across frameworks.",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#distinguishing-measurand-observed-data-and-estimated-value",
    "href": "chapters/data_exploration.html#distinguishing-measurand-observed-data-and-estimated-value",
    "title": "9  Data exploration",
    "section": "9.2 Distinguishing measurand, observed data, and estimated value",
    "text": "9.2 Distinguishing measurand, observed data, and estimated value\n\n\n\n\n\n\nTipPopulation abundance is changing over time\n\n\n\nPrompt: [What is the measurand? What are the observed data? What does the model return?]\n\n\nReveal\n\n\nMeasurand: True population abundance within a defined area/time window\n\nObserved: Counts/detections/encounters\n\nEstimated: Abundance available for detection during sampling, conditional on closure/availability",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#why-data-exploration-comes-first",
    "href": "chapters/data_exploration.html#why-data-exploration-comes-first",
    "title": "9  Data exploration",
    "section": "9.3 Why Data Exploration Comes First",
    "text": "9.3 Why Data Exploration Comes First\n\n[Explain the risks of skipping exploratory steps]\n[Frame exploration as hypothesis protection, not data dredging]",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#step-1-identifying-outliers",
    "href": "chapters/data_exploration.html#step-1-identifying-outliers",
    "title": "9  Data exploration",
    "section": "9.4 Step 1: Identifying Outliers",
    "text": "9.4 Step 1: Identifying Outliers\n\n[Define univariate vs. multivariate outliers]\n[Discuss different sources of outliers: error, biology, rare events]\n\n\n9.4.1 Graphical Identification of Univariate Outliers\n\n[Describe Cleveland plots, Tukey boxplots, and conditional boxplots]\n[Explain strengths and weaknesses of each]\n\n\n\n9.4.2 Statistical vs. Expert Judgment\n\n[Contrast formal statistical rules with biological or domain expertise]\n[Discuss risks of authority-based decisions]\n\n\n\n9.4.3 Multivariate Outliers\n\n[Introduce Mahalanobis distance and robust alternatives]\n[Explain why univariate methods can fail]\n\n\n\n9.4.4 What to Do When You Find Outliers\n\n[Outline options: removal, parallel analyses, transformation, or no action]\n[Emphasize transparency and justification]",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#step-2-examining-zeroes-in-the-data",
    "href": "chapters/data_exploration.html#step-2-examining-zeroes-in-the-data",
    "title": "9  Data exploration",
    "section": "9.5 Step 2: Examining Zeroes in the Data",
    "text": "9.5 Step 2: Examining Zeroes in the Data\n\n[Explain zero inflation and why it matters]\n[Describe when excess zeroes signal a modeling problem versus a biological pattern]",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#step-3-collinearity-among-predictors",
    "href": "chapters/data_exploration.html#step-3-collinearity-among-predictors",
    "title": "9  Data exploration",
    "section": "9.6 Step 3: Collinearity Among Predictors",
    "text": "9.6 Step 3: Collinearity Among Predictors\n\n[Define collinearity and why it destabilizes models]\n[Contrast implications for frequentist vs. Bayesian models]\n\n\n9.6.1 Detecting Collinearity\n\n[Describe scatterplots, correlation coefficients, and VIFs/GVIFs]\n[Explain interpretation thresholds (e.g., VIF &lt; 3)]\n\n\n\n9.6.2 Dealing with Collinearity\n\n[Describe selective variable removal]\n[Introduce PCA as a dimensionality-reduction approach]\n[Discuss biological reasoning in deciding what to retain]",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#step-4-considering-interaction-terms",
    "href": "chapters/data_exploration.html#step-4-considering-interaction-terms",
    "title": "9  Data exploration",
    "section": "9.7 Step 4: Considering Interaction Terms",
    "text": "9.7 Step 4: Considering Interaction Terms\n\n[Explain what interaction terms represent biologically]\n[Discuss sample size constraints and interpretability]",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#step-5-standardizing-scaling-or-leaving-covariates-alone",
    "href": "chapters/data_exploration.html#step-5-standardizing-scaling-or-leaving-covariates-alone",
    "title": "9  Data exploration",
    "section": "9.8 Step 5: Standardizing, Scaling, or Leaving Covariates Alone",
    "text": "9.8 Step 5: Standardizing, Scaling, or Leaving Covariates Alone\n\n[Define standardization and centering]\n[Explain how these choices affect interpretation of coefficients and intercepts]\n[Provide guidance on when each approach is appropriate]",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration.html#step-6-violations-of-homogeneity",
    "href": "chapters/data_exploration.html#step-6-violations-of-homogeneity",
    "title": "9  Data exploration",
    "section": "9.9 Step 6: Violations of Homogeneity",
    "text": "9.9 Step 6: Violations of Homogeneity\n\n[Introduce homoscedasticity and heteroscedasticity]\n[Explain why residuals—not raw data—are the focus]",
    "crumbs": [
      "Part I: Chaos to Columns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html",
    "href": "chapters/preprocessing.html",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "",
    "text": "10.1 Overview\nWelcome back. This marks the beginning of the data exploration phase of the course. In this video, we focus on basic exploratory steps that should always be done before formal modeling.\nThe goal here is not to perform inference, but to identify potential problems early—before they complicate or derail the analysis phase. In the next video, we will move on to coping with heterogeneity in the data, particularly through variance structures.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#the-data-exploration-toolkit",
    "href": "chapters/preprocessing.html#the-data-exploration-toolkit",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.2 The Data Exploration Toolkit",
    "text": "10.2 The Data Exploration Toolkit\nAcross this section of the course, we will work through five core data exploration steps:\n\nIdentifying outliers (univariate and multivariate)\n\nIdentifying extra zeros in the data\n\nAssessing multicollinearity\n\nThinking carefully about interactions\n\nDeciding whether to standardize covariates\n\nThese steps are meant to give you a practical toolkit for diagnosing issues early. Skipping them often leads to paralysis during the modeling phase. Many of these are mistakes I have made myself over the years—and learned from the hard way.\nIn this video, we focus on Steps 1 and 2: outliers and extra zeros.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#types-of-outliers",
    "href": "chapters/preprocessing.html#types-of-outliers",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.3 Types of Outliers",
    "text": "10.3 Types of Outliers\nOutliers arise for many reasons, and not all outliers are errors. It is useful to think about different types:\n\n10.3.1 Univariate Outliers\nThese occur in one dimension only—a single variable. For example, a body size measurement that is larger than expected relative to the rest of the population.\nThese values may be: - Slightly larger or smaller than expected - Rare but biologically plausible - Statistically extreme, depending on assumptions\n\n\n\n10.3.2 Multivariate Outliers\nMultivariate outliers occur when a data point is unusual in combination across variables, even if it is not extreme in any single variable.\nThese are extremely common in real datasets and often more important than univariate outliers.\n\n\n\n10.3.3 Influential Observations\nThese are data points that exert disproportionate influence on a regression model. They are typically identified after modeling (e.g., via Cook’s distance).\nThis approach is common, but it is not blind to inference and can introduce bias if used carelessly. Decisions about data inclusion should not be driven by p-values.\n\n\n\n10.3.4 Measurement and Processing Errors\nOutliers may arise from:\n\nData entry errors\nSpreadsheet mistakes\nMiscommunication during field measurements\nInstrument or observer error\n\nThis is why rigorous data quality control is essential.\n\n\n\n10.3.5 Natural Oddities (“Black Swans”)\nSome outliers are real and biologically meaningful. Rare events—such as extreme climatic years or unusual individuals—may have disproportionate ecological importance.\nAutomatically removing such observations risks losing genuine scientific insight.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#identifying-univariate-outliers",
    "href": "chapters/preprocessing.html#identifying-univariate-outliers",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.4 Identifying Univariate Outliers",
    "text": "10.4 Identifying Univariate Outliers\nA classic definition (Moore & McCabe):\n\nAn outlier is an observation that lies outside the overall pattern of a distribution.\n\nThis definition is inherently univariate. There are two broad approaches to identifying outliers:\n\n10.4.1 Statistical Identification\nThis depends on: - Sample size - Assumptions about the underlying distribution\nWith small sample sizes, this approach can be fragile and misleading.\n\n\n\n10.4.2 Expert Judgment\nThis relies on domain knowledge, but must be applied cautiously. Appeals to authority are not substitutes for statistical reasoning, though in some cases expert knowledge is indispensable.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#graphical-approaches-to-univariate-outliers",
    "href": "chapters/preprocessing.html#graphical-approaches-to-univariate-outliers",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.5 Graphical Approaches to Univariate Outliers",
    "text": "10.5 Graphical Approaches to Univariate Outliers\nBefore modeling, graphical inspection provides a useful first approximation.\n\n10.5.1 Cleveland Plots (Dot Charts)\nCleveland plots rank observations and display them visually. They make no distributional assumptions and are purely exploratory.\nThey are subjective, but effective for spotting values that warrant further attention.\n\n\n\n10.5.2 Tukey Box Plots\nTukey-style box plots are widely used and based on quantiles, not normality.\nKey components:\n\nThe box spans the interquartile range (IQR):\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\nWhiskers extend to \\(1.5 \\times \\text{IQR}\\)\nPoints beyond this threshold are flagged as outliers\nPoints beyond \\(3 \\times \\text{IQR}\\) may be flagged as extreme outliers\n\nThese thresholds are arbitrary but conventional. They provide a consistent rule of thumb rather than a strict statistical test.\n\n\n\n10.5.3 Conditional Box Plots\nConditional box plots split the data by a grouping factor (e.g., month, treatment, site).\nAdvantages: - Can scale box width by sample size - Highlight where outliers originate - Useful for diagnosing heterogeneity across groups",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#caveats-with-univariate-outliers",
    "href": "chapters/preprocessing.html#caveats-with-univariate-outliers",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.6 Caveats with Univariate Outliers",
    "text": "10.6 Caveats with Univariate Outliers\nBe especially cautious with small sample sizes.\nFor example, data drawn from a Gamma distribution (bounded at zero, right-skewed) may produce values flagged as outliers by Tukey’s rule—even when they are perfectly consistent with the true distribution.\nOutliers identified by a rule are not necessarily statistical anomalies.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#multivariate-outliers-1",
    "href": "chapters/preprocessing.html#multivariate-outliers-1",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.7 Multivariate Outliers",
    "text": "10.7 Multivariate Outliers\nMost datasets are multivariate. A data point may appear normal in each variable individually, yet be extreme in multivariate space.\nThis is where univariate methods fail.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#mahalanobis-distance",
    "href": "chapters/preprocessing.html#mahalanobis-distance",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.8 Mahalanobis Distance",
    "text": "10.8 Mahalanobis Distance\nA powerful method for identifying multivariate outliers is Mahalanobis distance, which accounts for covariance among variables.\nConceptually, it measures how far a point is from the multivariate centroid:\n\\[\nD^2 = (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\n\\]\nIn practice, this can be implemented using existing R packages (e.g., psych) and works well for moderate-dimensional datasets.\n\n\n10.8.1 Extensions: Minimum Covariance Determinant (MCD)\nA newer variant uses a subset (e.g., 75%) of the data to estimate covariance robustly, then flags points outside that structure.\nAdvantages: - Improves model convergence - Produces stable coefficient estimates\nDisadvantages: - Can be overly aggressive in flagging outliers - Requires careful justification\nThis approach is promising but still evolving.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#what-to-do-if-you-identify-an-outlier",
    "href": "chapters/preprocessing.html#what-to-do-if-you-identify-an-outlier",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.9 What to Do If You Identify an Outlier",
    "text": "10.9 What to Do If You Identify an Outlier\nThere is no single correct response. Options include:\n\n10.9.1 1. Remove the Outlier (With Justification)\nRemoval must be justified using multiple lines of evidence, such as: - Field notes - Known anomalies (e.g., storms, injuries) - Instrument failure\n\n\n\n10.9.2 2. Conduct Concurrent Analyses\nRun models: - With the outlier included - With the outlier removed\nReport both results transparently. If conclusions are unchanged, the outlier has little influence.\n\n\n\n10.9.3 3. Do Nothing (Often the Best Choice)\nWith adequate sample size, individual outliers rarely matter. If one point drastically changes results, the real issue is often insufficient data.\n\n\n\n10.9.4 4. Do Not Transform the Data\nData transformation is largely obsolete in modern statistical practice. It is unnecessary for most models and can actively distort inference.\nTransformations often fail to achieve their stated goals and introduce new problems.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#ethical-and-biological-considerations",
    "href": "chapters/preprocessing.html#ethical-and-biological-considerations",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.10 Ethical and Biological Considerations",
    "text": "10.10 Ethical and Biological Considerations\nDo not remove outliers that are biologically meaningful:\n\nExtreme climate years\nRare but dominant individuals\nUnusual but real ecological events\n\nThese often generate the most interesting hypotheses.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#extra-zeros-in-the-data",
    "href": "chapters/preprocessing.html#extra-zeros-in-the-data",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.11 Extra Zeros in the Data",
    "text": "10.11 Extra Zeros in the Data\nThe second focus of this video is identifying excess zeros, which can cause serious modeling issues.\nWe are not trying to force data into normality. Instead, we want to understand whether the data reasonably approximate a distribution we intend to model.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#why-extra-zeros-matter",
    "href": "chapters/preprocessing.html#why-extra-zeros-matter",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.12 Why Extra Zeros Matter",
    "text": "10.12 Why Extra Zeros Matter\nExcess zeros can:\n\nPrevent model convergence\nProduce unstable or misleading coefficient estimates\n\nIf the likelihood surface is poorly defined, the estimation algorithm may fail or settle on spurious solutions.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#visual-inspection-for-zero-inflation",
    "href": "chapters/preprocessing.html#visual-inspection-for-zero-inflation",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.13 Visual Inspection for Zero Inflation",
    "text": "10.13 Visual Inspection for Zero Inflation\nA practical approach:\n\nPlot a histogram\nIncrease the number of bins\nZoom into the lower range (e.g., 0–10)\n\nIf the frequency at zero is much higher than expected under a Poisson or Gamma distribution, zero inflation may be present.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#quantifying-zero-inflation",
    "href": "chapters/preprocessing.html#quantifying-zero-inflation",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.14 Quantifying Zero Inflation",
    "text": "10.14 Quantifying Zero Inflation\nA simple diagnostic:\n\nCalculate the proportion of zeros\nIf more than ~50% of observations are zero, standard GLMs may struggle\n\nImportantly, zeros are not bad data. They often represent real biological states (e.g., non-breeders, absence).\nThey simply require the appropriate model, which we will cover later.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/preprocessing.html#wrap-up",
    "href": "chapters/preprocessing.html#wrap-up",
    "title": "10  Data Exploration I: Outliers and Extra Zeros",
    "section": "10.15 Wrap-Up",
    "text": "10.15 Wrap-Up\nIn this video, we covered:\n\nTypes of outliers\nGraphical and statistical tools for identifying them\nEthical considerations for handling outliers\nIdentification of excess zeros and why they matter\n\nThese steps should be completed before formal modeling. They prevent bias, reduce frustration, and lead to more defensible inference.\nIn the next section, we will move on to heterogeneity and variance structures.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Exploration I: Outliers and Extra Zeros</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html",
    "href": "chapters/intro_modeling.html",
    "title": "11  modeling",
    "section": "",
    "text": "11.1 Statistical Philosophy and Practice",
    "crumbs": [
      "Part II: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>modeling</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html#statistical-philosophy-and-practice",
    "href": "chapters/intro_modeling.html#statistical-philosophy-and-practice",
    "title": "11  modeling",
    "section": "",
    "text": "11.1.1 A culture of the imperfect\n\n[Introduce aphorisms about imperfection and pragmatism]\n[Discuss why waiting for the “perfect” analysis is often counterproductive]\n\n\n\n11.1.2 Avoiding statistical traps\n\n[Describe common analytical pitfalls]\n[Emphasize critical reading of others’ analyses]\n\n\n\n11.1.3 Multiple valid paths\n\n[Introduce the idea that many analytical approaches can be defensible]\n[Connect to collaboration and intellectual humility]",
    "crumbs": [
      "Part II: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>modeling</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html#patterns-models-and-inference",
    "href": "chapters/intro_modeling.html#patterns-models-and-inference",
    "title": "11  modeling",
    "section": "11.2 Patterns, Models, and Inference",
    "text": "11.2 Patterns, Models, and Inference\n\n11.2.1 Detecting patterns\n\n[Describe visualization and exploratory analysis as pattern discovery]\n[Emphasize skepticism and iteration]\n\n\n\n11.2.2 Models as tools, not truths\n\n[Explain what models are (and are not)]\n[Discuss assumptions, simplifications, and abstraction]\n\n\n\n11.2.3 Inference and uncertainty\n\n[Describe uncertainty, variability, and confidence]\n[Distinguish statistical significance from scientific importance]",
    "crumbs": [
      "Part II: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>modeling</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html#limits-of-conventional-modeling",
    "href": "chapters/intro_modeling.html#limits-of-conventional-modeling",
    "title": "11  modeling",
    "section": "11.3 Limits of Conventional Modeling",
    "text": "11.3 Limits of Conventional Modeling\n\n11.3.1 Standard models you already know\n\n[Briefly list GLMs, GAMs, GLMMs, etc.]\n[Acknowledge their usefulness]\n\n\n\n11.3.2 Why these models are often insufficient\n\n[Discuss complexity, nonlinearity, dependence, and scale]\n[Explain mismatch between data-generating processes and model assumptions]\n\n\n\n11.3.3 What the “ideal” might look like\n\n[Pose the question of ideal inference without answering it fully]\n[Frame this as a motivating tension for the course]",
    "crumbs": [
      "Part II: Columns to Curves",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>modeling</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html",
    "href": "chapters/glm_poisson.html",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "",
    "text": "12.1 Overview\nWelcome back to Day 3. This is the second video on generalized linear models (GLMs), focusing specifically on models for non-normal data. In this lecture, we work through count data as a concrete case study to build intuition about model choice, diagnostics, prediction, and interpretation.\nThese ideas will be reinforced through hands-on practice in class.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#poisson-glms-for-count-data",
    "href": "chapters/glm_poisson.html#poisson-glms-for-count-data",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.2 Poisson GLMs for Count Data",
    "text": "12.2 Poisson GLMs for Count Data\nCount data are extremely common in ecological surveys and field studies. Whenever your response variable is a count (e.g., number of individuals, detections, or events), the Poisson distribution is often the natural starting point.\nA defining property of the Poisson distribution is:\n\nThe mean equals the variance.\n\nFormally,\n\\[\n\\mathbb{E}(Y) = \\mathrm{Var}(Y)\n\\]\nThis means the distribution is fully specified by a single parameter (the mean). Other distributions often require additional parameters to describe dispersion.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#link-functions-for-the-poisson-distribution",
    "href": "chapters/glm_poisson.html#link-functions-for-the-poisson-distribution",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.3 Link Functions for the Poisson Distribution",
    "text": "12.3 Link Functions for the Poisson Distribution\nSeveral link functions are available for Poisson GLMs:\n\nIdentity\n\nSquare root\n\nLog (default and most commonly used)\n\nThe log link is generally preferred because it ensures that fitted values are strictly positive, which is required for count data.\nWith a log link:\n\nThe linear predictor is on the log scale\nThe inverse link is the exponential function\nCoefficients are interpreted multiplicatively after back-transformation\n\nIf \\(\\eta\\) is the linear predictor, then the mean response is:\n\\[\n\\mu = \\exp(\\eta)\n\\]\nBack-transforming model output is critical for interpretation and prediction.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#assumptions-of-the-poisson-glm",
    "href": "chapters/glm_poisson.html#assumptions-of-the-poisson-glm",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.4 Assumptions of the Poisson GLM",
    "text": "12.4 Assumptions of the Poisson GLM\nFor a Poisson GLM to be appropriate, the response must satisfy:\n\\[\n\\mathrm{Var}(Y) \\approx \\mathbb{E}(Y)\n\\]\nTo evaluate this assumption, we calculate an overdispersion parameter. Ideally, this value should be close to 1.\n\nDispersion \\(&gt; 1\\): Overdispersion\nDispersion \\(&lt; 1\\): Underdispersion\n\nIn practice, overdispersion is far more common than underdispersion.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#calculating-the-dispersion-statistic",
    "href": "chapters/glm_poisson.html#calculating-the-dispersion-statistic",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.5 Calculating the Dispersion Statistic",
    "text": "12.5 Calculating the Dispersion Statistic\nIf dispersion is not reported automatically, it can be calculated as:\n\\[\n\\hat{c} = \\frac{\\sum r_i^2}{n - p}\n\\]\nwhere:\n\n\\(r_i\\) are Pearson residuals\n\n\\(n\\) is the number of observations\n\n\\(p\\) is the number of estimated parameters (including the intercept)\n\nFor mixed models, defining \\(p\\) becomes more complicated because the effective sample size lies between the number of observations and the number of random-effect levels.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#when-overdispersion-is-a-symptom",
    "href": "chapters/glm_poisson.html#when-overdispersion-is-a-symptom",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.6 When Overdispersion Is a Symptom",
    "text": "12.6 When Overdispersion Is a Symptom\nBefore assuming overdispersion is the primary issue, consider other common causes:\n\nModel misspecification (missing predictors)\nInfluential observations or outliers\nMissing interaction terms\nPredictors on inappropriate scales\nUnmodeled nonlinear relationships\nZero inflation\nUnaccounted dependency structure\n\nA standard GLM assumes independent observations with no spatial, temporal, or hierarchical structure.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#strategies-for-handling-overdispersion",
    "href": "chapters/glm_poisson.html#strategies-for-handling-overdispersion",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.7 Strategies for Handling Overdispersion",
    "text": "12.7 Strategies for Handling Overdispersion\n\n12.7.1 Quasi-Poisson Models\nQuasi-Poisson models adjust standard errors to account for overdispersion while leaving the mean structure unchanged. This is a technical fix and can work in some cases, but it is not always ideal.\n\n\n12.7.2 Negative Binomial Models (Preferred)\nThe negative binomial distribution includes two parameters:\n\nMean\nDispersion parameter\n\nThis allows the variance to exceed the mean:\n\\[\n\\mathrm{Var}(Y) = \\mu + \\alpha \\mu^2\n\\]\nwhere \\(\\alpha\\) controls overdispersion. This makes negative binomial GLMs a robust choice for overdispersed count data.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#why-distribution-choice-matters",
    "href": "chapters/glm_poisson.html#why-distribution-choice-matters",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.8 Why Distribution Choice Matters",
    "text": "12.8 Why Distribution Choice Matters\nAt large mean values, a Poisson distribution can visually resemble a normal distribution. However, the underlying assumptions remain different.\nIf variance is mischaracterized:\n\nStandard errors will be incorrect\nConfidence intervals will be misleading\np-values may be invalid\n\nA good decision workflow is:\n\nIs the response discrete or continuous?\nWhat are the bounds of the data?\nDoes variance increase with the mean?",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#example-dataset-fish-abundance-and-water-depth",
    "href": "chapters/glm_poisson.html#example-dataset-fish-abundance-and-water-depth",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.9 Example Dataset: Fish Abundance and Water Depth",
    "text": "12.9 Example Dataset: Fish Abundance and Water Depth\nWe now consider a dataset designed to address the question:\n\nHas the relationship between water depth and total fish abundance changed over time?\n\nBefore modeling, we:\n\nInspect variable structure\nRemove a spatial outlier (for teaching purposes)\nRescale depth for interpretability\n\nAlways examine your data first, but avoid making inferential claims from exploratory plots alone.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#ordinary-least-squares-as-a-baseline",
    "href": "chapters/glm_poisson.html#ordinary-least-squares-as-a-baseline",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.10 Ordinary Least Squares as a Baseline",
    "text": "12.10 Ordinary Least Squares as a Baseline\nWe begin with a Gaussian linear model:\n\\[\n\\text{Total abundance} \\sim \\text{Mean depth}\n\\]\nThe model reports a strong effect of depth. However, residual diagnostics reveal major violations:\n\nHeteroskedasticity\nSkewed residuals\nNon-normality\nInfluential observations\n\nQ–Q plots confirm these violations. This model is not appropriate for count data.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#residuals-in-glms",
    "href": "chapters/glm_poisson.html#residuals-in-glms",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.11 Residuals in GLMs",
    "text": "12.11 Residuals in GLMs\nIn GLMs, raw residuals (observed minus fitted) are not useful. Instead, use:\n\nPearson residuals\nDeviance residuals\n\nThese should be plotted against:\n\nFitted values\nEach predictor\nTime\nSpace\n\nResiduals should show no systematic structure across any variable.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#fitting-a-poisson-glm",
    "href": "chapters/glm_poisson.html#fitting-a-poisson-glm",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.12 Fitting a Poisson GLM",
    "text": "12.12 Fitting a Poisson GLM\nWe refit the model using a Poisson GLM with a log link:\n\\[\n\\log(\\mu_i) = \\beta_0 + \\beta_1 \\text{Depth}_i\n\\]\nThe model explains approximately 43% of the deviance:\n\\[\nR^2_{\\text{pseudo}} = \\frac{D_\\text{null} - D_\\text{residual}}{D_\\text{null}}\n\\]\nHowever, diagnostics reveal extreme overdispersion.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#explicit-overdispersion-diagnosis",
    "href": "chapters/glm_poisson.html#explicit-overdispersion-diagnosis",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.13 Explicit Overdispersion Diagnosis",
    "text": "12.13 Explicit Overdispersion Diagnosis\nUsing Pearson residuals, we compute:\n\\[\n\\hat{c} \\approx 115\n\\]\nThis indicates severe overdispersion and confirms that the Poisson model is mis-specified.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#model-refinement-attempts",
    "href": "chapters/glm_poisson.html#model-refinement-attempts",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.14 Model Refinement Attempts",
    "text": "12.14 Model Refinement Attempts\nWe explore several refinements:\n\nCook’s distance reveals many influential points\nAdding sampling period as a covariate improves fit marginally\nAdding an offset for sampling effort worsens dispersion\n\nEach step provides diagnostic information but does not resolve the issue.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#negative-binomial-glm",
    "href": "chapters/glm_poisson.html#negative-binomial-glm",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.15 Negative Binomial GLM",
    "text": "12.15 Negative Binomial GLM\nWe refit the same model using a negative binomial GLM:\n\\[\n\\log(\\mu_i) = \\beta_0 + \\beta_1 \\text{Depth}_i + \\beta_2 \\text{Period}_i\n\\]\nThis dramatically improves the model:\n\nDispersion \\(\\approx 1\\)\nResiduals stabilize\nCook’s distance values drop\nAIC strongly favors the negative binomial model",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#interpreting-the-final-model",
    "href": "chapters/glm_poisson.html#interpreting-the-final-model",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.16 Interpreting the Final Model",
    "text": "12.16 Interpreting the Final Model\nThe depth–abundance relationship is similar across periods. However, claims about similarity require explicit comparison between:\n\nAdditive models\nInteraction models\n\nOnly after comparing these models can we infer whether relationships truly differ.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#prediction-and-back-transformation",
    "href": "chapters/glm_poisson.html#prediction-and-back-transformation",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.17 Prediction and Back-Transformation",
    "text": "12.17 Prediction and Back-Transformation\nWhen predicting from GLMs:\n\nAlways use type = \"link\"\nRequest standard errors\nCompute confidence intervals on the link scale\nBack-transform using the inverse link\n\nFor a log link, predictions are back-transformed as:\n\\[\n\\hat{\\mu} = \\exp(\\hat{\\eta})\n\\]\nConfidence intervals are:\n\\[\n\\exp(\\hat{\\eta} \\pm 1.96 \\cdot \\text{SE})\n\\]\nThis produces asymmetric intervals appropriate for count data.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#a-cautionary-example",
    "href": "chapters/glm_poisson.html#a-cautionary-example",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.18 A Cautionary Example",
    "text": "12.18 A Cautionary Example\nIn a published study of reproductive success, predictions were plotted on the link scale, leading to biologically impossible negative values. The model itself was correct—the error occurred during prediction and visualization.\nThis mistake is common and avoidable.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#key-take-home-messages",
    "href": "chapters/glm_poisson.html#key-take-home-messages",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.19 Key Take-Home Messages",
    "text": "12.19 Key Take-Home Messages\n\nUse Poisson GLMs for count data whenever appropriate\nAlways check for zero inflation and overdispersion\nPrefer negative binomial models when overdispersion is present\nNever extrapolate beyond observed data ranges\nDefault to type = \"link\" when predicting\nAlways back-transform predictions and confidence intervals",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/glm_poisson.html#wrap-up",
    "href": "chapters/glm_poisson.html#wrap-up",
    "title": "12  Generalized Linear Models for Count Data",
    "section": "12.20 Wrap-Up",
    "text": "12.20 Wrap-Up\nThis concludes Day 3. In Day 4, we will continue building on these ideas and extend them to more complex modeling frameworks.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalized Linear Models for Count Data</span>"
    ]
  },
  {
    "objectID": "chapters/02_course_intro.html",
    "href": "chapters/02_course_intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Welcome to Quantitative Analysis of (Messy) Field Data!\nThanks for joining us this term! I couldn’t be more pleased at the large size of the class this term! This means that there are more perspectives to help guide us through the following phases of data analysis:\nThe complexity of the project, difficulty of field conditions, or an impending deadline can make the jumps from data collection to inference horribly daunting to young and old researchers alike. Our joint goal in this course is to become more comfortable with each of these steps, so that we not only work more efficiently but we also know a bit more about how to sense of our complex world. It is my sincerest hope that this course will not only teach you some about statistics (and some statistical traps) but that it will also give you a unique opportunity to work with your peers as you delve into the myriad issues that inevitably arise in data analysis. Working together means that we must first acknowledge that each one of us assimilates new material and produces syntheses in varied ways. All I ask is that each of you",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02_course_intro.html#welcome-to-quantitative-analysis-of-messy-field-data",
    "href": "chapters/02_course_intro.html#welcome-to-quantitative-analysis-of-messy-field-data",
    "title": "2  Introduction",
    "section": "",
    "text": "Messy field or lab data → processed data → models → inference",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02_course_intro.html#course-objectives",
    "href": "chapters/02_course_intro.html#course-objectives",
    "title": "2  Introduction",
    "section": "2.2 Course Objectives",
    "text": "2.2 Course Objectives\nBy the end of this course, you should be able to:\n\nThink quantitatively about messy ecological data. You should be on your way to developing a habit of translating real-world ecological questions into quantitative frameworks, while explicitly acknowledging uncertainty and bias in estimation and measurement.\nDistinguish between questions/hypotheses, data, and inferences. You should be able to (1) clearly separate ecological hypotheses, the data actually collected, and the quantities being estimated, and (2) understand why these distinctions matter for interpretation and decision-making.\nUnderstand the processes that generate data. You should be able to recognize how different study designs, variation in detection, observer effects, instrument error, and data processing shape and constrain the structure of ecological datasets.\nSelect, build, and critique statistical models as scientific tools. You should be comfortable choosing and using statistical models not as black boxes, but as explicit representations of assumptions about ecological processes, variation, and causal structure. You should never blindly choose a statistical model again!\nInterpret results in ecological —not just statistical— terms. You should be able to conceptually move beyond p-values and coefficients to clearly articulate what your results mean biologically, mechanistically, and practically.\nBe comfortable applying causal –and not just correlational– reasoning. You should be able to use causal thinking (conceptual models and directed acyclic graphs) to evaluate what can —and ehat cannot— be inferred from observational and experimental data.\nCommunicate quantitative results clearly, transparently, and honestl.y Present analyses, figures, and conclusions in ways that are transparent, reproducible, and appropriate for scientific audiences.\nDevelop durable, reproducible analytical workflows. You should be increasingly comfortable with good data-science practices that support clarity, versioning (even though we will not delve into GitHub this term), and reusability of analyses.\nDevelop new confidence working with unfamiliar, complex, imperfect datasets. You should leave this course better prepared to engage with real ecological data —without expecting it to be clean, complete, or simple.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02_course_intro.html#what-is-new-this-term-spring-2026",
    "href": "chapters/02_course_intro.html#what-is-new-this-term-spring-2026",
    "title": "2  Introduction",
    "section": "2.3 What is new this term (Spring 2026)?",
    "text": "2.3 What is new this term (Spring 2026)?\nFor Spring 2026, I have added attention to new topics that improve both efficiency of analysis workflows and our ability to understand causality at a deeper level. Specifically, I have chosen to add course components on:\n\nUsing large-language models (LLMs) –via ChatGPT– to improve analytical workflows. This term, I have integrated ChatGPT into the course as a way to support the process of analysis rather than to automate the analysis itself. Many of the hardest parts of quantitative work happen outside of writing code: figuring out why R code isn’t behaving as expected, checking whether an interpretation actually follows from a model, troubleshooting an obscure error message, or getting feedback on how tp clearly document a decision. Used carefully, ChatGPT can act like a sounding board for troubleshooting, sanity-checking, and refining explanations without taking over your critical thinking. Throughout the course, I have deliberately constrained ChatGPT’s role and scope and have tailored its guardrails to match the course progression. In other words, ChatGPT is used to help improve analytical workflow and decision-making, not to generate results or write code on a student’s behalf. We will work together to assess the utility of this tool as the term advances.\n\n\n\n\nUsing large-language models (LLMs) to improve teaching at scale. What does this mean? In past terms, between 10-15 students have been enrolled in this course. This term, I have allowed 26 students to enroll. Without a Teaching Assistant, this spawns a significant issue of scale. This is solvable if we use AI as a helpful tool. So, after spending some time as an LLM-consultant and tester, I decided to explicitly integrate chatGPT’s LLMs into my teaching/grading workflow. This means that students will use a custom GPT that I created specifically for ZOO/ECOL-5500 called JackalopeGPT (using ChatGPT) to run checks on their work prior to submission; this will solve some of the “tuning” issues (with codes, grammar, clarity, etc.) that I have seen in student submissions in previous years.\n\n\n\n\n\n\nStructural Causal Models (SCMs) I added Structural Causal Models (SCMs) at the end of the course to give us a way to think more clearly about complex systems where multiple variables influence each other at the same time. In the first part of the course, students will see the limits of fitting separate models (e.g. GLMs, GLMMs, GAMMs) for singular response variables; those models can work individually, but they often miss shared drivers, indirect effects, or how pieces of the complex system of exogenous and endogenous factors fit together. SCMs give us a way to lay out those relationships explicitly and analyze them as a connected structure (as is present in natural systems) rather than as a collection of isolated models. The goal isn’t to replace the modeling approaches we’ve already used but to improve inference by making our assumptions about the system clear and coherent.\nJackalopes! This course corrects a long-standing perspective that (1) jackalopes had deer-like antlers (Fig. 1), and (2) there was only a single species of jackalopes (Fig. 2). We will use the newly discovered cryptid ecosystem for course exercises and examples.\n\n\n\n\nFigure 1. Illustrations of the correct and incorrect ornament morphology of jackalopes.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/06_preparing_yourself.html",
    "href": "chapters/06_preparing_yourself.html",
    "title": "6  Preparing Yourself",
    "section": "",
    "text": "6.1 What do I need to do to get started in this class?\nTo prepare for the course, here is what you are expected to do by the end of the first week (specific information is below this list):",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparing Yourself</span>"
    ]
  },
  {
    "objectID": "chapters/06_preparing_yourself.html#what-do-i-need-to-do-now",
    "href": "chapters/06_preparing_yourself.html#what-do-i-need-to-do-now",
    "title": "6  Preparing Yourself",
    "section": "6.2 What do I need to do now?",
    "text": "6.2 What do I need to do now?\nTo prepare for the course, here is what you are expected to do by the end of the first week (specific information is below this list):\n\nJoin the ZOO-5500 Slack channel\nDownload, Install, and set up RStudio Desktop\nGather your dataset files for your analysis\n\n\n6.2.1 Join the ZOO-5500 Slack channel\nJust click on the list and follow instructions. We will use Slack to encourage ongoing, low-stakes discussion within the class. This will be for sharing questions, ideas, and insights in real time so that learning can continue outside of scheduled class meetings. This allows everyone (even me) to get answers to questions that I might be thinking about at odd times. It also allows for problems to be solved faster; rather than waiting until our synchronous Zoom session to ask about an issue, we can group-think over the course of the week. Remember that this is set up to improve collaboration; it does not mean that anyone should be responding outside of work hours or on the weekends. Below are the preferred channels for different kinds of communication:\n\n#announcements &gt;Read-only channel for important course information like deadlines, corrections, and post-class clarifications. Check this regularly.\n#course-questions &gt;For general course or assignment questions (due dates, unclear wording on websites) that others might also have. Expect peer responses and occasional instructor follow-ups.\n#data-help &gt;For data wrangling and technical data issues (imports, joins, NAs, plots). Not for model choice or interpretation.\n#model-talk &gt;For conceptual questions about models and assumptions. Discussion-focused; no code debugging.\n#analysis-workflow &gt;For questions about reproducibility, organization, reporting, and good analytical habits.\n#ai-use-and-logs For discussing allowed AI use and posting AI interaction logs when requested. No private communication or graded work.\n#papers-good-bad-ugly &gt;For highlighting examples of good and bad analyses in published work.\n#random &gt;For off-topic chat, random links, and fun stuff. This will help keep everything else focused.\n\n\n\n6.2.2 Download, Install, and set up RStudio Desktop\nInstall on your local machine. All coding and file organization will be done in RStudio. Note that the following is a very basic way of creating a set of project folders. You will see boxes with R code. Feel free to copy and run these, if necessary. Here are the basic steps:\n\nDownload and install RStudio Desktop. Navigate to your folder (in this case, your student folder).\nIn “Files” window (lower right panel in RStudio), click “More/Set as Working Directory”. This sets your working directory; this is always a good idea to do.\nCreate a New R Project in your student folder: “File/New Project/Existing Directory”. Name this project appropriately.\nCreate your File structure: Refer to this RStudio tutorial. In this class, we will modify this set of directories to make things a bit easier and more transparent for you. After completing the following steps, place your dataset in your “data/raw_data” folder (after you create it below). Navigate to this folder on your computer and drag-and-drop or copy-paste your data file(s) into this folder.\n\nCreate three folders (one suggested framework):\n\n/data (for raw, processed, clean datasets)\n/r_scripts (for R scripts, each containing a set of related functions)\n/output output (for models, graphs, tables)\n\nWe can do this by staying in our directory (within RStudio) and then using the dir.create function.\n\ndir.create(\"data\")\n\nYou can see your new folder (“data”) appear in the Files in the lower right window within RStudio. Now let’s create the other two folders.\n\ndir.create(\"r_scripts\")\ndir.create(\"output\")\n\nTo help with data processing (i.e. cleaning and organization)–something that we will discuss in coming days within the context of your analytical workflow–, we will create some subfolders within your newly created “data” folder. Let’s do this now. Note how you create a folder within the “data” folder by specifying the path:\n\ndir.create(\"data/raw_data\")\ndir.create(\"data/processed_data\")\ndir.create(\"data/metadata\")\n\nAnd that is how you create a basic R Project in RStudio! What you have done is create a set of folders and subfolders that have names that can be easily understood by other users. Feel free to create other subfolders now. Or, should you not like the file nomenclature used above, change the names to whatever you wish. Just be sure that this structure is as simple as possible so that other users understand what you have done.\nThere is an added benefit to creating an R Project in this way. What you have done is create a minimally reproducible data structure. There are two primary ways this can be accomplished (if you are not familiar with this. First method: A collaborator (or instructor or classmate) can simply navigate to your subfolder, set that as their working directory from within RStudio, and then open the .RProj file. Alternatively, you can create zip your root folder (the one labelled as your name, in this case) and then share your entire analysis, etc. All the recipient would need to do is unzip the folder, open the .Rproj file, and then “Set as Working Directory.”\n\n\n6.2.3 Gather your dataset files for your analysis\nPlace your data in your “data/raw_data” subfolder and then confirm that your files are present by running the list.files function.\n\nlist.files(\"data/raw_data\")\n\n\n\n\n\n\n\nNote\n\n\n\nWorking order means more than a spreadsheet or two.\nA dataset is only complete if it includes metadata. Metadata describe: - what the variables are\n- how the data were collected\n- under what conditions\n- with what assumptions and limitations\nIf you have questions about metadata, I have an old, dedicated lecture on metadata structures, and I strongly encourage you to reach out.\n\n\nYou are now ready to move to the next steps of loading necessary R packages and beginning your journey of data exploration!",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparing Yourself</span>"
    ]
  },
  {
    "objectID": "chapters/06_preparing_yourself.html#instructions",
    "href": "chapters/06_preparing_yourself.html#instructions",
    "title": "6  Preparing Yourself",
    "section": "6.3 Instructions",
    "text": "6.3 Instructions\n\n6.3.1 Join the ZOO-5500 Slack channel\nJust click on the list and follow instructions. We will use Slack to encourage ongoing, low-stakes discussion within the class. This will be for sharing questions, ideas, and insights in real time so that learning can continue outside of scheduled class meetings. This allows everyone (even me) to get answers to questions that I might be thinking about at odd times. It also allows for problems to be solved faster; rather than waiting until our synchronous Zoom session to ask about an issue, we can group-think over the course of the week. Remember that this is set up to improve collaboration; it does not mean that anyone should be responding outside of work hours or on the weekends. Below are the preferred channels for different kinds of communication:\n\n#announcements &gt;Read-only channel for important course information like deadlines, corrections, and post-class clarifications. Check this regularly.\n#course-questions &gt;For general course or assignment questions (due dates, unclear wording on websites) that others might also have. Expect peer responses and occasional instructor follow-ups.\n#data-help &gt;For data wrangling and technical data issues (imports, joins, NAs, plots). Not for model choice or interpretation.\n#model-talk &gt;For conceptual questions about models and assumptions. Discussion-focused; no code debugging.\n#analysis-workflow &gt;For questions about reproducibility, organization, reporting, and good analytical habits.\n#ai-use-and-logs For discussing allowed AI use and posting AI interaction logs when requested. No private communication or graded work.\n#papers-good-bad-ugly &gt;For highlighting examples of good and bad analyses in published work.\n#random &gt;For off-topic chat, random links, and fun stuff. This will help keep everything else focused.\n\n\n\n6.3.2 Download, Install, and set up RStudio Desktop\nInstall on your local machine. All coding and file organization will be done in RStudio. Note that the following is a very basic way of creating a set of project folders. You will see boxes with R code. Feel free to copy and run these, if necessary. Here are the basic steps:\n\nDownload and install RStudio Desktop. Navigate to your folder (in this case, your student folder).\nIn “Files” window (lower right panel in RStudio), click “More/Set as Working Directory”. This sets your working directory; this is always a good idea to do.\nCreate a New R Project in your student folder: “File/New Project/Existing Directory”. Name this project appropriately.\nCreate your File structure: Refer to this RStudio tutorial. In this class, we will modify this set of directories to make things a bit easier and more transparent for you. After completing the following steps, place your dataset in your “data/raw_data” folder (after you create it below). Navigate to this folder on your computer and drag-and-drop or copy-paste your data file(s) into this folder.\n\nCreate three folders (one suggested framework):\n\n/data (for raw, processed, clean datasets)\n/r_scripts (for R scripts, each containing a set of related functions)\n/output output (for models, graphs, tables)\n\nWe can do this by staying in our directory (within RStudio) and then using the dir.create function.\n\ndir.create(\"data\")\n\nYou can see your new folder (“data”) appear in the Files in the lower right window within RStudio. Now let’s create the other two folders.\n\ndir.create(\"r_scripts\")\ndir.create(\"output\")\n\nTo help with data processing (i.e. cleaning and organization)–something that we will discuss in coming days within the context of your analytical workflow–, we will create some subfolders within your newly created “data” folder. Let’s do this now. Note how you create a folder within the “data” folder by specifying the path:\n\ndir.create(\"data/raw_data\")\ndir.create(\"data/processed_data\")\ndir.create(\"data/metadata\")\n\nAnd that is how you create a basic R Project in RStudio! What you have done is create a set of folders and subfolders that have names that can be easily understood by other users. Feel free to create other subfolders now. Or, should you not like the file nomenclature used above, change the names to whatever you wish. Just be sure that this structure is as simple as possible so that other users understand what you have done.\nThere is an added benefit to creating an R Project in this way. What you have done is create a minimally reproducible data structure. There are two primary ways this can be accomplished (if you are not familiar with this. First method: A collaborator (or instructor or classmate) can simply navigate to your subfolder, set that as their working directory from within RStudio, and then open the .RProj file. Alternatively, you can create zip your root folder (the one labelled as your name, in this case) and then share your entire analysis, etc. All the recipient would need to do is unzip the folder, open the .Rproj file, and then “Set as Working Directory.”\n\n\n6.3.3 Gather your dataset files for your analysis\nPlace your data in your “data/raw_data” subfolder and then confirm that your files are present by running the list.files function.\n\nlist.files(\"data/raw_data\")\n\nYou are now ready to move to the next steps of loading necessary R packages and beginning your journey of data exploration!",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparing Yourself</span>"
    ]
  },
  {
    "objectID": "chapters/06_preparing_yourself.html#create-three-folders-one-suggested-framework",
    "href": "chapters/06_preparing_yourself.html#create-three-folders-one-suggested-framework",
    "title": "6  Preparing Yourself",
    "section": "6.3 Create three folders (one suggested framework):",
    "text": "6.3 Create three folders (one suggested framework):\n\ndata (for raw, processed, clean datasets)\nR (for R scripts, each containing a set of related functions)\noutput (for models, graphs, tables)\n\nWe can do this by staying in our directory (within RStudio) and then using the dir.create function.\n\ndir.create(\"data\")\n\nYou can see your new folder (“data”) appear in the Files in the lower right window within RStudio. Now let’s create the other two folders.\n\ndir.create(\"r_scripts\")\ndir.create(\"output\")\n\nTo help with data processing (i.e. cleaning and organization)–something that we will discuss in coming days within the context of your analytical workflow–, we will create some subfolders within your newly created “data” folder. Let’s do this now. Note how you create a folder within the “data” folder by specifying the path:\n\ndir.create(\"data/raw_data\")\ndir.create(\"data/processed_data\")\ndir.create(\"data/metadata\")\n\nFeel free to create other subfolders now. Or, should you not like the file nomenclature used above, change the names to whatever you wish. And that is how you create a basic R Project in RStudio! What you have done is create a set of folders and subfolders that have names that can be easily understood by other users.\nThere is an added benefit to creating an R Project in this way. What you have done is create a minimally reproducible data structure. First method: A collaborator (or instructor or classmate) can simply navigate to your subfolder, set that as their working directory from within RStudio, and then open the .RProj file. Alternatively, you can create zip your root folder (the one labelled as your name, in this case) and then share your entire analysis, etc. All the recipient would need to do is unzip the folder, open the .Rproj file, and then “Set as Working Directory.” Place your data in your “data/raw_data” subfolder.\nAfter copying and pasting your data file into this subfolder, confirm that your files is there by using the list.files function.\n\nlist.files(\"data/raw_data\")\n\ncharacter(0)\n\n\nYou are now ready to move to the next steps of loading necessary R packages and beginning your journey of data exploration!",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparing Yourself</span>"
    ]
  },
  {
    "objectID": "chapters/05_ai.html",
    "href": "chapters/05_ai.html",
    "title": "5  GenAI Use Guidelines",
    "section": "",
    "text": "5.1 Timeline\nResponsible use of generative artificial intelligence (GenAI)\nThis documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/05_ai.html#timeline-this-documented-was-updated-on-06-january-2026.-this-document-should-be-reviewed-periodically-particularly-when-adopting-new-genai-tools-or-when-disciplinary-norms-evolve-while-resisting-harmful-shifting-baselines.",
    "href": "chapters/05_ai.html#timeline-this-documented-was-updated-on-06-january-2026.-this-document-should-be-reviewed-periodically-particularly-when-adopting-new-genai-tools-or-when-disciplinary-norms-evolve-while-resisting-harmful-shifting-baselines.",
    "title": "5  GenAI in Science",
    "section": "5.2 Timeline: This documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "text": "5.2 Timeline: This documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "chapters/05_ai.html#purpose",
    "href": "chapters/05_ai.html#purpose",
    "title": "5  GenAI Use Guidelines",
    "section": "5.2 Purpose",
    "text": "5.2 Purpose\nThis document reviews principles, guard rails, and documentation practices for the responsible use of generative artificial intelligence (GenAI) tools in scientific research and analysis. One such GenAI tool that most of us are familiar with is ChatGPT (Generative Pre-trained Transformer), a GenAI model that can detect (and arguably understand) and generate human-like text by predicting what comes next in a sentence. Large Language Models (LLMs) are very advanced GenAI systems trained on absolutely massive amounts of text to answer complex questions, write in certain rhetorical tones, summarize information, or have conversations in natural language. Given this tool’s widespread utility, it can certainly be misused. Therefore, the guidelines below attempt to ensure that GenAI augments human reasoning without compromising scientific validity, reproducibility, or accountability. In all aspects of the present academic work, GenAI tools like ChatGPT are therefore treated as assistive tools –comparable to statistical software (like R) or calculators– not as autonomous analysts or skilled authors.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/05_ai.html#core-principles-of-responsible-use-of-genai",
    "href": "chapters/05_ai.html#core-principles-of-responsible-use-of-genai",
    "title": "5  GenAI Use Guidelines",
    "section": "5.3 Core principles of responsible use of GenAI",
    "text": "5.3 Core principles of responsible use of GenAI\n\nAccountability: The human researcher retains full responsibility for –and is held accountable for– all analytical decisions, interpretations, text and code, and inference. The use of GenAI does not relieve any burdens of authorship, responsibility, or liability. Importantly, this means that the human researcher is allowed –and, in most cases, is encouraged– to use AI-produced code, as long as the research has vetted and error-checked. This likewise assumes that the researcher accepts responsibility for any and all errors arising from AI assistance. That is, GenAI may accelerate work, but it may not replace understanding. If a researcher cannot defend a decision without the GenAI present, the decision is invalid.\nScientific Primacy: Scientific reasoning precedes and constrains GenAI use. Hypotheses, data-generating assumptions, and model structures must be defined by the researcher. GenAI may clarify or critique these decisions but may not create them from scratch without human justification.\nTransparency: All GenAI use must be documented in a way that allows another scientist to understand how GenAI influenced the work. GenAI-assisted reasoning must be distinguishable from original analysis.\nReproducibility: All results must be reproducible without access to GenAI tools. Data, code, and associated documentation must be sufficient to reproduce results independently. GenAI may assist development but must not be a hidden dependency.\nProportionality: The level of documentation naturally should be proportional to the influence of GenAI. Minor stylistic assistance requires minimal logging, while conceptual or analytical assistance requires a larger volume of explicit documentation and justification.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/05_ai.html#what-is-allowed-and-what-is-not-allowed",
    "href": "chapters/05_ai.html#what-is-allowed-and-what-is-not-allowed",
    "title": "5  GenAI Use Guidelines",
    "section": "5.4 What is allowed, and what is not allowed?",
    "text": "5.4 What is allowed, and what is not allowed?\n\nPermitted Uses of GPTs in Scientific Work\n\nConceptual Clarification: GenAI may be used to explain statistical concepts, examine modeling assumptions, and interpret model diagnostics (at a broad level).\nPlanning and Reflection: GenAI may assist with refining research questions, generating assumption checklists, stress-testing interpretations, and identifying alternative explanations that are then vetted for ecological sanity by the human researcher.\nWriting Support: GenAI may be used to improve clarity, organization, and tone, and to identify points of ambiguity or inferential overreach. AI absolutely must not invent methods, results, or citations.\nCode Understanding: GenAI may explain what existing code does, diagnose warnings or errors conceptually, and suggest stylistic or reproducibility improvements. AI may not replace independent code comprehension.\n\n\n\n\nProhibited or Restricted Uses of GenAIin Scientific Work\n\nReplacement of Scientific Judgment: GenAI must not be used to select models, error distributions, priors, or random-effects structures without independent human justification, nor to interpret results without verification.\nUndocumented Analysis Generation: GenAI must not generate full end-to-end analytical pipelines without explanation, or produce black-box code whose logic is not understood by the researcher.\nFabrication: GenAI must not be used to invent data, methods, results, or citations, or to create post-hoc justifications unsupported by the analysis.\nOutcome Optimization: GenAI must not be used to iteratively prompt for statistically significant results, improved AIC values, or exaggerated certainty.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/05_ai.html#genai-interaction-log",
    "href": "chapters/05_ai.html#genai-interaction-log",
    "title": "5  GenAI Use Guidelines",
    "section": "5.5 GenAI Interaction Log",
    "text": "5.5 GenAI Interaction Log\n\n5.5.1 Purpose of the Log\nResearchers should maintain an AI Interaction Log alongside their analysis notebook to document meaningful AI involvement in the research process. (This is a required component for students in ZOO/ECOL-5500 starting Spring 2026)\n\nRequired Information: Entries record the date, GenAI tool used, purpose of interaction, nature of assistance provided, key takeaway, and the verification step performed.\nWhen Logging Is Required: Logging is required when GenAI influences model choice, interpretation, methodological justification, or scientific claims. Minor stylistic or grammatical use does not require detailed logging.\nVerification Obligations: Any GenAI-suggested content must be independently verified using primary literature, software documentation, diagnostic checks, or independent reasoning. Unverified AI output must not appear in final analyses.\nAuthorship and Attribution: GenAI tools are not authors and do not receive citation credit. If required by journals or funders, GenAI use may be acknowledged in a neutral Methods or Acknowledgments statement.\nEthical Safeguards: GenAI use must not obscure uncertainty, inflate confidence, reduce methodological transparency, or disadvantage collaborators or students with limited access to AI tools.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/05_ai.html#how-do-i-log-my-use-of-genai",
    "href": "chapters/05_ai.html#how-do-i-log-my-use-of-genai",
    "title": "3  GenAI in Science",
    "section": "3.7 How do I log my use of GenAI?",
    "text": "3.7 How do I log my use of GenAI?\nWell, it should be as simple as putting an entry in my field notebook.",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html",
    "href": "chapters/04_syllabus.html",
    "title": "3  Syllabus",
    "section": "",
    "text": "3.1 Location and meeting times\nTuesday/Thursday on Zoom; 11:00AM - 12:15PM (Mountain Time) Spring term 2026",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#location-and-meeting-times",
    "href": "chapters/04_syllabus.html#location-and-meeting-times",
    "title": "Syllabus",
    "section": "Location and meeting times",
    "text": "Location and meeting times\nTuesday/Thursday on Zoom; 11:00AM - 12:15PM (Mountain Time) Spring term 2026",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#office-hours",
    "href": "chapters/04_syllabus.html#office-hours",
    "title": "3  Syllabus",
    "section": "3.2 Office hours",
    "text": "3.2 Office hours\nDue to the nature of the course, I will not have specific office hours for the course. I have extended class times (after our weekly meetings) to allow more time to ask questions and for one-on-one work. These are optional and may be canceled in some weeks depending on your interest in meeting.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#prerequisites",
    "href": "chapters/04_syllabus.html#prerequisites",
    "title": "3  Syllabus",
    "section": "3.3 Prerequisites",
    "text": "3.3 Prerequisites\nTo ensure your success in this course, the following are required: - You must have a dataset to be analyzed this semester. This is very important. This class will only cover data reformatting; we will not cover data processing (except as necessary in specific cases). - Your data analysis must not be used in another (past or present). - You must be at least in your second year of graduate school. - You must have some exposure to using the Program R (tidyverse preferred, but base R is nice too). - You must have taken a statistics course in the last five years.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#assessment-and-grading-standards",
    "href": "chapters/04_syllabus.html#assessment-and-grading-standards",
    "title": "3  Syllabus",
    "section": "3.4 Assessment and Grading Standards",
    "text": "3.4 Assessment and Grading Standards\nThis course is graded as Pass or Fail (technically “Satisfactory” or “Unsatisfactory”). To pass the course you need to do the following:\n\nParticipate at least 80% of the class meetings. Simply inform me of your absences (for health reasons, field research, etc.), and then do what you can to catch up with the work.\nTurn in all assignments on their due dates (see Course Calendar) and receive a Green Light on all assignment submissions by the end of term (see Assignments & Grading). This is especially critical until Spring Break, after which there will be more flexibility in your schedule.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#attendanceparticipation-policy",
    "href": "chapters/04_syllabus.html#attendanceparticipation-policy",
    "title": "3  Syllabus",
    "section": "3.5 Attendance/Participation Policy",
    "text": "3.5 Attendance/Participation Policy\nThis is a graduate level course, and you are here for your own benefit. That being said, I expect you to come to class, stay engaged with the material, and not only learn how to do your own analysis but understand other types of data and analyses by working with your group members. If you do this, you should have analyzed your own data by the end of the semester and have part of a manuscript completed. Please email me ahead of time when you will be unable to attend class for a valid reason.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "href": "chapters/04_syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "title": "3  Syllabus",
    "section": "3.6 How to succeed in the course (beyond your wildest and funniest dreams)",
    "text": "3.6 How to succeed in the course (beyond your wildest and funniest dreams)\nGraduate school can be considerably challenging, as everyone is attempting to juggle research, teaching, classes, health, and family, all while coping with unexpected stressors. Course information is flying at you from every direction; there are many specific terms and concepts that you need to learn and operationalize. So, here are some reminders for you (even though I know you don’t need these):\n\nAsk questions! Even though there are no exams, take copious notes and work collaboratively to build course notes.\nDon’t be afraid to redirect the flow of the course. 5000-level courses should be flexible and fun. I want to give you time to think about and discuss the material. I’m willing to alter the pace of the course, change the order of topics, or devise new exercises for you. This is intended to be fun (while simultaneously transforming you into analytical gurus)! So, just talk to me about how I can help!\nRead all the material in this course guidebook. Many online courses require much more reading; this one does not.\nShow up to as many of the synchronous (Zoom) discussions as you can. When we meet together online, our goal will be to solidify everyone’s understanding of different concepts and how they are linked. These concepts will be useful as you navigate your own analysis project.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#expectations",
    "href": "chapters/04_syllabus.html#expectations",
    "title": "3  Syllabus",
    "section": "3.7 Expectations",
    "text": "3.7 Expectations\nAs your instructor, you should expect me to:\n\nTry my very hardest to make the course go smoothly (the reason you now have this nice new online course guide); but please be prepared for the inevitable hiccups. No matter how hard we all try, there always seem to be a few obstacles (like internet going down for a couple of hours when we’re on Zoom).\nRespond to questions within 24 hours during the work week. However, I likely will not respond during the weekend (unless there is an urgent matter).\nRespect you not only as a learner but as a colleague.\nUnderstand that these are strange and sometimes unforgiving times. We all have varying levels of tolerance and resistance to stress. If you are having a hard time for whatever reason, please communicate with me. I suck at judging, but I can do a hell of a job listening and working with you to solve a problem.\n\nAs a student, you are expected to:\n\nBe respectful of everyone in the class, including me.\nAsk for help if needed.\nTreat your presence in the classroom and your enrollment in this course as you would a job; Act professionally, arrive on time, pay attention, complete your work in a timely and professional manner, and treat your learning seriously.\nUnderstand that everyone is going through different things (family events, etc.), and be understanding of each other.\nBe engaged in the course.\nBe engaged within your assigned groups and help each other learn. Teaching another group member something you know solidifies your own knowledge and also sets you up to be a great future colleague.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#r-code-readings-and-discussion-sections",
    "href": "chapters/04_syllabus.html#r-code-readings-and-discussion-sections",
    "title": "3  Syllabus",
    "section": "3.8 R Code, readings, and discussion sections",
    "text": "3.8 R Code, readings, and discussion sections\nAll R code required for both instruction and hands-on exercises is available within this course book. Unlike past versions of this course, the present iteration no longer has traditional lectures. That said, I may respond to your questions by creating mini-presentations for you. I will share such material immediately after our discussions.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#classroom-climate-and-conduct",
    "href": "chapters/04_syllabus.html#classroom-climate-and-conduct",
    "title": "3  Syllabus",
    "section": "3.9 Classroom Climate and Conduct",
    "text": "3.9 Classroom Climate and Conduct\nAgain, you will be respectful towards your classmates and your instructors. Spirited debate and disagreement are to be expected in any classroom, and all perspectives will be heard, but we will behave civilly and with respect towards one another. Personal attacks, offensive language, name-calling, and dismissive gestures (eye-rolling, saying “whatever”, etc.) are not warranted in a learning atmosphere. Plus, in my opinion, such behavior shows an ability to problem-solve, which is counter to the mission of any university. As your instructor, I have the right to dismiss you from the classroom if you engage in disrespectful or disruptive behavior. Lastly, for the privacy of your fellow students, please do not record the lectures (unless with permission of the instructor).\n\n\n\n\n\n\nNoteA note on knowledge-sharing\n\n\n\n\n\nOur classroom is a shared intellectual space. Questions, mistakes, and partial understanding are part of learning. Please remember that supporting one another’s intellectual growth matters more than performative corrections or demonstrations of expertise.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#classroom-statement-on-diversity",
    "href": "chapters/04_syllabus.html#classroom-statement-on-diversity",
    "title": "3  Syllabus",
    "section": "3.10 Classroom Statement on Diversity",
    "text": "3.10 Classroom Statement on Diversity\nThe University of Wyoming values an educational environment that is diverse, equitable, and inclusive. The diversity that students and faculty bring to class, including age, country of origin, culture, disability, economic class, ethnicity, gender identity, immigration status, linguistic, political affiliation, race, religion, sexual orientation, veteran status, worldview, and other social and cultural diversity is valued, respected, and considered a resource for learning. we understand that our UW community members represent a rich variety of backgrounds and perspectives. We are committed to providing an atmosphere for learning that respects diversity of all types. While working together to build this community, we ask all members–from students to staff to faculty–to:\n\nBe transparent about pre-existing biases and beliefs.\nDo not hesitate to share their unique experiences and perspectives.\nBe open to the views of others.\nHonor the uniqueness of their colleagues.\nAppreciate the opportunity that we have to learn from each other in this community.\nValue each other’s opinions and communicate in a respectful manner.\nKeep confidential any discussions of a personal (or professional) nature.\nUse this opportunity together to discuss ways in which we can create an inclusive environment in this course and across the University community.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#duty-to-report",
    "href": "chapters/04_syllabus.html#duty-to-report",
    "title": "3  Syllabus",
    "section": "3.11 Duty to Report",
    "text": "3.11 Duty to Report\nUW faculty are committed to supporting students and upholding the University’s non-discrimination policy. Under Title IX, discrimination based upon sex and gender is prohibited. If you experience an incident of sex- or gender-based discrimination, we encourage you to report it. While you may talk to a faculty member, understand that as a “Responsible Employee” of the University, the faculty member is required to report information you share about the incident to the University’s Title IX Coordinator (you may choose whether you or anyone involved is identified by name). If you would like to speak with someone who may be able to afford you privacy or confidentiality, there are people who can meet with you. Faculty can help direct you or you may find info about UW policy and resources at http://www.uwyo.edu/reportit. While we want you to feel comfortable coming to us with issues you may be struggling with or concerns you may be having, please be aware that we have some reporting requirements that are part of our job requirements at UW. You do not have to go through the experience alone. For example, if you inform us of an issue of sexual harassment, sexual assault, or discrimination we will keep the information as private as we can, but we am required to bring it to the attention of the institution’s Title IX Coordinator. If you would like to talk to those offices directly, you can contact Equal Opportunity Report and Response (Bureau of Mines Room 319, 766-5200, report-it@uwyo.edu, www.uwyo.edu/reportit). Additionally, you can also report incidents or complaints to the UW Police Department. You can also get support at the STOP Violence program (stopviolence@uwyo.edu, www.uwyo.edu/stop, 766-3296) (or SAFE Project (www.safeproject.org, campus@safeproject.org, 766-3434, 24-Hour hotline: 745-3556). Assistance and resources are available, and you are not required to make a formal complaint or participate in an investigation to access them. Another common example is if you are struggling with an issue that may be traumatic, or under unusual stress. We will likely inform the Dean of Students Office or Counseling Center. If you would like to reach out directly to them for assistance, you can contact them by going to www.uwyo.edu/dos/uwyocares.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#disability-statement",
    "href": "chapters/04_syllabus.html#disability-statement",
    "title": "3  Syllabus",
    "section": "3.12 Disability Statement",
    "text": "3.12 Disability Statement\nIf you have a physical, learning, sensory or psychological disability and require accommodations, please let me know as soon as possible. You will need to register with, and provide documentation of your disability to University Disability Support Services (UDSS) in SEO, room 330 Knight Hall.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/04_syllabus.html#academic-honesty",
    "href": "chapters/04_syllabus.html#academic-honesty",
    "title": "3  Syllabus",
    "section": "3.13 Academic Honesty",
    "text": "3.13 Academic Honesty\nThe University of Wyoming is built upon a strong foundation of integrity, respect and trust. All members of the university community have a responsibility to be honest and the right to expect honesty from others. Any form of academic dishonesty is unacceptable to our community and will not be tolerated [from the University Catalog]. Teachers and students should report suspected violations of standards of academic honesty to the instructor, department head, or dean. Other University regulations can be found here",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/03_course_roadmap.html",
    "href": "chapters/03_course_roadmap.html",
    "title": "6  Course Roadmap",
    "section": "",
    "text": "6.1 Course Roadmap\nThe course progresses as follows:\nGiven the scope of the entire course, we may only scratch the surface of some topics. Even our cursory treatment will convince you why these subjects matter for scientists who have messy data.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "chapters/03_course_roadmap.html#course-roadmap",
    "href": "chapters/03_course_roadmap.html#course-roadmap",
    "title": "6  Course Roadmap",
    "section": "",
    "text": "Metrology\nData exploration, outliers, visual inspection\nData heterogeneity\nGeneralized Linear Models (GLMs)\nGeneral principles of model validation\nGeneralized Linear Mixed Models (GLMMs)\nInformation Theory of Model Selection\nGeneralized Additive (Mixed) Models (GAMMs)\nSpatial and Temporal autocorrelation\nStructural Causal Models (but still not Bayesian)",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "chapters/03_course_roadmap.html#how-this-course-is-structured",
    "href": "chapters/03_course_roadmap.html#how-this-course-is-structured",
    "title": "6  Course Roadmap",
    "section": "6.2 How This Course Is Structured",
    "text": "6.2 How This Course Is Structured\nWe will meet twice weekly via Zoom. work in small groups, and you will also work extensively with your own data. By early October (around October 4), your dataset should be in working order—not just technically usable, but conceptually ready to explain to others.\n\n\n\n\n\n\nNote\n\n\n\nWorking order does not mean “a single spreadsheet.”\nA dataset is only complete if it includes metadata. Metadata describe: - what the variables are\n- how the data were collected\n- under what conditions\n- with what assumptions and limitations\nIf you have questions about metadata, I have a dedicated lecture on metadata structures, and I strongly encourage you to reach out.\n\n\n\n6.2.1 What to expect next\n\n[Describe what will happen in the remainder of the first class]\n[Outline upcoming assignments or activities]",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "chapters/03_course_roadmap.html#getting-started",
    "href": "chapters/03_course_roadmap.html#getting-started",
    "title": "3  Course Roadmap",
    "section": "3.3 Getting Started",
    "text": "3.3 Getting Started\n\n3.3.1 Who are you?\n\n[Prompt students to introduce themselves]\n[Encourage sharing of disciplinary background]\n\n\n\n3.3.2 Your data\n\n[Ask students to describe the data they work with or hope to work with]\n[Surface common challenges and anxieties]\n\n\n\n3.3.3 Your concerns and expectations\n\n[Invite discussion of fears, gaps, or uncertainties]",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Roadmap</span>"
    ]
  },
  {
    "objectID": "chapters/06_preparing_yourself.html#getting-started",
    "href": "chapters/06_preparing_yourself.html#getting-started",
    "title": "6  Preparing Yourself",
    "section": "",
    "text": "6.1.1 Who are you?\n\n[Prompt students to introduce themselves]\n[Encourage sharing of disciplinary background]\n\n\n\n6.1.2 Your data\n\n[Ask students to describe the data they work with or hope to work with]\n[Surface common challenges and anxieties]\n\n\n\n6.1.3 Your concerns and expectations\n\n[Invite discussion of fears, gaps, or uncertainties]",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparing Yourself</span>"
    ]
  },
  {
    "objectID": "chapters/05_ai.html#timeline",
    "href": "chapters/05_ai.html#timeline",
    "title": "5  GenAI in Science",
    "section": "5.2 Timeline",
    "text": "5.2 Timeline\nThis documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI in Science</span>"
    ]
  },
  {
    "objectID": "chapters/resources.html",
    "href": "chapters/resources.html",
    "title": "1  resources",
    "section": "",
    "text": "2 Course Resources\n\nSyllabus\nAI use policy\nZoom link",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>resources</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html",
    "href": "chapters/calendar_sp26.html",
    "title": "4  Course calendar",
    "section": "",
    "text": "4.1 Calendar overview\nUse the calendar table below to see what’s coming each week; simply click a week (first column link) to jump to full details, and then expand sections to see what to do before, during, and after class. This calendar may change slightly as we progress through the term. You will receive immediate notification of any and all changes.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#course-calendar",
    "href": "chapters/calendar_sp26.html#course-calendar",
    "title": "2  calendar_sp26",
    "section": "",
    "text": "Week\nTopics\nIn-class exercise\nMilestone assignment\nStress level\nGrade weight (%)\n\n\n\n\n1 (19-Jan-2026)\nCourse intro; RStudio setup; AI guardrails\nProject scoping discussion\n—\nVery low\n5\n\n\n2 (26-Jan-2026)\nFile structure; reproducible workflows; AI logs\nFile audit walkthrough\nAnalysis Concept Note\nLow\n5\n\n\n3 (02-Feb-2026)\nData exploration; measurement & uncertainty\nExploratory data analysis walkthrough\n—\nVery low\n0\n\n\n4 (09-Feb-2026)\nGLMs (theory + practice)\nGLM exercise\nData Readiness Note\nLow\n10\n\n\n5 (16-Feb-2026)\nGLMs; AIC & information theory\nModel comparison exercise\n—\nLow\n5\n\n\n6 (23-Feb-2026)\nGLMMs; effective sample size\nGLMM exercise\n—\nModerate\n15\n\n\n7 (02-Mar-2026)\nGAMs\nGAM exercise\nWorking Model (draft)\nModerate\n0\n\n\n8 (09-Mar-2026)\nGAMMs; spatial & temporal heterogeneity\nModel refinement exercise\nWorking Model (final lock)\nModerate\n15\n\n\n9 (16-Mar-2026)\nStudent Spring Break\n—\n—\nNone\n0\n\n\n10 (23-Mar-2026)\nStructural Causal Modeling (SCM)\nCausal diagram critique\n—\nLow\n10\n\n\n11 (30-Mar-2026)\nInstructor Spring Break\n—\n—\nNone\n0\n\n\n12 (06-Apr-2026)\nPrediction & uncertainty\nPrediction checks\nInterpretation Memo\nLow\n10\n\n\n13 (13-Apr-2026)\nSynthesis & justification\nPeer + AI review\n—\nModerate\n0\n\n\n14 (20-Apr-2026)\nTables, figures, reporting standards\nTable/figure workshop\nResults Section\nLow\n10\n\n\n15 (27-Apr-2026)\nWriting Results sections\nDraft clinic\nFull Draft\nModerate\n15\n\n\n16 (04-May-2026)\nReflection & closure\nCourse wrap-up\nRevision Plan (not executed)\nNone\n5",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>calendar_sp26</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-1",
    "href": "chapters/calendar_sp26.html#week-1",
    "title": "2  calendar_sp26",
    "section": "2.3 Week 1",
    "text": "2.3 Week 1\nDate: 19 Jan 2026\nTopics: Course introduction; RStudio setup; AI guardrails\nStress level: Very low\nGrade weight: 5%\n\n2.3.1 Before class\n\nReview syllabus\nInstall R and RStudio\nRead AI use and documentation expectations\n\n\n\n2.3.2 In class\n\nCourse overview and expectations\nProject scoping discussion\nInitial workflow setup\n\n\n\n2.3.3 After class\n\nNo submission due\nBegin thinking about a potential project dataset or question",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>calendar_sp26</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-2",
    "href": "chapters/calendar_sp26.html#week-2",
    "title": "2  Course calendar",
    "section": "2.2 Week 2",
    "text": "2.2 Week 2\nDate: 26 Jan 2026\nTopics: File structure; reproducible workflows; AI logs\nStress level: Low\nGrade weight: 5%\n\n2.2.1 Before class\n\nReview example project directory structures\nRead guidance on reproducible workflows\n\n\n\n2.2.2 In class\n\nFile audit walkthrough\nDiscussion of AI interaction logging\n\n\n\n2.2.3 After class\n\nSubmit: Analysis Concept Note",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-3",
    "href": "chapters/calendar_sp26.html#week-3",
    "title": "2  Course calendar",
    "section": "2.3 Week 3",
    "text": "2.3 Week 3\nDate: 02 Feb 2026\nTopics: Data exploration; measurement & uncertainty\nStress level: Very low\nGrade weight: 0%\n\n2.3.1 Before class\n\nSkim examples of exploratory data analysis (EDA)\nReview notes on uncertainty and measurement error\n\n\n\n2.3.2 In class\n\nExploratory data analysis walkthrough\nDiscussion of data limitations and noise\n\n\n\n2.3.3 After class\n\nNo submission due\nBegin informal exploration of your own data",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-4",
    "href": "chapters/calendar_sp26.html#week-4",
    "title": "2  Course calendar",
    "section": "2.4 Week 4",
    "text": "2.4 Week 4\nDate: 09 Feb 2026\nTopics: GLMs (theory + practice)\nStress level: Low\nGrade weight: 10%\n\n2.4.1 Before class\n\nRead GLM conceptual overview\nReview examples of common link functions\n\n\n\n2.4.2 In class\n\nGLM exercise\nTranslating scientific questions into model form\n\n\n\n2.4.3 After class\n\nSubmit: Data Readiness Note",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-5",
    "href": "chapters/calendar_sp26.html#week-5",
    "title": "2  Course calendar",
    "section": "2.5 Week 5",
    "text": "2.5 Week 5\nDate: 16 Feb 2026\nTopics: GLMs; AIC & information theory\nStress level: Low\nGrade weight: 5%\n\n2.5.1 Before class\n\nReview information-theoretic model comparison\nRead example AIC workflows\n\n\n\n2.5.2 In class\n\nModel comparison exercise\nInterpreting relative support\n\n\n\n2.5.3 After class\n\nNo submission due\nRefine candidate model sets",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-6",
    "href": "chapters/calendar_sp26.html#week-6",
    "title": "2  Course calendar",
    "section": "2.6 Week 6",
    "text": "2.6 Week 6\nDate: 23 Feb 2026\nTopics: GLMMs; effective sample size\nStress level: Moderate\nGrade weight: 15%\n\n2.6.1 Before class\n\nRead GLMM overview\nReview random effects motivation\n\n\n\n2.6.2 In class\n\nGLMM exercise\nDiscussion of pseudoreplication and effective sample size\n\n\n\n2.6.3 After class\n\nNo submission due\nBegin transitioning models to mixed frameworks where appropriate",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-7",
    "href": "chapters/calendar_sp26.html#week-7",
    "title": "2  Course calendar",
    "section": "2.7 Week 7",
    "text": "2.7 Week 7\nDate: 02 Mar 2026\nTopics: GAMs\nStress level: Moderate\nGrade weight: 0%\n\n2.7.1 Before class\n\nReview motivation for smooth terms\nRead conceptual introduction to GAMs\n\n\n\n2.7.2 In class\n\nGAM exercise\nInterpreting smooths vs parametric effects\n\n\n\n2.7.3 After class\n\nSubmit: Working Model (draft)",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-8",
    "href": "chapters/calendar_sp26.html#week-8",
    "title": "2  Course calendar",
    "section": "2.8 Week 8",
    "text": "2.8 Week 8\nDate: 09 Mar 2026\nTopics: GAMMs; spatial & temporal heterogeneity\nStress level: Moderate\nGrade weight: 15%\n\n2.8.1 Before class\n\nReview examples of spatial and temporal structure\nRead GAMM case studies\n\n\n\n2.8.2 In class\n\nModel refinement exercise\nDiagnosing remaining structure in residuals\n\n\n\n2.8.3 After class\n\nSubmit: Working Model (final lock)",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-9",
    "href": "chapters/calendar_sp26.html#week-9",
    "title": "2  Course calendar",
    "section": "2.9 Week 9",
    "text": "2.9 Week 9\nDate: 16 Mar 2026\nTopics: Student Spring Break\nStress level: None\nGrade weight: 0%\n\n2.9.1 Notes\n\nNo class\nNo assignments due\nRecommended: rest, catch up, or light review if needed",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-10",
    "href": "chapters/calendar_sp26.html#week-10",
    "title": "2  Course calendar",
    "section": "2.10 Week 10",
    "text": "2.10 Week 10\nDate: 23 Mar 2026\nTopics: Structural Causal Modeling (SCM)\nStress level: Low\nGrade weight: 10%\n\n2.10.1 Before class\n\nRead SCM overview and motivation\nReview example causal diagrams\n\n\n\n2.10.2 In class\n\nCausal diagram critique\nDiscussion of assumptions and identification\n\n\n\n2.10.3 After class\n\nNo submission due\nConsider how SCM reframes your modeling decisions",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-11",
    "href": "chapters/calendar_sp26.html#week-11",
    "title": "2  Course calendar",
    "section": "2.11 Week 11",
    "text": "2.11 Week 11\nDate: 30 Mar 2026\nTopics: Instructor Spring Break\nStress level: None\nGrade weight: 0%\n\n2.11.1 Notes\n\nNo class\nNo assignments due",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-12",
    "href": "chapters/calendar_sp26.html#week-12",
    "title": "2  Course calendar",
    "section": "2.12 Week 12",
    "text": "2.12 Week 12\nDate: 06 Apr 2026\nTopics: Prediction & uncertainty\nStress level: Low\nGrade weight: 10%\n\n2.12.1 Before class\n\nReview prediction vs inference distinctions\nRead examples of uncertainty communication\n\n\n\n2.12.2 In class\n\nPrediction checks\nEvaluating extrapolation risk\n\n\n\n2.12.3 After class\n\nSubmit: Interpretation Memo",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-13",
    "href": "chapters/calendar_sp26.html#week-13",
    "title": "2  Course calendar",
    "section": "2.13 Week 13",
    "text": "2.13 Week 13\nDate: 13 Apr 2026\nTopics: Synthesis & justification\nStress level: Moderate\nGrade weight: 0%\n\n2.13.1 Before class\n\nReview synthesis examples from prior studies\n\n\n\n2.13.2 In class\n\nPeer + AI review\nJustifying analytical decisions\n\n\n\n2.13.3 After class\n\nNo submission due\nPrepare figures and tables for results",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-14",
    "href": "chapters/calendar_sp26.html#week-14",
    "title": "2  Course calendar",
    "section": "2.14 Week 14",
    "text": "2.14 Week 14\nDate: 20 Apr 2026\nTopics: Tables, figures, reporting standards\nStress level: Low\nGrade weight: 10%\n\n2.14.1 Before class\n\nReview reporting guidelines\nExamine good and bad figure examples\n\n\n\n2.14.2 In class\n\nTable/figure workshop\nEmphasis on clarity and restraint\n\n\n\n2.14.3 After class\n\nSubmit: Results Section",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-15",
    "href": "chapters/calendar_sp26.html#week-15",
    "title": "2  Course calendar",
    "section": "2.15 Week 15",
    "text": "2.15 Week 15\nDate: 27 Apr 2026\nTopics: Writing Results sections\nStress level: Moderate\nGrade weight: 15%\n\n2.15.1 Before class\n\nReview example Results sections\nReflect on narrative flow\n\n\n\n2.15.2 In class\n\nDraft clinic\nFocused feedback on structure and interpretation\n\n\n\n2.15.3 After class\n\nSubmit: Full Draft",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#week-16",
    "href": "chapters/calendar_sp26.html#week-16",
    "title": "2  Course calendar",
    "section": "2.16 Week 16",
    "text": "2.16 Week 16\nDate: 04 May 2026\nTopics: Reflection & closure\nStress level: None\nGrade weight: 5%\n\n2.16.1 Before class\n\nReview feedback on full draft\n\n\n\n2.16.2 In class\n\nCourse wrap-up\nReflection on analytical growth\n\n\n\n2.16.3 After class\n\nSubmit: Revision Plan (not executed)",
    "crumbs": [
      "Course resources",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#weekly-information",
    "href": "chapters/calendar_sp26.html#weekly-information",
    "title": "4  Course calendar",
    "section": "4.2 Weekly Information",
    "text": "4.2 Weekly Information\nClick to expand sections below to see expectations before, during, and after class.\n\n\n\n\n\n\nTipWeek 1 (19-Jan-2026)\n\n\n\n\n\nTopics: Course introduction; RStudio setup; AI guardrails\nStress level: Very low\nGrade weight: 0%\n\nBefore class\n\nAbsolutely nothing.\n\n\n\nIn class\n\nCourse overview and expectations\nReview syllabus\nRead AI policy and documentation expectations\nProject scoping discussion\n\n\n\nAfter class\n\nNo submission due\nInstall R and RStudio\nRead AI use and documentation expectations\nBegin gathering your dataset files\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 2 (26-Jan-2026)\n\n\n\n\n\nTopics: File structure; reproducible workflows; AI logs\nStress level: Low\nGrade weight: 5%\n\nBefore class\n\nReview example project directory structures\nRead guidance on reproducible workflows\n\n\n\nIn class\n\nFile audit walkthrough\nDiscussion of AI interaction logging\n\n\n\nAfter class\n\nSubmit: Analysis Concept Note\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 3 (02-Feb-2026)\n\n\n\n\n\nTopics: Data exploration; measurement & uncertainty\nStress level: Very low\nGrade weight: 5%\n\nBefore class\n\nSkim examples of exploratory data analysis (EDA)\nReview notes on uncertainty and measurement error\n\n\n\nIn class\n\nExploratory data analysis walkthrough\nDiscussion of data limitations and noise\n\n\n\nAfter class\n\nNo submission due\nBegin informal exploration of your own data\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 4 (09-Feb-2026)\n\n\n\n\n\nTopics: GLMs (theory + practice)\nStress level: Low\nGrade weight: 10%\n\nBefore class\n\nRead GLM conceptual overview\nReview examples of common link functions\n\n\n\nIn class\n\nGLM exercise\nTranslating scientific questions into model form\n\n\n\nAfter class\n\nSubmit: Data Readiness Note\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 5 (16-Feb-2026)\n\n\n\n\n\nTopics: GLMs; AIC & information theory\nStress level: Low\nGrade weight: 5%\n\nBefore class\n\nReview information-theoretic model comparison\nRead example AIC workflows\n\n\n\nIn class\n\nModel comparison exercise\nInterpreting relative support\n\n\n\nAfter class\n\nNo submission due\nRefine candidate model sets\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 6 (23-Feb-2026)\n\n\n\n\n\nTopics: GLMMs; effective sample size\nStress level: Moderate\nGrade weight: 15%\n\nBefore class\n\nRead GLMM overview\nReview motivation for random effects\n\n\n\nIn class\n\nGLMM exercise\nDiscussion of pseudoreplication and effective sample size\n\n\n\nAfter class\n\nNo submission due\nBegin transitioning models to mixed frameworks where appropriate\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 7 (02-Mar-2026)\n\n\n\n\n\nTopics: GAMs\nStress level: Moderate\nGrade weight: 0%\n\nBefore class\n\nReview motivation for smooth terms\nRead conceptual introduction to GAMs\n\n\n\nIn class\n\nGAM exercise\nInterpreting smooths vs parametric effects\n\n\n\nAfter class\n\nSubmit: Working Model (draft)\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 8 (09-Mar-2026)\n\n\n\n\n\nTopics: GAMMs; spatial & temporal heterogeneity\nStress level: Moderate\nGrade weight: 15%\n\nBefore class\n\nReview examples of spatial and temporal structure\nRead GAMM case studies\n\n\n\nIn class\n\nModel refinement exercise\nDiagnosing remaining structure in residuals\n\n\n\nAfter class\n\nSubmit: Working Model (final lock)\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 9 (16-Mar-2026)\n\n\n\n\n\nTopics: Student Spring Break\nStress level: None\nGrade weight: 0%\n\nNotes\n\nNo class\nNo assignments due\nRecommended: rest, catch up, or light review if needed\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 10 (23-Mar-2026)\n\n\n\n\n\nTopics: Structural Causal Modeling (SCM)\nStress level: Low\nGrade weight: 10%\n\nBefore class\n\nRead SCM overview and motivation\nReview example causal diagrams\n\n\n\nIn class\n\nCausal diagram critique\nDiscussion of assumptions and identification\n\n\n\nAfter class\n\nNo submission due\nConsider how SCM reframes your modeling decisions\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 11 (30-Mar-2026)\n\n\n\n\n\nTopics: Instructor Spring Break\nStress level: None\nGrade weight: 0%\n\nNotes\n\nNo class\nNo assignments due\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 12 (06-Apr-2026)\n\n\n\n\n\nTopics: Prediction & uncertainty\nStress level: Low\nGrade weight: 10%\n\nBefore class\n\nReview prediction vs inference distinctions\nRead examples of uncertainty communication\n\n\n\nIn class\n\nPrediction checks\nEvaluating extrapolation risk\n\n\n\nAfter class\n\nSubmit: Interpretation Memo\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 13 (13-Apr-2026)\n\n\n\n\n\nTopics: Synthesis & justification\nStress level: Moderate\nGrade weight: 0%\n\nBefore class\n\nReview synthesis examples from prior studies\n\n\n\nIn class\n\nPeer + AI review\nJustifying analytical decisions\n\n\n\nAfter class\n\nNo submission due\nPrepare figures and tables for results\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 14 (20-Apr-2026)\n\n\n\n\n\nTopics: Tables, figures, reporting standards\nStress level: Low\nGrade weight: 10%\n\nBefore class\n\nReview reporting guidelines\nExamine good and bad figure examples\n\n\n\nIn class\n\nTable/figure workshop\nEmphasis on clarity and restraint\n\n\n\nAfter class\n\nSubmit: Results Section\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 15 (27-Apr-2026)\n\n\n\n\n\nTopics: Writing Results sections\nStress level: Moderate\nGrade weight: 15%\n\nBefore class\n\nReview example Results sections\nReflect on narrative flow\n\n\n\nIn class\n\nDraft clinic\nFocused feedback on structure and interpretation\n\n\n\nAfter class\n\nSubmit: Full Draft\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 16 (04-May-2026)\n\n\n\n\n\nTopics: Reflection & closure\nStress level: None\nGrade weight: 5%\n\nBefore class\n\nReview feedback on full draft\n\n\n\nIn class\n\nCourse wrap-up\nReflection on analytical growth\n\n\n\nAfter class\n\nSubmit: Revision Plan (not executed)",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#calendar-overview",
    "href": "chapters/calendar_sp26.html#calendar-overview",
    "title": "4  Course calendar",
    "section": "",
    "text": "Week\nTopics\nIn-class exercise\nMilestone assignment\nStress level\nGrade weight (%)\n\n\n\n\nWeek 119-Jan-2026\nCourse intro; RStudio setup; AI guardrails\nProject scoping discussion\n—\nVery low\n5\n\n\nWeek 226-Jan-2026\nFile structure; reproducible workflows; AI logs\nFile audit walkthrough\nAnalysis Concept Note\nLow\n5\n\n\nWeek 302-Feb-2026\nData exploration; measurement & uncertainty\nExploratory data analysis walkthrough\n—\nVery low\n0\n\n\nWeek 409-Feb-2026\nGLMs (theory + practice)\nGLM exercise\nData Readiness Note\nLow\n10\n\n\nWeek 516-Feb-2026\nGLMs; AIC & information theory\nModel comparison exercise\n—\nLow\n5\n\n\nWeek 623-Feb-2026\nGLMMs; effective sample size\nGLMM exercise\n—\nModerate\n15\n\n\nWeek 702-Mar-2026\nGAMs\nGAM exercise\nWorking Model (draft)\nModerate\n0\n\n\nWeek 809-Mar-2026\nGAMMs; spatial & temporal heterogeneity\nModel refinement exercise\nWorking Model (final lock)\nModerate\n15\n\n\nWeek 916-Mar-2026\nStudent Spring Break\n—\n—\nNone\n0\n\n\nWeek 1023-Mar-2026\nStructural Causal Modeling (SCM)\nCausal diagram critique\n—\nLow\n10\n\n\nWeek 1130-Mar-2026\nInstructor Spring Break\n—\n—\nNone\n0\n\n\nWeek 1206-Apr-2026\nPrediction & uncertainty\nPrediction checks\nInterpretation Memo\nLow\n10\n\n\nWeek 1313-Apr-2026\nSynthesis & justification\nPeer + AI review\n—\nModerate\n0\n\n\nWeek 1420-Apr-2026\nTables, figures, reporting standards\nTable/figure workshop\nResults Section\nLow\n10\n\n\nWeek 1527-Apr-2026\nWriting Results sections\nDraft clinic\nFull Draft\nModerate\n15\n\n\nWeek 1604-May-2026\nReflection & closure\nCourse wrap-up\nRevision Plan (not executed)\nNone\n5",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "index.html#topics-covered",
    "href": "index.html#topics-covered",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "Topics covered",
    "text": "Topics covered\nThe course covers the following topics, roughly in the following order:\n\nMetrology: measurands, measurement error, uncertainty, and data quality\n\nExploratory data analysis of messy data\n\nGeneralized Linear Models (GLMs): theory and implementation\n\nModel comparison using AIC and information-theoretic approaches\n\nGeneralized Linear Mixed Models (GLMMs) and effective sample size\n\nGeneralized Additive Models (GAMs)\n\nGeneralized Additive Mixed Models (GAMMs)\n\nModeling spatial and temporal heterogeneity\n\nStructural Causal Modeling (SCM) and causal diagrams\n\nPrediction, uncertainty propagation, and model validation\n\nSynthesis of analytical results and justification of inference\n\nTables, figures, and reporting standards for quantitative results\n\nWriting defensible and interpretable Results sections\n\nGiven this ambitious scope, we may only scratch the surface of some topics. Even our cursory treatment is aimed at convincing you why these subjects matter for scientists who have messy data.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Information</span>"
    ]
  },
  {
    "objectID": "chapters/06_preparing_yourself.html#what-do-i-need-to-do-to-get-started-in-this-class",
    "href": "chapters/06_preparing_yourself.html#what-do-i-need-to-do-to-get-started-in-this-class",
    "title": "6  Preparing Yourself",
    "section": "",
    "text": "Join the ZOO-5500 Slack channel\nDownload, Install, and set up RStudio Desktop\nGather your dataset files for your analysis\n\n\n6.1.1 Join the ZOO-5500 Slack channel\nJust click on the list and follow instructions. We will use Slack to encourage ongoing, low-stakes discussion within the class. This will be for sharing questions, ideas, and insights in real time so that learning can continue outside of scheduled class meetings. This allows everyone (even me) to get answers to questions that I might be thinking about at odd times. It also allows for problems to be solved faster; rather than waiting until our synchronous Zoom session to ask about an issue, we can group-think over the course of the week. Remember that this is set up to improve collaboration; it does not mean that anyone should be responding outside of work hours or on the weekends. Below are the preferred channels for different kinds of communication:\n\n#announcements &gt;Read-only channel for important course information like deadlines, corrections, and post-class clarifications. Check this regularly.\n#course-questions &gt;For general course or assignment questions (due dates, unclear wording on websites) that others might also have. Expect peer responses and occasional instructor follow-ups.\n#data-help &gt;For data wrangling and technical data issues (imports, joins, NAs, plots). Not for model choice or interpretation.\n#model-talk &gt;For conceptual questions about models and assumptions. Discussion-focused; no code debugging.\n#analysis-workflow &gt;For questions about reproducibility, organization, reporting, and good analytical habits.\n#ai-use-and-logs For discussing allowed AI use and posting AI interaction logs when requested. No private communication or graded work.\n#papers-good-bad-ugly &gt;For highlighting examples of good and bad analyses in published work.\n#random &gt;For off-topic chat, random links, and fun stuff. This will help keep everything else focused.\n\n\n\n6.1.2 Download, Install, and set up RStudio Desktop\nInstall on your local machine. All coding and file organization will be done in RStudio. Note that the following is a very basic way of creating a set of project folders. You will see boxes with R code. Feel free to copy and run these, if necessary. Here are the basic steps:\n\nDownload and install RStudio Desktop. Navigate to your folder (in this case, your student folder).\nIn “Files” window (lower right panel in RStudio), click “More/Set as Working Directory”. This sets your working directory; this is always a good idea to do.\nCreate a New R Project in your student folder: “File/New Project/Existing Directory”. Name this project appropriately.\nCreate your File structure: Refer to this RStudio tutorial. In this class, we will modify this set of directories to make things a bit easier and more transparent for you. After completing the following steps, place your dataset in your “data/raw_data” folder (after you create it below). Navigate to this folder on your computer and drag-and-drop or copy-paste your data file(s) into this folder.\n\nCreate three folders (one suggested framework):\n\n/data (for raw, processed, clean datasets)\n/r_scripts (for R scripts, each containing a set of related functions)\n/output output (for models, graphs, tables)\n\nWe can do this by staying in our directory (within RStudio) and then using the dir.create function.\n\ndir.create(\"data\")\n\nYou can see your new folder (“data”) appear in the Files in the lower right window within RStudio. Now let’s create the other two folders.\n\ndir.create(\"r_scripts\")\ndir.create(\"output\")\n\nTo help with data processing (i.e. cleaning and organization)–something that we will discuss in coming days within the context of your analytical workflow–, we will create some subfolders within your newly created “data” folder. Let’s do this now. Note how you create a folder within the “data” folder by specifying the path:\n\ndir.create(\"data/raw_data\")\ndir.create(\"data/processed_data\")\ndir.create(\"data/metadata\")\n\nAnd that is how you create a basic R Project in RStudio! What you have done is create a set of folders and subfolders that have names that can be easily understood by other users. Feel free to create other subfolders now. Or, should you not like the file nomenclature used above, change the names to whatever you wish. Just be sure that this structure is as simple as possible so that other users understand what you have done.\nThere is an added benefit to creating an R Project in this way. What you have done is create a minimally reproducible data structure. There are two primary ways this can be accomplished (if you are not familiar with this. First method: A collaborator (or instructor or classmate) can simply navigate to your subfolder, set that as their working directory from within RStudio, and then open the .RProj file. Alternatively, you can create zip your root folder (the one labelled as your name, in this case) and then share your entire analysis, etc. All the recipient would need to do is unzip the folder, open the .Rproj file, and then “Set as Working Directory.”\n\n\n6.1.3 Gather your dataset files for your analysis\nPlace your data in your “data/raw_data” subfolder and then confirm that your files are present by running the list.files function.\n\nlist.files(\"data/raw_data\")\n\n\n\n\n\n\n\nNote\n\n\n\nWorking order means more than a spreadsheet or two.\nA dataset is only complete if it includes metadata. Metadata describe: - what the variables are\n- how the data were collected\n- under what conditions\n- with what assumptions and limitations\nIf you have questions about metadata, I have an old, dedicated lecture on metadata structures, and I strongly encourage you to reach out.\n\n\nYou are now ready to move to the next steps of loading necessary R packages and beginning your journey of data exploration!",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparing Yourself</span>"
    ]
  },
  {
    "objectID": "index.html#quicklinks",
    "href": "index.html#quicklinks",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "Quicklinks",
    "text": "Quicklinks\nCollaborative Statistics References",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Information</span>"
    ]
  },
  {
    "objectID": "chapters/02_course_intro.html#topics-covered",
    "href": "chapters/02_course_intro.html#topics-covered",
    "title": "2  Introduction",
    "section": "Topics covered",
    "text": "Topics covered\nThe course covers the following topics, roughly in the following order:\n\nMetrology: measurands, measurement error, uncertainty, and data quality\n\nExploratory data analysis of messy data\n\nGeneralized Linear Models (GLMs): theory and implementation\n\nModel comparison using AIC and information-theoretic approaches\n\nGeneralized Linear Mixed Models (GLMMs) and effective sample size\n\nGeneralized Additive Models (GAMs)\n\nGeneralized Additive Mixed Models (GAMMs)\n\nModeling spatial and temporal heterogeneity\n\nStructural Causal Modeling (SCM) and causal diagrams\n\nPrediction, uncertainty propagation, and model validation\n\nSynthesis of analytical results and justification of inference\n\nTables, figures, and reporting standards for quantitative results\n\nWriting defensible and interpretable Results sections\n\nGiven this ambitious scope, we may only scratch the surface of some topics. Even our cursory treatment is aimed at convincing you why these subjects matter for scientists who have messy data.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html",
    "href": "chapters/assignment_descriptions.html",
    "title": "Milestone Assignments",
    "section": "",
    "text": "This page describes the milestone assignments referenced in the course calendar. Each section below explains the purpose of the milestone, what to submit, and what “good” typically looks like.\n\n\n\n\n\n\nNoteAnalysis Concept Note (Week 2)\n\n\n\n\n\nPurpose:\nLock in a feasible analysis plan early, so you’re not improvising later. This is about scope, clarity, and workflow—not results.\n\nWhat to include\n\nWorking title (can change later)\nYour focal question (1–3 sentences; plain language)\nStudy system and units of analysis (what is one “row,” conceptually?)\nResponse variable(s) and predictor(s) you expect to use (draft list is fine)\nHypothesized relationships\n\nShort bullets; include directionality when possible\n\nData source and file plan\n\nWhere the data live\n\nWhat you will name files\n\nWhat the folder structure will be\n\nPlanned model family (tentative)\n\ne.g., GLM / GLMM / GAM / GAMM, and why that’s your first guess\n\nPotential complications you already suspect\n\nMissing data, non-independence, zero inflation, seasonality, etc.\n\n\n\n\nSubmission format\n\n1–2 pages (PDF) or a short Quarto/Markdown page\nClear headings (don’t bury the key information)\n\n\n\n“Good” looks like\n\nA question that is answerable with the data you actually have\nReasonable scope for one semester\nA workflow plan that makes your project reproducible from day one\n\n\n\n\n\n\n\n\n\n\n\nNoteData Readiness Note (Week 4)\n\n\n\n\n\nPurpose:\nDemonstrate that your dataset is analysis-ready—or identify exactly what still blocks you. This is your chance to be honest about limitations before modeling.\n\nWhat to include\n\nDataset summary\n\nNumber of rows/observations\n\nKey grouping structure (site, individual, date, etc.)\n\nData dictionary (lightweight)\n\nVariable name → meaning → units → type (numeric, factor, date)\n\nMissingness and exclusions\n\nWhat’s missing, where, and what you did about it (if anything)\n\nMeasurement and uncertainty notes\n\nKnown measurement error, detection limits, observer effects, instrument drift, etc.\n\nBasic EDA outputs (minimal but informative)\n\n1–3 plots that reveal structure (distributions, relationships, time trends)\n\nA short paragraph interpreting what matters\n\nReadiness decision\n\n“Ready to model” or “Not ready yet,” with a clear plan to get there\n\n\n\n\nSubmission format\n\n1–2 pages (PDF) or Quarto page with embedded plots\n\n\n\n“Good” looks like\n\nYou can explain what each variable means and whether it’s trustworthy\nYou’ve surfaced the biggest problems early\nYour EDA is purposeful, not a plot dump\n\n\n\n\n\n\n\n\n\n\n\nNoteWorking Model — Draft (Week 7)\n\n\n\n\n\nPurpose:\nPut a real model on the table. This is a draft: imperfect is fine, but it must be runnable and defensible.\n\nWhat to include\n\nYour current model formula(s) (plain language and/or code notation)\nModel type (GLM, GLMM, GAM, etc.) and why it fits the data structure\nKey diagnostics or checks (draft level)\n\nAnything that suggests it’s “in the ballpark” or clearly broken\n\nOne short interpretation paragraph\n\nWhat the model suggests (careful language; no over-claiming)\n\nA short “known issues” list\n\nWhat you know is wrong or incomplete\n\nWhat you plan to try next\n\n\n\n\nSubmission format\n\nQuarto page (preferred) or PDF with model description and outputs\nInclude enough detail that someone else could reproduce the model from your write-up\n\n\n\n“Good” looks like\n\nA runnable model aligned with your question\nHonest diagnostics and a clear plan to improve\n\n\n\n\n\n\n\n\n\n\n\nNoteWorking Model — Final Lock (Week 8)\n\n\n\n\n\nPurpose:\nFreeze the core model so the rest of the semester can focus on prediction, interpretation, and reporting instead of endless tinkering.\n\nWhat to include\n\nFinal model specification\n\nFinal formula, family/link, random effects or smooths, correlation structures if used\n\nJustification\n\nWhy this structure is appropriate for the data and the question\n\nModel checks\n\nA small set of diagnostics you can defend\n\nNotes on remaining limitations\n\nWhat changes are no longer allowed\n\nAfter the lock, presentation and interpretation may improve, but model shopping should stop without compelling justification\n\n\n\n\nSubmission format\n\nQuarto page (preferred) or PDF\nInclude key outputs you’ll rely on later (even if figures improve)\n\n\n\n“Good” looks like\n\nA stable model you can explain to a skeptical reader\nEvidence you tested assumptions rather than hoping\n\n\n\n\n\n\n\n\n\n\n\nNoteInterpretation Memo (Week 12)\n\n\n\n\n\nPurpose:\nPractice disciplined interpretation and uncertainty-aware reasoning. This is where you show that you can tell the truth about what the model does—and does not—say.\n\nWhat to include\n\nMain result(s) in plain language\nUncertainty\n\nWhat is uncertain and why\n\nConfidence intervals, credible intervals, prediction intervals, or other appropriate summaries\n\nPrediction vs. inference clarity\n\nAre you predicting new cases, explaining mechanisms, or both?\n\nSensitivity or robustness (lightweight)\n\nOne small check that increases confidence (or reveals fragility)\n\nLimitations\n\nThe most important threats to inference and how they affect interpretation\n\n\n\n\nSubmission format\n\n~1–2 pages (PDF) or Quarto page\nEmphasis on writing quality and intellectual restraint\n\n\n\n“Good” looks like\n\nClear, honest claims tied directly to model output\nUncertainty is centered, not hidden\nNo story time beyond what the evidence supports\n\n\n\n\n\n\n\n\n\n\n\nNoteResults Section (Week 14)\n\n\n\n\n\nPurpose:\nDraft the Results section you would submit in a real paper or report: concise, structured, and supported by tables and figures.\n\nWhat to include\n\nShort orienting paragraph\n\nWhat you modeled and what the reader should look for\n\nCore results\n\n2–4 key findings, depending on project scope\n\nTables and/or figures\n\nClean, interpretable, labeled, and referenced in text\n\nMinimal interpretation\n\nState what you found; deeper interpretation comes later or is clearly separated\n\n\n\n\nSubmission format\n\nQuarto page (ideal) or PDF\nInclude figure and table captions\n\n\n\n“Good” looks like\n\nFindings are understandable without reading your code\nFigures and tables do real work\nClaims match the evidence shown\n\n\n\n\n\n\n\n\n\n\n\nNoteFull Draft (Week 15)\n\n\n\n\n\nPurpose:\nAssemble a complete, coherent draft that reads like a real scientific product. Structure and flow matter here.\n\nWhat to include\n\nAt minimum: Introduction, Methods, Results, Discussion (or equivalent)\nConsistent terminology\n\nVariable names, units, and definitions do not drift\n\nAll figures and tables in near-final form\nReferences and citations, as appropriate\n\n\n\nSubmission format\n\nQuarto render (HTML or PDF preferred)\nEnsure figures and tables are legible and captioned\n\n\n\n“Good” looks like\n\nA document that could be revised into something publishable\nClear logic and honest uncertainty\nNo missing major components\n\n\n\n\n\n\n\n\n\n\n\nNoteRevision Plan (Not Executed) (Week 16)\n\n\n\n\n\nPurpose:\nDemonstrate that you can revise strategically. You will not execute revisions; you will propose them.\n\nWhat to include\n\nTop five revision priorities\n\nRanked, with a sentence explaining why each matters\n\nWhat you would change (specific)\n\n“Rewrite paragraph X to clarify Y”\n\n“Replace Figure 2 with a plot that shows Z”\n\nWhat evidence you would need\n\nAdditional checks, plots, sensitivity analyses, etc.\n\nIf you had 10 more hours\n\nWhat would you do first?\n\nIf you had 40 more hours\n\nWhat would you do that you cannot do now?\n\n\n\n\nSubmission format\n\n1 page (PDF) or short Quarto page\n\n\n\n“Good” looks like\n\nSpecific, actionable revisions\nClearly prioritized and realistic\nDemonstrates mature judgment about improving inference and communication",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html#analysis-concept-note",
    "href": "chapters/assignment_descriptions.html#analysis-concept-note",
    "title": "Milestone Assignments",
    "section": "",
    "text": "What to include\n\nWorking title (can change later)\nYour focal question (1–3 sentences; plain language)\nStudy system and units of analysis (what is one “row,” conceptually?)\nResponse variable(s) and predictor(s) you expect to use (draft list is fine)\nHypothesized relationships\n\nShort bullets; include directionality when possible\n\nData source and file plan\n\nWhere the data live\n\nWhat you will name files\n\nWhat the folder structure will be\n\nPlanned model family (tentative)\n\ne.g., GLM / GLMM / GAM / GAMM, and why that’s your first guess\n\nPotential complications you already suspect\n\nMissing data, non-independence, zero inflation, seasonality, etc.\n\n\n\n\nSubmission format\n\n1–2 pages (PDF) or a short Quarto/Markdown page\nClear headings (don’t bury the key information)\n\n\n\n“Good” looks like\n\nA question that is answerable with the data you actually have\nReasonable scope for one semester\nA workflow plan that makes your project reproducible from day one",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html#data-readiness-note",
    "href": "chapters/assignment_descriptions.html#data-readiness-note",
    "title": "Milestone Assignments",
    "section": "Data Readiness Note",
    "text": "Data Readiness Note\nWhen it appears in the calendar: Week 4\nPurpose:\nDemonstrate that your dataset is analysis-ready—or identify exactly what still blocks you. This is your chance to be honest about limitations before modeling.\n\nWhat to include\n\nDataset summary\n\nNumber of rows/observations\n\nKey grouping structure (site, individual, date, etc.)\n\nData dictionary (lightweight)\n\nVariable name → meaning → units → type (numeric, factor, date)\n\nMissingness and exclusions\n\nWhat’s missing, where, and what you did about it (if anything)\n\nMeasurement and uncertainty notes\n\nKnown measurement error, detection limits, observer effects, instrument drift, etc.\n\nBasic EDA outputs (minimal but informative)\n\n1–3 plots that reveal structure (distributions, relationships, time trends)\n\nA short paragraph interpreting what matters\n\nReadiness decision\n\n“Ready to model” or “Not ready yet,” with a clear plan to get there\n\n\n\n\nSubmission format\n\n1–2 pages (PDF) or Quarto page with embedded plots\n\n\n\n“Good” looks like\n\nYou can explain what each variable means and whether it’s trustworthy\nYou’ve surfaced the biggest problems early\nYour EDA is purposeful, not a plot dump",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html#working-model-draft",
    "href": "chapters/assignment_descriptions.html#working-model-draft",
    "title": "Milestone Assignments",
    "section": "Working Model (Draft)",
    "text": "Working Model (Draft)\nWhen it appears in the calendar: Week 7\nPurpose:\nPut a real model on the table. This is a draft: imperfect is fine, but it must be runnable and defensible.\n\nWhat to include\n\nYour current model formula(s) (plain language and/or code notation)\nModel type (GLM, GLMM, GAM, etc.) and why it fits the data structure\nKey diagnostics or checks (draft level)\n\nAnything that suggests it’s “in the ballpark” or clearly broken\n\nOne short interpretation paragraph\n\nWhat the model suggests (careful language; no over-claiming)\n\nA short “known issues” list\n\nWhat you know is wrong or incomplete\n\nWhat you plan to try next\n\n\n\n\nSubmission format\n\nQuarto page (preferred) or PDF with model description and outputs\nInclude enough detail that someone else could reproduce the model from your write-up\n\n\n\n“Good” looks like\n\nA runnable model aligned with your question\nHonest diagnostics and a clear plan to improve",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html#working-model-final-lock",
    "href": "chapters/assignment_descriptions.html#working-model-final-lock",
    "title": "Milestone Assignments",
    "section": "Working Model (Final Lock)",
    "text": "Working Model (Final Lock)\nWhen it appears in the calendar: Week 8\nPurpose:\nFreeze the core model so the rest of the semester can focus on prediction, interpretation, and reporting instead of endless tinkering.\n\nWhat to include\n\nFinal model specification\n\nFinal formula, family/link, random effects or smooths, correlation structures if used\n\nJustification\n\nWhy this structure is appropriate for the data and the question\n\nModel checks\n\nA small set of diagnostics you can defend\n\nNotes on remaining limitations\n\nWhat changes are no longer allowed\n\nAfter the lock, presentation and interpretation may improve, but model shopping should stop without compelling justification\n\n\n\n\nSubmission format\n\nQuarto page (preferred) or PDF\nInclude key outputs you’ll rely on later (even if figures improve)\n\n\n\n“Good” looks like\n\nA stable model you can explain to a skeptical reader\nEvidence you tested assumptions rather than hoping",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html#interpretation-memo",
    "href": "chapters/assignment_descriptions.html#interpretation-memo",
    "title": "Milestone Assignments",
    "section": "Interpretation Memo",
    "text": "Interpretation Memo\nWhen it appears in the calendar: Week 12\nPurpose:\nPractice disciplined interpretation and uncertainty-aware reasoning. This is where you show that you can tell the truth about what the model does—and does not—say.\n\nWhat to include\n\nMain result(s) in plain language\nUncertainty\n\nWhat is uncertain and why\n\nConfidence intervals, credible intervals, prediction intervals, or other appropriate summaries\n\nPrediction vs. inference clarity\n\nAre you predicting new cases, explaining mechanisms, or both?\n\nSensitivity or robustness (lightweight)\n\nOne small check that increases confidence (or reveals fragility)\n\nLimitations\n\nThe most important threats to inference and how they affect interpretation\n\n\n\n\nSubmission format\n\n~1–2 pages (PDF) or Quarto page\nEmphasis on writing quality and intellectual restraint\n\n\n\n“Good” looks like\n\nClear, honest claims tied directly to model output\nUncertainty is centered, not hidden\nNo story time beyond what the evidence supports",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html#results-section",
    "href": "chapters/assignment_descriptions.html#results-section",
    "title": "Milestone Assignments",
    "section": "Results Section",
    "text": "Results Section\nWhen it appears in the calendar: Week 14\nPurpose:\nDraft the Results section you would submit in a real paper or report: concise, structured, and supported by tables and figures.\n\nWhat to include\n\nShort orienting paragraph\n\nWhat you modeled and what the reader should look for\n\nCore results\n\n2–4 key findings, depending on project scope\n\nTables and/or figures\n\nClean, interpretable, labeled, and referenced in text\n\nMinimal interpretation\n\nState what you found; deeper interpretation comes later or is clearly separated\n\n\n\n\nSubmission format\n\nQuarto page (ideal) or PDF\nInclude figure and table captions\n\n\n\n“Good” looks like\n\nFindings are understandable without reading your code\nFigures and tables do real work\nClaims match the evidence shown",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html#full-draft",
    "href": "chapters/assignment_descriptions.html#full-draft",
    "title": "Milestone Assignments",
    "section": "Full Draft",
    "text": "Full Draft\nWhen it appears in the calendar: Week 15\nPurpose:\nAssemble a complete, coherent draft that reads like a real scientific product. Structure and flow matter here.\n\nWhat to include\n\nAt minimum: Introduction, Methods, Results, Discussion (or equivalent)\nConsistent terminology\n\nVariable names, units, and definitions do not drift\n\nAll figures and tables in near-final form\nReferences and citations, as appropriate\n\n\n\nSubmission format\n\nQuarto render (HTML or PDF preferred)\nEnsure figures and tables are legible and captioned\n\n\n\n“Good” looks like\n\nA document that could be revised into something publishable\nClear logic and honest uncertainty\nNo missing major components",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_descriptions.html#revision-plan-not-executed",
    "href": "chapters/assignment_descriptions.html#revision-plan-not-executed",
    "title": "Milestone Assignments",
    "section": "Revision Plan (Not Executed)",
    "text": "Revision Plan (Not Executed)\nWhen it appears in the calendar: Week 16\nPurpose:\nDemonstrate that you can revise strategically. You will not execute revisions; you will propose them.\n\nWhat to include\n\nTop five revision priorities\n\nRanked, with a sentence explaining why each matters\n\nWhat you would change (specific)\n\n“Rewrite paragraph X to clarify Y”\n\n“Replace Figure 2 with a plot that shows Z”\n\nWhat evidence you would need\n\nAdditional checks, plots, sensitivity analyses, etc.\n\nIf you had 10 more hours\n\nWhat would you do first?\n\nIf you had 40 more hours\n\nWhat would you do that you cannot do now?\n\n\n\n\nSubmission format\n\n1 page (PDF) or short Quarto page\n\n\n\n“Good” looks like\n\nSpecific, actionable revisions\nClearly prioritized and realistic\nDemonstrates mature judgment about improving inference and communication",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Milestone Assignments</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html",
    "href": "chapters/assignment_grades.html",
    "title": "Assignments & Grades",
    "section": "",
    "text": "Milestone Assignment descriptions\nThis page first describes the Milestone Assignments referenced in the course calendar. Each section below explains the purpose of the milestone, what to submit, and what “good” typically looks like. Grading rubrics are described at the end of this page.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html#milestone-assignment-descriptions",
    "href": "chapters/assignment_grades.html#milestone-assignment-descriptions",
    "title": "Assignments & Grades",
    "section": "",
    "text": "NoteAnalysis Concept Note (Week 2)\n\n\n\n\n\nPurpose:\nLock in a feasible analysis plan early, so you’re not improvising later. This is about scope, clarity, and workflow—not results.\n\nWhat to include\n\nWorking title (can change later)\nYour focal question (1–3 sentences; plain language)\nStudy system and units of analysis (what is one “row,” conceptually?)\nResponse variable(s) and predictor(s) you expect to use (draft list is fine)\nHypothesized relationships\n\nShort bullets; include directionality when possible\n\nData source and file plan\n\nWhere the data live\n\nWhat you will name files\n\nWhat the folder structure will be\n\nPlanned model family (tentative)\n\ne.g., GLM / GLMM / GAM / GAMM, and why that’s your first guess\n\nPotential complications you already suspect\n\nMissing data, non-independence, zero inflation, seasonality, etc.\n\n\n\n\nSubmission format\n\n1–2 pages (PDF) or a short Quarto/Markdown page\nClear headings (don’t bury the key information)\n\n\n\n“Good” looks like\n\nA question that is answerable with the data you actually have\nReasonable scope for one semester\nA workflow plan that makes your project reproducible from day one\n\n\n\n\n\n\n\n\n\n\n\nNoteData Readiness Note (Week 4)\n\n\n\n\n\nPurpose:\nDemonstrate that your dataset is analysis-ready—or identify exactly what still blocks you. This is your chance to be honest about limitations before modeling.\n\nWhat to include\n\nDataset summary\n\nNumber of rows/observations\n\nKey grouping structure (site, individual, date, etc.)\n\nData dictionary (lightweight)\n\nVariable name → meaning → units → type (numeric, factor, date)\n\nMissingness and exclusions\n\nWhat’s missing, where, and what you did about it (if anything)\n\nMeasurement and uncertainty notes\n\nKnown measurement error, detection limits, observer effects, instrument drift, etc.\n\nBasic EDA outputs (minimal but informative)\n\n1–3 plots that reveal structure (distributions, relationships, time trends)\n\nA short paragraph interpreting what matters\n\nReadiness decision\n\n“Ready to model” or “Not ready yet,” with a clear plan to get there\n\n\n\n\nSubmission format\n\n1–2 pages (PDF) or Quarto page with embedded plots\n\n\n\n“Good” looks like\n\nYou can explain what each variable means and whether it’s trustworthy\nYou’ve surfaced the biggest problems early\nYour EDA is purposeful, not a plot dump\n\n\n\n\n\n\n\n\n\n\n\nNoteWorking Model — Draft (Week 7)\n\n\n\n\n\nPurpose:\nPut a real model on the table. This is a draft: imperfect is fine, but it must be runnable and defensible.\n\nWhat to include\n\nYour current model formula(s) (plain language and/or code notation)\nModel type (GLM, GLMM, GAM, etc.) and why it fits the data structure\nKey diagnostics or checks (draft level)\n\nAnything that suggests it’s “in the ballpark” or clearly broken\n\nOne short interpretation paragraph\n\nWhat the model suggests (careful language; no over-claiming)\n\nA short “known issues” list\n\nWhat you know is wrong or incomplete\n\nWhat you plan to try next\n\n\n\n\nSubmission format\n\nQuarto page (preferred) or PDF with model description and outputs\nInclude enough detail that someone else could reproduce the model from your write-up\n\n\n\n“Good” looks like\n\nA runnable model aligned with your question\nHonest diagnostics and a clear plan to improve\n\n\n\n\n\n\n\n\n\n\n\nNoteWorking Model — Final Lock (Week 8)\n\n\n\n\n\nPurpose:\nFreeze the core model so the rest of the semester can focus on prediction, interpretation, and reporting instead of endless tinkering.\n\nWhat to include\n\nFinal model specification\n\nFinal formula, family/link, random effects or smooths, correlation structures if used\n\nJustification\n\nWhy this structure is appropriate for the data and the question\n\nModel checks\n\nA small set of diagnostics you can defend\n\nNotes on remaining limitations\n\nWhat changes are no longer allowed\n\nAfter the lock, presentation and interpretation may improve, but model shopping should stop without compelling justification\n\n\n\n\nSubmission format\n\nQuarto page (preferred) or PDF\nInclude key outputs you’ll rely on later (even if figures improve)\n\n\n\n“Good” looks like\n\nA stable model you can explain to a skeptical reader\nEvidence you tested assumptions rather than hoping\n\n\n\n\n\n\n\n\n\n\n\nNoteInterpretation Memo (Week 12)\n\n\n\n\n\nPurpose:\nPractice disciplined interpretation and uncertainty-aware reasoning. This is where you show that you can tell the truth about what the model does—and does not—say.\n\nWhat to include\n\nMain result(s) in plain language\nUncertainty\n\nWhat is uncertain and why\n\nConfidence intervals, credible intervals, prediction intervals, or other appropriate summaries\n\nPrediction vs. inference clarity\n\nAre you predicting new cases, explaining mechanisms, or both?\n\nSensitivity or robustness (lightweight)\n\nOne small check that increases confidence (or reveals fragility)\n\nLimitations\n\nThe most important threats to inference and how they affect interpretation\n\n\n\n\nSubmission format\n\n~1–2 pages (PDF) or Quarto page\nEmphasis on writing quality and intellectual restraint\n\n\n\n“Good” looks like\n\nClear, honest claims tied directly to model output\nUncertainty is centered, not hidden\nNo story time beyond what the evidence supports\n\n\n\n\n\n\n\n\n\n\n\nNoteResults Section (Week 14)\n\n\n\n\n\nPurpose:\nDraft the Results section you would submit in a real paper or report: concise, structured, and supported by tables and figures.\n\nWhat to include\n\nShort orienting paragraph\n\nWhat you modeled and what the reader should look for\n\nCore results\n\n2–4 key findings, depending on project scope\n\nTables and/or figures\n\nClean, interpretable, labeled, and referenced in text\n\nMinimal interpretation\n\nState what you found; deeper interpretation comes later or is clearly separated\n\n\n\n\nSubmission format\n\nQuarto page (ideal) or PDF\nInclude figure and table captions\n\n\n\n“Good” looks like\n\nFindings are understandable without reading your code\nFigures and tables do real work\nClaims match the evidence shown\n\n\n\n\n\n\n\n\n\n\n\nNoteFull Draft (Week 15)\n\n\n\n\n\nPurpose:\nAssemble a complete, coherent draft that reads like a real scientific product. Structure and flow matter here.\n\nWhat to include\n\nAt minimum: Introduction, Methods, Results, Discussion (or equivalent)\nConsistent terminology\n\nVariable names, units, and definitions do not drift\n\nAll figures and tables in near-final form\nReferences and citations, as appropriate\n\n\n\nSubmission format\n\nQuarto render (HTML or PDF preferred)\nEnsure figures and tables are legible and captioned\n\n\n\n“Good” looks like\n\nA document that could be revised into something publishable\nClear logic and honest uncertainty\nNo missing major components\n\n\n\n\n\n\n\n\n\n\n\nNoteRevision Plan (Not Executed) (Week 16)\n\n\n\n\n\nPurpose:\nDemonstrate that you can revise strategically. You will not execute revisions; you will propose them.\n\nWhat to include\n\nTop five revision priorities\n\nRanked, with a sentence explaining why each matters\n\nWhat you would change (specific)\n\n“Rewrite paragraph X to clarify Y”\n\n“Replace Figure 2 with a plot that shows Z”\n\nWhat evidence you would need\n\nAdditional checks, plots, sensitivity analyses, etc.\n\nIf you had 10 more hours\n\nWhat would you do first?\n\nIf you had 40 more hours\n\nWhat would you do that you cannot do now?\n\n\n\n\nSubmission format\n\n1 page (PDF) or short Quarto page\n\n\n\n“Good” looks like\n\nSpecific, actionable revisions\nClearly prioritized and realistic\nDemonstrates mature judgment about improving inference and communication",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html#grading",
    "href": "chapters/assignment_grades.html#grading",
    "title": "Assignments & Grades",
    "section": "Grading",
    "text": "Grading\n\n\nNumerical grades are downright loathsome. For all assignments, I use the Traffic Signal Grading Scale:\n\n\n\nGrade\nSimple meaning\nScientific judgement\n\n\n\n\nGreen\nGood to Proceed\nScientifically coherent for this milestone\n\n\nYellow\nExercise Caution\nViable, but revision is required\n\n\nRed\nStop and Rework\nNot yet ready for scientific judgment\n\n\n\nYellow is not bad. It just means there are certain elements that you should tighten up before moving on.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html#universal-milestone-grading-rubric",
    "href": "chapters/assignment_grades.html#universal-milestone-grading-rubric",
    "title": "Assignments & Grades",
    "section": "Universal Milestone Grading Rubric",
    "text": "Universal Milestone Grading Rubric\nThe Traffic Signal Grading Scale applies to all Milestone Assignments. Grading reflects scientific readiness relative to the purpose of the milestone.\n\n\n\n\n\n\n\n\n\nDimension\n🟢 GREEN: Good to Proceed\n🟡 YELLOW: Exercise Caution\n🔴 RED: Stop and Rework\n\n\n\n\nPurpose & intent\nClearly meets the core intent of the milestone\nPartially meets intent; key elements need revision\nDoes not meet the core intent of the milestone\n\n\nQuestion or objective\nClear, focused, and appropriate for this stage\nPresent but vague, drifting, or overly broad\nUnclear, unfocused, or missing\n\n\nAlignment (question–data–methods)\nQuestion, data, and approach are coherently aligned\nPartial alignment; gaps or weak links remain\nFundamental mismatch between components\n\n\nReasoning & assumptions\nReasoning is coherent; assumptions or limitations acknowledged\nReasoning incomplete or assumptions implicit\nReasoning unclear or unsupported\n\n\nClaims & language\nClaims are appropriate for the stage of analysis\nSome overreach or ambiguous language\nClaims exceed what the work can support\n\n\nCourse GPT self-check\nUsed appropriately; issues addressed or justified\nUsed, but issues only partially addressed\nLittle or no meaningful evidence of self-checking\n\n\nScientific readiness\nWork is ready to move to the next stage\nViable but requires targeted revision\nNot yet in a usable scientific state\n\n\n\nThis universal grading scale is meant to help you quickly understand where your work stands, not to highlight small mistakes. My goal here is to make expectations clear, keep grading transparent, and help you focus on doing solid, defensible science –all without unnecessary stress over points or formatting (which we will let GenAI work on for you). The scale’s primary benefits include:\n\nIt is easy to read. The traffic light color scheme (green, yellow, and red) tell you right away whether your work is ready to move forward, needs some fixing, or needs a rethink.\nIt is the same for every assignment. I use this scale for all milestone assignments. Therefore, it is predictable for every assignment.\nIt centers on scientific readiness, not scientific perfection (which, technically, does not exist). You are not being graded on having the “right” answer. What matters most to me is whether your question, data, and reasoning co-exist coherently together.\nIt matches how real science works. Research almost never gets it right on the first try; drafts, checks, and revisions are normal.\nIt allows for rapid feedback. GenAI will do some of the busy work associated with troubleshooting coding issues or improving clarity, which will give me more time to gauge the scientific readiness of your work.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  }
]