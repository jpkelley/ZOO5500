---
title: "Tutorial: Data Exploration"
date: ""
format:
  html: default
execute:
  echo: true
  warning: false
  message: false
  error: false
editor: visual
editor_options: 
  chunk_output_type: console
---

## Tutorial: Data Exploration

In the tutorial below, you will witness some common approaches to examine datasets for outliers and multicollinearity. Some of these have severe limitations, which will be explained. The primary takeaway from this initial approach is that there is not a single, optimal recipe for understanding all the issues with your data *prior* to analysis. The process of discovery is iterative across several parts of your workflow. So, just be patient with this first step.

::: {.callout-note collapse="false"}
# What to expect from future tutorials

In future weeks, you will be given a similar tutorial (with a very simple worked example) to work through on your own prior to class. In-class sessions will be used to answer questions pertaining to (1) the exercise itself and (2) the application of the tools and approaches to your own dataset.
:::

## Workspace set-up

You are free to organize your R Project folder in whatever way works best for you, as this course imposes very little file-structure overhead. One simple option is to create a subfolder such as `data/class_exercise_data` (using the `dir.create("data/class_exercise_data")` command to hold any example files used in class. If you need a refresher on project setup and organization, refer back to the [**Preparing Yourself**](https://jpkelley.github.io/ZOO5500/) section from Week 1.

First, load the required packages by sourcing the file below. There a number of ways to do this (I have noticed several ways that different labs at UW do this, and that is more than acceptable). For your convenience, I have provided the following file (click to save; you may need to add the `.r` extension manually):

-   [`load_packages.r`](../chapters/class_r_scripts/load_packages.r)

Place this file into a folder called `class_r_scripts` (just a recommendation!).

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(error = FALSE, warning = FALSE, message = FALSE)

# Core packages
library(tidyverse) # meta-package (includes ggplot, stringr, forcats, dplyr, etc.)
library(lubridate) #parsing, manipulating, and computing with date–time data
library(rio) # Data I/O
library(mgcv) # generalized additive modeling and smooth term estimation
library(MASS) # base-recommended, but not tidyverse
library(Matrix) # some functions related to core statistical infrastructure
library(fields) # spatial statistics and geostatistical utilities
library(psych) # psychometric and multivariate descriptive analysis
library(MuMIn) # multi-model inference and model comparison
library(insight) # provides model introspection
library(ggeffects) # produces tidy outputs compatible with ggplot2
library(GGally)  # extends ggplot2 but is not tidyverse

```

```{r}
#| eval: false

source("class_r_scripts/load_packages.r")
```

You should include this sourcing step at the beginning of all independent R scripts. The exception is if and when you use a formal reproducible analysis workflow packages (e.g. the `targets` package) or you need Quarto, which requires that you load packages and their dependencies in another way. This will require removing the `source` function below.

::: callout-note
## **The `tidyverse` meta-package in R**

Keep in my that the `tidyverse` package in R is a *meta* package and contains many standalone R packages, including:

-   `ggplot2`: declarative construction of statistical graphics
-   `dplyr`: relational and vectorized data manipulation operations
-   `tidyr`: restructuring datasets into tidy (rectangular) form
-   `readr`: standardized, high-performance import of tabular data
-   `purrr`: functional programming utilities for iterative workflows
-   `tibble`: enhanced data-frame representation with explicit printing semantics
-   `stringr`: consistent, vectorized string processing
-   `forcats`: handling and releveling of categorical variables
:::

------------------------------------------------------------------------

## Our dataset: Impacts of mud-dwelling jackalopes on dissolved oxygen

::::: columns
::: {.column width="70%"}
These data for this exercise come from a totally absolutely real experiment on a newly discovered species of jackalope, *Lepusantilocapra pelonates* (“mud-dwelling jackalope”). Researchers observed that mudflats with higher densities of *L. pelonate's* tended to have higher dissolved oxygen. At the same time, nearby forest salt licks provided salt that varied naturally across time and space, and *L. pelonates* were frequently observed foraging at them.

To test whether jackalope density and salt availability had causal effects on dissolved oxygen, the researchers constructed artificial mudflats and experimentally varied the number of *L. pelonates* (0–4 individuals) and salt availability (low, medium, high). Each unique combination was represented by a single mudflat, and dissolved oxygen was measured at each site. These are the data from that totally real experiment.
:::

::: {.column width="30%"}
![](../graphics/jackalope_graphics/muddwelling_jackalope.png)
:::
:::::

To do these exercises on your own, you will need the following files (simply click to save):

-   [`lepusantilocapra_pelonates_experiment.csv`](../chapters/class_exercise_data/lepusantilocapra_pelonates_experiment.csv)
-   [`lepusantilocapra_climate_data.csv`](../chapters/class_exercise_data/lepusantilocapra_climate_data.csv)

Place the data files into your `class_exercise_data` folder.

Let us read in these data and save it to an object called `d` (for “data”). Yes, according to best practices for naming, this could have a more informative name. Because this is (a) a singular dataset for a simple analysis, and (b) a simple class example, we will use a shorthand to prevent some extra keystrokes. But just know this is technically *very bad practice*. Also, for the sake of streamlining this example, this file has no associated metadata (this will be shown later when we get to the modeling phase). Already, we are flawed programmers. Let us move forward and try to forget these incidents.

```{r}
d <- read.csv("class_exercise_data/lepusantilocapra_pelonates_experiment.csv")
```

This was a successful (and very simple) data ingestion. (Again, there are many ways to ingest data, so use whatever you are most comfortable with.)

---

## Applying the Data Exploration Toolkit

As you recall, we have the following steps of the *ante hoc* Data Exploration Toolkit:

-   Tool #1: Assess *potential* outliers (univariate and multivariate)
-   Tool #2: Assess the presence of excess zeros
-   Tool #3: Explore interactions and structural relationships
-   Tool #4: Assess potential information redundancy among covariates (e.g., multicollinearity)
-   Tool #5: Decide whether covariate standardization is warranted

The good news is that we can generally combine the first three steps by simply looking at the overall structure of your dataset in several ways. These "ways" are non-prescriptive; you should continue to explore until you understand all the dimensions of your raw data. Let's begin!

## Basic examination of broad-scale data structure

First, we need to check the data types that were imported; this is a critical first step, as this can let you immediately detect any red flags. Though the `str()` (or the `glimpse()` function) is good for initial checking:

```{r}
str(d)
```

We note that `number_jackalope` is an integer. We could certainly keep it like this, but, given how few categories there are (and the fact that this was an experimental condition), we decide that it is best to convert it to a factor.

```{r}
d <- d |>
  mutate(number_jackalope = factor(number_jackalope )) |>
  mutate(salt_treatment = factor(salt_treatment))
```

::: {.callout-warning collapse="true"}
## Digression about `base` versus `magrittr`/`tidyverse` pipes

Note that the native pipe operator `|>` was used in the above `tidy` code snippit.

| Feature | `|>` (`base` R) | `%>%` (`magrittr` / `tidyverse`) |
|------------------|-------------------|-----------------------------------|
| Introduced | R 4.1.0 (`base` R) | Earlier (via `**`magrittr`**`, widely adopted by tidyverse) |
| Requires package | No | Yes (`library(magrittr)` or `library(tidyverse)`) |
| Default recommendation | Preferred going forward | Legacy / tidyverse-centric |
| Placeholder support | No implicit placeholder | Supports `.` placeholder |
| Example placeholder use | Not possible | `df %>% mutate(x = . + 1)` |
| Anonymous functions | Uses `\(x)` syntax | Uses `.` or `{}` blocks |
| Example | `x |> mean()` | `x %>% mean()` |
| Readability (simple pipelines) | Very clean | Very clean |
| Readability (complex pipelines) | Can become verbose | Often clearer due to `.` |
| Compatibility with older R | Requires R ≥ 4.1 | Works in older R |
| Debugging / tooling | Excellent (native) | Excellent |
| Performance | Slightly faster (base) | Slight overhead (negligible) |
| Teaching clarity | Emphasizes base R | ️ Adds extra abstraction |
| Future-proofing | Yes | ️ Likely to persist but not expand |
| Works with tidyverse verbs | Yes | Yes |
:::

You can then run `str(d)` or `glimpse()` again to see this change.

## Data structure: visualizing variation within and among groups

Let us now examine how the treatments and the response variable are structured and if there are any statistical outliers. There are many approaches to this, and most of these formal outlier tests (Grubb's test, Dixon's Q test, and Rosner's test) are based on the condition of dat normality. As such, they have context-dependent performance; that is, they fail at low sample sizes and have variable ability to detect outliers dependent on sample size. If you want to use these tests despite these weaknesses, you can find them in the `EnvStats` and `outliers` packages.

It is important at this step to avoid making any inferences about the relationships. As far as you, the responsible data analyst, are concerned, there are no relationships. You are simply looking for anything odd in the dataset: spurious datapoints, data outside the range of data (though you could use the `range()` function to see this), etc. Let us conduct a simple examination using one tabular and six graphical approaches:

-   Step 1: Simple `summary`
-   Step 2: Boxplot (Tukey-style)
-   Step 3: Violin plot
-   Step 4: Scatterplot
-   Step 5: Density plot
-   Step 6: Heatmap
-   Step 7: Interaction plot

### Simple `summary` and examination of data structure

```{r}
summary(d)
```

This returns information about range, median value, and sample size within groups. **Use this first to discover any major issues.**

You can then graphically view aspects of your raw dataset. So, let us create a few plots using the ggplot2 package. Please refer to the `ggplot2` [**Cheat Sheet**](../chapters/class_exercise_data/data_visualization.pdf) (right click for download) for more information about this.  

::: {.callout-tip collapse="true"}
## **A recipe for a `ggplot2` visualization**

Use the `ggplot2` [**Cheat Sheet**](../chapters/class_exercise_data/data_visualization.pdf) (right click for download) and find the appropriately useful arguments to replace the angle bracketed (`< >`) sections (that is, remove the angle brackets in your final code).

```{r}
#| eval: false

data |> 
  ggplot(
    aes(x = <x_variable>, y = <y_variable>, <optional_aesthetic> = <variable>)
  ) +
  geom_<geometry>(<geometry_options>) + 
  labs(
    title = "<plot title>",
    x = "<x-axis label>",
    y = "<y-axis label>",
    <other_labels>
    ) +
  theme_<theme_name>() + 
  <optional_additional_layers>

```
This follows the [Grammar of Graphics (GoG)](https://en.wikipedia.org/wiki/Wilkinson's_Grammar_of_Graphics) approach introduced by Leland Wilkinson in 1999. All the verbiage (e.g. _facets_, _aesthetics_, _geometry_) was then used as the basis for the first `ggplot` version by Hadley Wickham in 2007 (first book published in 2009; [now available online](https://ggplot2-book.org/)). Wilkinson's book on graphical theory is a great read and a great example of how to develop a set or irreducible guidelines for a seemingly complex system. 
:::

### Boxplot (Tukey-style)

For Step #1 of our Data Exploration Toolkit, we can start the process of identifying potential univariate outliers by using a Tukey-style boxplot for visual inspection. The following code takes our dataset (d) and then sends it forward to the `ggplot` function. As you read on the Cheat Sheet (linked above) and the brief recipe in the callout box above, you need to set what is called an “aesthetic” (abbreviated to “aes” in the function) that contains what you want to be your `x` and `y` variables. You can also set the fill color to vary by `salt_treatment`s too. Then, the next lines add some axis and graph labels and set a theme (i.e. changes colors of the whole plotting area).

```{r}
d |> 
  ggplot(aes(x = salt_treatment, y = oxygen_ppm, fill = salt_treatment)) +
  geom_boxplot() +
  labs(title = "Boxplot by salt treatment", x = "salt_treatment", y = "oxygen_ppm") +
  theme_minimal()
```

### Cleveland Dot Plot

This is another simple way of visualizing potential outlier. Fancy name; unfancy visualization. 

```{r}
#| message: false
#| warning: false

d |>
  ggplot(aes(x = oxygen_ppm, y = salt_treatment)) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Cleveland dot plot by salt treatment",
    x = "oxygen_ppm",
    y = "salt_treatment"
  ) +
  theme_minimal()
```

Now, what can be said about potential outliers? In our boxplot, we see two black dots each group that seem to be outside of the normal range of those groups. These are _potential_ outliers. You should go back and examine these datapoints to see if there is anything strange about them. But be aware of how Tukey-style boxplots calculate outliers; it can be misleading if you do not understand Tukey's original approach. Let us also examine the boxplots by different *L. pelonates* densities (i.e. the jackalope density treatments):

```{r}
d |> 
  ggplot(aes(x = number_jackalope, y = oxygen_ppm, fill = number_jackalope)) +
  geom_boxplot() +
  labs(title = "Boxplot by number of jackalope", x = "number_jackalope", y = "oxygen_ppm") +
  theme_minimal()
```

The Tukey boxplot is not showing any black “outlier” dots in the groups. So, using this approach, there do not seem to be any structural outliers. You can **repeat this process** on every variable in your dataset. It is generally effective for flagging major issues (anomalous values, etc.). Most likely, however, this approach will flag values that end up being perfectly reasonable. That is more than acceptable. At the very worst, you have put your eyes on the dataset a few more times.


### Violin plot

A violin plot visualizes distributions using a mirrored density plot. As such, the default is to extend the density profile past the data boundaries. This feature can be suppressed by setting the `trim` argument in the `geom_violin` to `FALSE`.

```{r}
ggplot(d, aes(x = number_jackalope, y = oxygen_ppm, fill = salt_treatment)) +
  geom_violin(trim = FALSE) +
  labs(title = "distribution of oxygen ppm by # jackalope and salt treatment",
       x = "number of jackalopes",
       y = "oxygen ppm") +
  theme_minimal()
```

Because violin plots do not explicitly show outliers, they may be worse in some ways than Tukey-style boxplots for visual data exploration. Furthermore, violin plots may be harder to interpret than boxplot, may be space-inefficient (in terms of Edward Tufte's [well-known argument for reduced non-data ink](https://jtr13.github.io/cc19/tuftes-principles-of-data-ink.html) in visualizations), and may smooth over important details of the data (i.e. it's difficult to visually assess the location of the 25th and 75th percentiles, if that is something you are accustomed to). Violin plots also can be misleading, especially with small sample sizes. Use them, but think critically about what information they are providing.

### Scatter plot

Do not overlook the utility of simple scatterplot. A scatterplot is useful for seeing the raw relationship between a predictor and a response across treatments. It helps you assess overall trends, clustering, and potential outliers while keeping individual observations visible.

```{r}
ggplot(d, aes(x = number_jackalope, y = oxygen_ppm, color = salt_treatment, shape = salt_treatment)) +
  geom_jitter(width = 0.1, height = 0) +
  labs(title = "oxygen ppm scatter plot by # jackalope and salt treatment",
       x = "# jackalope",
       y = "oxygen ppm") +
  theme_minimal()
```

### Density plot

A density plot is very useful for quickly comparing the overall distribution of a response variable within or between groups. It helps you see differences in spread, skewness, and overlap between treatments that may not be obvious from summary statistics alone.

```{r}
ggplot(d, aes(x = oxygen_ppm, fill = salt_treatment)) +
  geom_density(alpha = 0.5) +
  labs(title = "density of oxygen ppm by salt treatment",
       x = "oxygen ppm",
       y = "density") +
  theme_minimal()
```
For our example, this approach gives us a good idea of the relative spread of each of salt_treatment response. We can also produce the same plot, but we could split it by jackalope treatment instead. (In this case, it's a bit messy.)

```{r}
ggplot(d, aes(x = oxygen_ppm, fill = number_jackalope)) +
  geom_density(alpha = 0.5) +
  labs(title = "density of oxygen ppm by # jackalope",
       x = "oxygen ppm",
       y = "density") +
  theme_minimal()
```

### Heatmap

```{r}
d |>
  group_by(number_jackalope, salt_treatment) |>
  summarise(mean_oxygen = mean(oxygen_ppm), .groups = "drop") |>
  ggplot(aes(x = number_jackalope, y = salt_treatment, fill = mean_oxygen)) +
  geom_tile() +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "heatmap: mean oxygen ppm by # jackalope and salt treatment",
       x = "number of jackalope",
       y = "salt treatment",
       fill = "mean oxygen ppm") +
  theme_minimal()
```

### Interaction plot (Step #3)

Step #3 of our Data Exploration Toolkit is to examine our interaction structure. A simple interaction plot is useful for getting a quick, visual sense of whether the relationship between a predictor and a response might change across levels of another factor. By faceting the same boxplot across salt treatments (i.e. creating "small multiples" _sensu_ Edward Tufte), you can more easily compare both the typical oxygen levels and the spread of the data for each jackalope count within each treatment. This does _not_ test for an interaction, but it is a fairly good reality check for overall patterns --such as shifts in medians or changes in variance-- that might justify including an interaction term later on. 

```{r}
ggplot(d, aes(x = number_jackalope, y = oxygen_ppm)) +
  geom_boxplot() +
  facet_wrap(~salt_treatment) +
  labs(title = "Oxygen ppm by number of jackalopes across salt treatments",
       x = "# jackalope",
       y = "oxygen ppm") +
  theme_minimal()
```

This is not particularly useful for detecting data anomalies, but it could be useful for helping you understand where data may be sparse (i.e. low sample sizes) or where there may be missing combinations of treatments (which can lead to lack of model convergence).

---
## Inspecting for multicollinearity (Step #4)

The jackalopeologists then conducted a preliminary quantification of the climate in which *L. pelonates* was found. They used remote sensors to measure humidity, precipitation, temperature, temperature variability, and heating degree days. We don’t think we can include all the variables in our statistical model, so we want to first inspect if any of the variables are correlated (i.e. they tell us redundant information). Let’s read in these climate data and save it to an object called “climate”:

```{r}
climate <- import("class_exercise_data/lepusantilocapra_climate_data.csv")
```

And then look at the structure of the data:

```{r}
str(climate)
```

Normally, we would have the associated metadata so that you could see the units for each of these variables. For this example, the units are not important.

We will proceed to visually examine multicollinearity (correlation) between these climate metrics, but we will also include–as a basic measure of strength of any correlation– the Pearson correlation metric. Let’s use a pairwise plot to visually inspect these. Note that the correlation coefficient (which varies between 0-1) is on the opposite diagonal from the graph.

```{r}
climate |>
  ggpairs(
    upper = list(continuous = "cor"),
    lower = list(continuous = "smooth"),
    diag  = list(continuous = "barDiag")
  )
# "upper" = Show Pearson correlation coefficients in the upper panel
# "lower" = Optionally show scatterplots with smoothing in the lower panel
# "diag"  = Optionally show histograms on the diagonal
```

The `GGally::ggpairs` function markedly improves upon similar functionality of base R. What do we see? We see that:

-   humidity and precipitation are strongly and positively correlated (r = 0.648)
-   temperature and temperature variation are strongly and positively correlated (r = 0.584)
-   temperature and heating degree days are strongly and negatively correlated (r = -0.998)
-   temperature variation and heating degree days are strongly and negatively correlated (r = -0.588)

So, what to do, what to do, what to do? The next part is one solution to this issue. We clearly have some variables that might be providing redundant information. This could impact our modeling (specifically inflating the variance of a given regression coefficient). We can explicitly examine this using Variance Inflation Factors (VIFs), which are calculated for each term in a model. Note that there are options for VIFs applied to both continuous and categorical variables. Let us do this for continuous variables for these climate data. Most available functions in R require regression models; however, there are some approaches--such as the `usdm` package--that require only data frames of multiple variables. So, let us use that.:

```{r}
vif_results <- usdm::vif(climate)
vif_results
```

This is a clear example that two variables, temperature and heating_deg_days, are highly correlated with a number of other variables. That is, they potentially provide redundant information.

If variables are negatively correlated, consider retaining them both. From there, you can decide to:

-   Eliminate one of the positively correlated variables from our analysis.
-   Conduct a factor-reduction analysis to create a new variable that captures the correlated variation between correlated variables.

The former sounds lazy and arbitrary. Let us proceed with a factor reduction approach. For this, one of the most widely accepted approaches is a principal components analysis (PCA). There are many related factor reduction methods out there as well that may work better for non-normal data, etc., but PCA generally works fairly well.

Our first step is to conduct a PCA without specifying how many new components (new variables) should be generated.

```{r}
pca_results <- stats::princomp(
  ~ humidity + precipitation + temperature + temp_variation + heating_deg_days,
  data = climate,
  cor = TRUE
) # using the namespace operator
```

Note that the above code does not work with pipes. This behavior happens sometimes with `tidy`.

Let us now examine the results output, specifically the standard deviation values (i.e. “Comp.1”, etc.). If you square these values, you will have what is called an *eigenvalue*, where values \>1 indicate that the new principal component accounts for more variance than the original variables. So, your first step is square the standard deviations and then use this *eigenvalue* to decide how many new components (i.e. new variables) to retain.

```{r}
pca_results$sdev ^ 2  # this squares the standard deviations
```

It looks like the first two components work well. Now, we want to force a PCA to contain just two components, and we want those two variables to be as independent as follows. We therefore conduct what is called a rotated PCA to achieve maximum information of each component. Note that we are now using the `principal` function found in the `psych` package:

```{r}
pca_results_rotated <- psych::principal(climate, nfactors = 2, rotate = "varimax", covar = FALSE) # using a namespace operator
pca_results_rotated$loadings
```

Let’s dissect this. First, look at the loadings table. These show the linear correlations between the rotated principal components (RC1 and RC2) and the original climate variables. Therefore, RC1 is strongly correlated with temperature, temperature variation, and heating degree days, whereas RC2 is correlated with humidity and precipitation.

You can see this generally reflected when you plot PCA axes of variation. A safe default is to biplot the `princomp()` object:

```{r}
biplot(pca_results)
```

One advantage of PCA/ICA or other factor reduction methods is that you can use expert knowledge to decide on the number of resulting axes. Certainly, in the end, these may not capture much total variation. This flexibility may improve interpretability.

We can then create two new rotated scores for each observation (row). But what are these new values? These are simply new continous values that capture the variation attributable to the set of variables that correlated with each rotated component (PC1, PC2, etc.).

The statistical convention in most fields is to accept all rotated components that sum to **\>80%** of cumulative explained variation. You can output these new rotated scores by running the following code and then inputting these as new columns in your dataset. Then, run the model with each of these new *proxy variables* as new factors in your model(s):

```{r}
pca_results_rotated$scores
```




**End of tutorial**