[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "",
    "text": "Essential Course Information",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Information</span>"
    ]
  },
  {
    "objectID": "index.html#essential-course-information",
    "href": "index.html#essential-course-information",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "",
    "text": "Instructor\nPatrick Kelley\n\n\n\n\nEmail\njkelle24@uwyo.edu\n\n\nSchedule\nTu / Th, 11:00–12:15 MT\n\n\nDates\n20-Jan-2026 to 08-May-2026\n\n\nMeeting Type\nsynchronous remote\n\n\nLocation\nJoin via Zoom\n\n\nCommunication\nSlack (private link sent to your email)",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Information</span>"
    ]
  },
  {
    "objectID": "index.html#quicklinks",
    "href": "index.html#quicklinks",
    "title": "Quantitative Analysis of (Messy) Field Data",
    "section": "Quicklinks",
    "text": "Quicklinks\nCourse Calendar\n\n\n\n\n\n\n\n\nNoteClass collaboration tools\n\n\n\n\n\nCollaborative Statistics References\n\n\n\n\n\n\n\n\n\n\n\n\nWarningWorkflow templates\n\n\n\n\n\nYAML Metadata Template (general)\n\n\n\n\n\n\n\n\n\n\n\n\nTipAssignment templates\n\n\n\n\n\nAnalysis Concept Note\n\n\n\n\n\n\nThis course was created using Quarto (via R Studio) using custom CSS and HTML.\n\n\n\n\n\n\nCautionAI usage transparency\n\n\n\n\n\nIn the construction of this website, GenAI (via ChatGPT) was used in the following ways:\n\nGenerating graphics of cryptid jackalope species. This was part of an exercise in prompt engineering—specifically, learning how to design minimalist prompts that produce consistent visual themes across all outputs.\nConverting the Canvas-based course site into Quarto markdown. This included restructuring content, standardizing headers, and translating existing materials into a reproducible, version-controlled format (for hosting on GitHub).\nAssistive learning of custom CSS, in parallel with consulting online CSS documentation and manuals (i.e., as a tutor and debugger, not a design authority).\nReformatting and updating existing tables, including updating of revised dates, assignments, and course logistics.\nError-checking and troubleshooting markdown and Quarto code. This was necessary during site builds.\nImproving organization across course components. This included suggestions for cross-referencing related pages and identifying places where additional examples or clarifications would improve learning outcomes.\nEvaluating pedagogical alignment. This involved getting feedback on readability, cognitive load, and whether learning objectives were being met by the material being presented.\nEditorial support to ensure that tone and level of explanatory detail was appropriate for graduate students. This included tightening some language and reducing informational redundancy across pages.\n\nGenAI was used as an assistive tool throughout; it was not used as a content authority, decision-maker, or substitute for subject-matter expertise. All instructional choices, interpretations, and final edits remain the responsibility of the instructor (in accordance with AI Use Policy).",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Information</span>"
    ]
  },
  {
    "objectID": "chapters/course_intro.html",
    "href": "chapters/course_intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Welcome to Quantitative Analysis of (Messy) Field Data!\nThanks for joining us this term! I couldn’t be more pleased at the large size of the class this term! This means that there are more perspectives to help guide us through the following phases of data analysis:\nThe complexity of the project, difficulty of field conditions, or an impending deadline can make the jumps from data collection to inference horribly daunting to young and old researchers alike. Our joint goal in this course is to become more comfortable with each of these steps, so that we not only work more efficiently but we also know a bit more about how to sense of our complex world. It is my sincerest hope that this course will not only teach you some about statistics (and some statistical traps) but that it will also give you a unique opportunity to work with your peers as you delve into the myriad issues that inevitably arise in data analysis. Working together means that we must first acknowledge that each one of us assimilates new material and produces syntheses in varied ways. All I ask is that each of you",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/course_intro.html#welcome-to-quantitative-analysis-of-messy-field-data",
    "href": "chapters/course_intro.html#welcome-to-quantitative-analysis-of-messy-field-data",
    "title": "2  Introduction",
    "section": "",
    "text": "Messy field or lab data → processed data → models → inference",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/course_intro.html#course-objectives",
    "href": "chapters/course_intro.html#course-objectives",
    "title": "2  Introduction",
    "section": "2.2 Course Objectives",
    "text": "2.2 Course Objectives\nBy the end of this course, you should be able to:\n\nThink quantitatively about messy ecological data. You should be on your way to developing a habit of translating real-world ecological questions into quantitative frameworks, while explicitly acknowledging uncertainty and bias in estimation and measurement.\nDistinguish between questions/hypotheses, data, and inferences. You should be able to (1) clearly separate ecological hypotheses, the data actually collected, and the quantities being estimated, and (2) understand why these distinctions matter for interpretation and decision-making.\nUnderstand the processes that generate data. You should be able to recognize how different study designs, variation in detection, observer effects, instrument error, and data processing shape and constrain the structure of ecological datasets.\nSelect, build, and critique statistical models as scientific tools. You should be comfortable choosing and using statistical models not as black boxes, but as explicit representations of assumptions about ecological processes, variation, and causal structure. You should never blindly choose a statistical model again!\nInterpret results in ecological —not just statistical— terms. You should be able to conceptually move beyond p-values and coefficients to clearly articulate what your results mean biologically, mechanistically, and practically.\nBe comfortable applying causal –and not just correlational– reasoning. You should be able to use causal thinking (conceptual models and directed acyclic graphs) to evaluate what can —and ehat cannot— be inferred from observational and experimental data.\nCommunicate quantitative results clearly, transparently, and honestl.y Present analyses, figures, and conclusions in ways that are transparent, reproducible, and appropriate for scientific audiences.\nDevelop durable, reproducible analytical workflows. You should be increasingly comfortable with good data-science practices that support clarity, versioning (even though we will not delve into GitHub this term), and reusability of analyses.\nDevelop new confidence working with unfamiliar, complex, imperfect datasets. You should leave this course better prepared to engage with real ecological data —without expecting it to be clean, complete, or simple.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/course_intro.html#topics-covered",
    "href": "chapters/course_intro.html#topics-covered",
    "title": "2  Introduction",
    "section": "Topics covered",
    "text": "Topics covered\nThe course covers the following topics, roughly in the following order:\n\nMetrology: measurands, measurement error, uncertainty, and data quality\n\nExploratory data analysis of messy data\n\nGeneralized Linear Models (GLMs): theory and implementation\n\nModel comparison using AIC and information-theoretic approaches\n\nGeneralized Linear Mixed Models (GLMMs) and effective sample size\n\nGeneralized Additive Models (GAMs)\n\nGeneralized Additive Mixed Models (GAMMs)\n\nModeling spatial and temporal heterogeneity\n\nStructural Causal Modeling (SCM) and causal diagrams\n\nPrediction, uncertainty propagation, and model validation\n\nSynthesis of analytical results and justification of inference\n\nTables, figures, and reporting standards for quantitative results\n\nWriting defensible and interpretable Results sections\n\nGiven this ambitious scope, we may only scratch the surface of some topics. Even our cursory treatment is aimed at convincing you why these subjects matter for scientists who have messy data.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/course_intro.html#what-is-new-this-term-spring-2026",
    "href": "chapters/course_intro.html#what-is-new-this-term-spring-2026",
    "title": "2  Introduction",
    "section": "2.3 What is new this term (Spring 2026)?",
    "text": "2.3 What is new this term (Spring 2026)?\nFor Spring 2026, I have added attention to new topics that improve both efficiency of analysis workflows and our ability to understand causality at a deeper level. Specifically, I have chosen to add course components on:\n\nUsing large-language models (LLMs) –via ChatGPT– to improve analytical workflows. This term, I have integrated ChatGPT into the course as a way to support the process of analysis rather than to automate the analysis itself. Many of the hardest parts of quantitative work happen outside of writing code: figuring out why R code isn’t behaving as expected, checking whether an interpretation actually follows from a model, troubleshooting an obscure error message, or getting feedback on how tp clearly document a decision. Used carefully, ChatGPT can act like a sounding board for troubleshooting, sanity-checking, and refining explanations without taking over your critical thinking. Throughout the course, I have deliberately constrained ChatGPT’s role and scope and have tailored its guardrails to match the course progression. In other words, ChatGPT is used to help improve analytical workflow and decision-making, not to generate results or write code on a student’s behalf. We will work together to assess the utility of this tool as the term advances.\n\n\n\n\nUsing large-language models (LLMs) to improve teaching at scale. What does this mean? In past terms, between 10-15 students have been enrolled in this course. This term, I have allowed 26 students to enroll. Without a Teaching Assistant, this spawns a significant issue of scale. This is solvable if we use AI as a helpful tool. So, after spending some time as an LLM-consultant and tester, I decided to explicitly integrate chatGPT’s LLMs into my teaching/grading workflow. This means that students will use a custom GPT that I created specifically for ZOO/ECOL-5500 called JackalopeGPT (using ChatGPT) to run checks on their work prior to submission; this will solve some of the “tuning” issues (with codes, grammar, clarity, etc.) that I have seen in student submissions in previous years.\n\n\n\n\n\n\nStructural Causal Models (SCMs) I added Structural Causal Models (SCMs) at the end of the course to give us a way to think more clearly about complex systems where multiple variables influence each other at the same time. In the first part of the course, students will see the limits of fitting separate models (e.g. GLMs, GLMMs, GAMMs) for singular response variables; those models can work individually, but they often miss shared drivers, indirect effects, or how pieces of the complex system of exogenous and endogenous factors fit together. SCMs give us a way to lay out those relationships explicitly and analyze them as a connected structure (as is present in natural systems) rather than as a collection of isolated models. The goal isn’t to replace the modeling approaches we’ve already used but to improve inference by making our assumptions about the system clear and coherent.\nJackalopes! This course corrects a long-standing perspective that (1) jackalopes had deer-like antlers (Fig. 1), and (2) there was only a single species of jackalopes (Fig. 2). We will use the newly discovered cryptid ecosystem for course exercises and examples.\n\n\n\n\nFigure 1. Illustrations of the correct and incorrect ornament morphology of jackalopes.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html",
    "href": "chapters/syllabus.html",
    "title": "3  Syllabus",
    "section": "",
    "text": "3.1 Location and meeting times\nTuesday/Thursday on Zoom; 11:00AM - 12:15PM (Mountain Time) Spring term 2026",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#office-hours",
    "href": "chapters/syllabus.html#office-hours",
    "title": "3  Syllabus",
    "section": "3.2 Office hours",
    "text": "3.2 Office hours\nDue to the nature of the course, I will not have specific office hours for the course. I have extended class times (after our weekly meetings) to allow more time to ask questions and for one-on-one work. These are optional and may be canceled in some weeks depending on your interest in meeting.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#prerequisites",
    "href": "chapters/syllabus.html#prerequisites",
    "title": "3  Syllabus",
    "section": "3.3 Prerequisites",
    "text": "3.3 Prerequisites\nTo ensure your success in this course, the following are required: - You must have a dataset to be analyzed this semester. This is very important. This class will only cover data reformatting; we will not cover data processing (except as necessary in specific cases). - Your data analysis must not be used in another (past or present). - You must be at least in your second year of graduate school. - You must have some exposure to using the Program R (tidyverse preferred, but base R is nice too). - You must have taken a statistics course in the last five years.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#assessment-and-grading-standards",
    "href": "chapters/syllabus.html#assessment-and-grading-standards",
    "title": "3  Syllabus",
    "section": "3.4 Assessment and Grading Standards",
    "text": "3.4 Assessment and Grading Standards\nThis course is graded as Pass or Fail (technically “Satisfactory” or “Unsatisfactory”). To pass the course you need to do the following:\n\nParticipate at least 80% of the class meetings. Simply inform me of your absences (for health reasons, field research, etc.), and then do what you can to catch up with the work.\nTurn in all assignments on their due dates (see Course Calendar) and receive a Green Light on all assignment submissions by the end of term (see Assignments & Grading). This is especially critical until Spring Break, after which there will be more flexibility in your schedule.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#attendanceparticipation-policy",
    "href": "chapters/syllabus.html#attendanceparticipation-policy",
    "title": "3  Syllabus",
    "section": "3.5 Attendance/Participation Policy",
    "text": "3.5 Attendance/Participation Policy\nThis is a graduate level course, and you are here for your own benefit. That being said, I expect you to come to class, stay engaged with the material, and not only learn how to do your own analysis but understand other types of data and analyses by working with your group members. If you do this, you should have analyzed your own data by the end of the semester and have part of a manuscript completed. Please email me ahead of time when you will be unable to attend class for a valid reason.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "href": "chapters/syllabus.html#how-to-succeed-in-the-course-beyond-your-wildest-and-funniest-dreams",
    "title": "3  Syllabus",
    "section": "3.6 How to succeed in the course (beyond your wildest and funniest dreams)",
    "text": "3.6 How to succeed in the course (beyond your wildest and funniest dreams)\nGraduate school can be considerably challenging, as everyone is attempting to juggle research, teaching, classes, health, and family, all while coping with unexpected stressors. Course information is flying at you from every direction; there are many specific terms and concepts that you need to learn and operationalize. So, here are some reminders for you (even though I know you don’t need these):\n\nAsk questions! Even though there are no exams, take copious notes and work collaboratively to build course notes.\nDon’t be afraid to redirect the flow of the course. 5000-level courses should be flexible and fun. I want to give you time to think about and discuss the material. I’m willing to alter the pace of the course, change the order of topics, or devise new exercises for you. This is intended to be fun (while simultaneously transforming you into analytical gurus)! So, just talk to me about how I can help!\nRead all the material in this course guidebook. Many online courses require much more reading; this one does not.\nShow up to as many of the synchronous (Zoom) discussions as you can. When we meet together online, our goal will be to solidify everyone’s understanding of different concepts and how they are linked. These concepts will be useful as you navigate your own analysis project.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#expectations",
    "href": "chapters/syllabus.html#expectations",
    "title": "3  Syllabus",
    "section": "3.7 Expectations",
    "text": "3.7 Expectations\nAs your instructor, you should expect me to:\n\nTry my very hardest to make the course go smoothly (the reason you now have this nice new online course guide); but please be prepared for the inevitable hiccups. No matter how hard we all try, there always seem to be a few obstacles (like internet going down for a couple of hours when we’re on Zoom).\nRespond to questions within 24 hours during the work week. However, I likely will not respond during the weekend (unless there is an urgent matter).\nRespect you not only as a learner but as a colleague.\nUnderstand that these are strange and sometimes unforgiving times. We all have varying levels of tolerance and resistance to stress. If you are having a hard time for whatever reason, please communicate with me. I suck at judging, but I can do a hell of a job listening and working with you to solve a problem.\n\nAs a student, you are expected to:\n\nBe respectful of everyone in the class, including me.\nAsk for help if needed.\nTreat your presence in the classroom and your enrollment in this course as you would a job; Act professionally, arrive on time, pay attention, complete your work in a timely and professional manner, and treat your learning seriously.\nUnderstand that everyone is going through different things (family events, etc.), and be understanding of each other.\nBe engaged in the course.\nBe engaged within your assigned groups and help each other learn. Teaching another group member something you know solidifies your own knowledge and also sets you up to be a great future colleague.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#r-code-readings-and-discussion-sections",
    "href": "chapters/syllabus.html#r-code-readings-and-discussion-sections",
    "title": "3  Syllabus",
    "section": "3.8 R Code, readings, and discussion sections",
    "text": "3.8 R Code, readings, and discussion sections\nAll R code required for both instruction and hands-on exercises is available within this course book. Unlike past versions of this course, the present iteration no longer has traditional lectures. That said, I may respond to your questions by creating mini-presentations for you. I will share such material immediately after our discussions.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#classroom-climate-and-conduct",
    "href": "chapters/syllabus.html#classroom-climate-and-conduct",
    "title": "3  Syllabus",
    "section": "3.9 Classroom Climate and Conduct",
    "text": "3.9 Classroom Climate and Conduct\nAgain, you will be respectful towards your classmates and your instructors. Spirited debate and disagreement are to be expected in any classroom, and all perspectives will be heard, but we will behave civilly and with respect towards one another. Personal attacks, offensive language, name-calling, and dismissive gestures (eye-rolling, saying “whatever”, etc.) are not warranted in a learning atmosphere. Plus, in my opinion, such behavior shows an ability to problem-solve, which is counter to the mission of any university. As your instructor, I have the right to dismiss you from the classroom if you engage in disrespectful or disruptive behavior. Lastly, for the privacy of your fellow students, please do not record the lectures (unless with permission of the instructor).\n\n\n\n\n\n\nNoteA note on knowledge-sharing\n\n\n\n\n\nOur classroom is a shared intellectual space. Questions, mistakes, and partial understanding are part of learning. Please remember that supporting one another’s intellectual growth matters more than performative corrections or demonstrations of expertise.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#classroom-statement-on-diversity",
    "href": "chapters/syllabus.html#classroom-statement-on-diversity",
    "title": "3  Syllabus",
    "section": "3.10 Classroom Statement on Diversity",
    "text": "3.10 Classroom Statement on Diversity\nThe University of Wyoming values an educational environment that is diverse, equitable, and inclusive. The diversity that students and faculty bring to class, including age, country of origin, culture, disability, economic class, ethnicity, gender identity, immigration status, linguistic, political affiliation, race, religion, sexual orientation, veteran status, worldview, and other social and cultural diversity is valued, respected, and considered a resource for learning. we understand that our UW community members represent a rich variety of backgrounds and perspectives. We are committed to providing an atmosphere for learning that respects diversity of all types. While working together to build this community, we ask all members–from students to staff to faculty–to:\n\nBe transparent about pre-existing biases and beliefs.\nDo not hesitate to share their unique experiences and perspectives.\nBe open to the views of others.\nHonor the uniqueness of their colleagues.\nAppreciate the opportunity that we have to learn from each other in this community.\nValue each other’s opinions and communicate in a respectful manner.\nKeep confidential any discussions of a personal (or professional) nature.\nUse this opportunity together to discuss ways in which we can create an inclusive environment in this course and across the University community.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#duty-to-report",
    "href": "chapters/syllabus.html#duty-to-report",
    "title": "3  Syllabus",
    "section": "3.11 Duty to Report",
    "text": "3.11 Duty to Report\nUW faculty are committed to supporting students and upholding the University’s non-discrimination policy. Under Title IX, discrimination based upon sex and gender is prohibited. If you experience an incident of sex- or gender-based discrimination, we encourage you to report it. While you may talk to a faculty member, understand that as a “Responsible Employee” of the University, the faculty member is required to report information you share about the incident to the University’s Title IX Coordinator (you may choose whether you or anyone involved is identified by name). If you would like to speak with someone who may be able to afford you privacy or confidentiality, there are people who can meet with you. Faculty can help direct you or you may find info about UW policy and resources at http://www.uwyo.edu/reportit. While we want you to feel comfortable coming to us with issues you may be struggling with or concerns you may be having, please be aware that we have some reporting requirements that are part of our job requirements at UW. You do not have to go through the experience alone. For example, if you inform us of an issue of sexual harassment, sexual assault, or discrimination we will keep the information as private as we can, but we am required to bring it to the attention of the institution’s Title IX Coordinator. If you would like to talk to those offices directly, you can contact Equal Opportunity Report and Response (Bureau of Mines Room 319, 766-5200, report-it@uwyo.edu, www.uwyo.edu/reportit). Additionally, you can also report incidents or complaints to the UW Police Department. You can also get support at the STOP Violence program (stopviolence@uwyo.edu, www.uwyo.edu/stop, 766-3296) (or SAFE Project (www.safeproject.org, campus@safeproject.org, 766-3434, 24-Hour hotline: 745-3556). Assistance and resources are available, and you are not required to make a formal complaint or participate in an investigation to access them. Another common example is if you are struggling with an issue that may be traumatic, or under unusual stress. We will likely inform the Dean of Students Office or Counseling Center. If you would like to reach out directly to them for assistance, you can contact them by going to www.uwyo.edu/dos/uwyocares.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#disability-statement",
    "href": "chapters/syllabus.html#disability-statement",
    "title": "3  Syllabus",
    "section": "3.12 Disability Statement",
    "text": "3.12 Disability Statement\nIf you have a physical, learning, sensory or psychological disability and require accommodations, please let me know as soon as possible. You will need to register with, and provide documentation of your disability to University Disability Support Services (UDSS) in SEO, room 330 Knight Hall.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/syllabus.html#academic-honesty",
    "href": "chapters/syllabus.html#academic-honesty",
    "title": "3  Syllabus",
    "section": "3.13 Academic Honesty",
    "text": "3.13 Academic Honesty\nThe University of Wyoming is built upon a strong foundation of integrity, respect and trust. All members of the university community have a responsibility to be honest and the right to expect honesty from others. Any form of academic dishonesty is unacceptable to our community and will not be tolerated [from the University Catalog]. Teachers and students should report suspected violations of standards of academic honesty to the instructor, department head, or dean. Other University regulations can be found here",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html",
    "href": "chapters/calendar_sp26.html",
    "title": "4  Course calendar",
    "section": "",
    "text": "4.1 Calendar overview\nUse the calendar table below to see what’s coming each week; simply click a week (first column link) to jump to full details, and then expand sections to see what to do before, during, and after class. This calendar may change slightly as we progress through the term. You will receive immediate notification of any and all changes.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#calendar-overview",
    "href": "chapters/calendar_sp26.html#calendar-overview",
    "title": "4  Course calendar",
    "section": "",
    "text": "Note\n\n\n\nAll Milestone Assignments due by end of day on Friday.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nIn-class exercise\nMilestone assignment (due Friday)\nStress level\nGrade weight (%)\n\n\n\n\nWeek 119-Jan-2026\nCourse intro; RStudio setup; AI guardrails\nProject scoping discussion\n—\nVery low\n0\n\n\nWeek 226-Jan-2026\nFile structure; reproducible workflows; AI logs\nFile audit walkthrough\nAnalysis Concept Note\nLow\n5\n\n\nWeek 302-Feb-2026\nData exploration; measurement & uncertainty\nExploratory data analysis walkthrough\n—\nVery low\n0\n\n\nWeek 409-Feb-2026\nGLMs (theory + practice)\nGLM exercise\nData Readiness Note\nLow\n10\n\n\nWeek 516-Feb-2026\nGLMs; AIC & information theory\nModel comparison exercise\n—\nLow\n5\n\n\nWeek 623-Feb-2026\nGLMMs; effective sample size\nGLMM exercise\n—\nModerate\n15\n\n\nWeek 702-Mar-2026\nGAMs\nGAM exercise\nWorking Model (draft)\nModerate\n0\n\n\nWeek 809-Mar-2026\nGAMMs; spatial & temporal heterogeneity\nModel refinement exercise\nWorking Model (final lock)\nModerate\n15\n\n\nWeek 916-Mar-2026\nStudent Spring Break\n—\n—\nNone\n0\n\n\nWeek 1023-Mar-2026\nStructural Causal Modeling (SCM)\nCausal diagram critique\n—\nLow\n10\n\n\nWeek 1130-Mar-2026\nInstructor Spring Break\n—\n—\nNone\n0\n\n\nWeek 1206-Apr-2026\nPrediction & uncertainty\nPrediction checks\nInterpretation Memo\nLow\n10\n\n\nWeek 1313-Apr-2026\nSynthesis & justification\nPeer + AI review\n—\nModerate\n0\n\n\nWeek 1420-Apr-2026\nTables, figures, reporting standards\nTable/figure workshop\nResults Section\nLow\n10\n\n\nWeek 1527-Apr-2026\nWriting Results sections\nDraft clinic\nFull Draft\nModerate\n15\n\n\nWeek 1604-May-2026\nReflection & closure\nCourse wrap-up\nRevision Plan (not executed)\nNone\n5",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/calendar_sp26.html#weekly-information",
    "href": "chapters/calendar_sp26.html#weekly-information",
    "title": "4  Course calendar",
    "section": "4.2 Weekly Information",
    "text": "4.2 Weekly Information\nClick to expand sections below to see expectations before, during, and after class.\n\n\n\n\n\n\nTipWeek 1 (19-Jan-2026)\n\n\n\n\n\nTopics: Course introduction; RStudio setup; AI guardrails\nStress level: Very low\nGrade weight: 0%\n\nBefore class\n\nAbsolutely nothing.\n\n\n\nIn class\n\nCourse overview and expectations\nReview syllabus\nRead AI policy and documentation expectations\nProject scoping discussion\n\n\n\nAfter class\n\nNo submission due\nInstall R and RStudio\nRead AI use and documentation expectations\nBegin gathering your dataset files\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 2 (26-Jan-2026)\n\n\n\n\n\nTopics: File structure; reproducible workflows; AI logs\nStress level: Low\nGrade weight: 5%\n\nBefore class\n\nReview example project directory structures\nRead guidance on reproducible workflows\n\n\n\nIn class\n\nFile audit walkthrough\nDiscussion of AI interaction logging\n\n\n\nAfter class\n\nSubmit: Analysis Concept Note\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 3 (02-Feb-2026)\n\n\n\n\n\nTopics: Data exploration; measurement & uncertainty\nStress level: Very low\nGrade weight: 5%\n\nBefore class\n\nSkim examples of exploratory data analysis (EDA)\nReview notes on uncertainty and measurement error\n\n\n\nIn class\n\nExploratory data analysis walkthrough\nDiscussion of data limitations and noise\n\n\n\nAfter class\n\nNo submission due\nBegin informal exploration of your own data\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 4 (09-Feb-2026)\n\n\n\n\n\nTopics: GLMs (theory + practice)\nStress level: Low\nGrade weight: 10%\n\nBefore class\n\nRead GLM conceptual overview\nReview examples of common link functions\n\n\n\nIn class\n\nGLM exercise\nTranslating scientific questions into model form\n\n\n\nAfter class\n\nSubmit: Data Readiness Note\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 5 (16-Feb-2026)\n\n\n\n\n\nTopics: GLMs; AIC & information theory\nStress level: Low\nGrade weight: 5%\n\nBefore class\n\nReview information-theoretic model comparison\nRead example AIC workflows\n\n\n\nIn class\n\nModel comparison exercise\nInterpreting relative support\n\n\n\nAfter class\n\nNo submission due\nRefine candidate model sets\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 6 (23-Feb-2026)\n\n\n\n\n\nTopics: GLMMs; effective sample size\nStress level: Moderate\nGrade weight: 15%\n\nBefore class\n\nRead GLMM overview\nReview motivation for random effects\n\n\n\nIn class\n\nGLMM exercise\nDiscussion of pseudoreplication and effective sample size\n\n\n\nAfter class\n\nNo submission due\nBegin transitioning models to mixed frameworks where appropriate\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 7 (02-Mar-2026)\n\n\n\n\n\nTopics: GAMs\nStress level: Moderate\nGrade weight: 0%\n\nBefore class\n\nReview motivation for smooth terms\nRead conceptual introduction to GAMs\n\n\n\nIn class\n\nGAM exercise\nInterpreting smooths vs parametric effects\n\n\n\nAfter class\n\nSubmit: Working Model (draft)\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 8 (09-Mar-2026)\n\n\n\n\n\nTopics: GAMMs; spatial & temporal heterogeneity\nStress level: Moderate\nGrade weight: 15%\n\nBefore class\n\nReview examples of spatial and temporal structure\nRead GAMM case studies\n\n\n\nIn class\n\nModel refinement exercise\nDiagnosing remaining structure in residuals\n\n\n\nAfter class\n\nSubmit: Working Model (final lock)\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 9 (16-Mar-2026)\n\n\n\n\n\nTopics: Student Spring Break\nStress level: None\nGrade weight: 0%\n\nNotes\n\nNo class\nNo assignments due\nRecommended: rest, catch up, or light review if needed\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 10 (23-Mar-2026)\n\n\n\n\n\nTopics: Structural Causal Modeling (SCM)\nStress level: Low\nGrade weight: 10%\n\nBefore class\n\nRead SCM overview and motivation\nReview example causal diagrams\n\n\n\nIn class\n\nCausal diagram critique\nDiscussion of assumptions and identification\n\n\n\nAfter class\n\nNo submission due\nConsider how SCM reframes your modeling decisions\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 11 (30-Mar-2026)\n\n\n\n\n\nTopics: Instructor Spring Break\nStress level: None\nGrade weight: 0%\n\nNotes\n\nNo class\nNo assignments due\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 12 (06-Apr-2026)\n\n\n\n\n\nTopics: Prediction & uncertainty\nStress level: Low\nGrade weight: 10%\n\nBefore class\n\nReview prediction vs inference distinctions\nRead examples of uncertainty communication\n\n\n\nIn class\n\nPrediction checks\nEvaluating extrapolation risk\n\n\n\nAfter class\n\nSubmit: Interpretation Memo\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 13 (13-Apr-2026)\n\n\n\n\n\nTopics: Synthesis & justification\nStress level: Moderate\nGrade weight: 0%\n\nBefore class\n\nReview synthesis examples from prior studies\n\n\n\nIn class\n\nPeer + AI review\nJustifying analytical decisions\n\n\n\nAfter class\n\nNo submission due\nPrepare figures and tables for results\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 14 (20-Apr-2026)\n\n\n\n\n\nTopics: Tables, figures, reporting standards\nStress level: Low\nGrade weight: 10%\n\nBefore class\n\nReview reporting guidelines\nExamine good and bad figure examples\n\n\n\nIn class\n\nTable/figure workshop\nEmphasis on clarity and restraint\n\n\n\nAfter class\n\nSubmit: Results Section\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 15 (27-Apr-2026)\n\n\n\n\n\nTopics: Writing Results sections\nStress level: Moderate\nGrade weight: 15%\n\nBefore class\n\nReview example Results sections\nReflect on narrative flow\n\n\n\nIn class\n\nDraft clinic\nFocused feedback on structure and interpretation\n\n\n\nAfter class\n\nSubmit: Full Draft\n\n\n\n\n\n\n\n\n\n\n\nTipWeek 16 (04-May-2026)\n\n\n\n\n\nTopics: Reflection & closure\nStress level: None\nGrade weight: 5%\n\nBefore class\n\nReview feedback on full draft\n\n\n\nIn class\n\nCourse wrap-up\nReflection on analytical growth\n\n\n\nAfter class\n\nSubmit: Revision Plan (not executed)",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Course calendar</span>"
    ]
  },
  {
    "objectID": "chapters/ai_policy.html",
    "href": "chapters/ai_policy.html",
    "title": "5  GenAI Use Guidelines",
    "section": "",
    "text": "5.1 Timeline\nResponsible use of generative artificial intelligence (GenAI)\nThis documented was updated on 06-January-2026. This document should be reviewed periodically, particularly when adopting new GenAI tools or when disciplinary norms evolve (while resisting harmful, shifting baselines).",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/ai_policy.html#purpose",
    "href": "chapters/ai_policy.html#purpose",
    "title": "5  GenAI Use Guidelines",
    "section": "5.2 Purpose",
    "text": "5.2 Purpose\nThis document reviews principles, guard rails, and documentation practices for the responsible use of generative artificial intelligence (GenAI) tools in scientific research and analysis. One such GenAI tool that most of us are familiar with is ChatGPT (Generative Pre-trained Transformer), a GenAI model that can detect (and arguably understand) and generate human-like text by predicting what comes next in a sentence. Large Language Models (LLMs) are very advanced GenAI systems trained on absolutely massive amounts of text to answer complex questions, write in certain rhetorical tones, summarize information, or have conversations in natural language. Given this tool’s widespread utility, it can certainly be misused. Therefore, the guidelines below attempt to ensure that GenAI augments human reasoning without compromising scientific validity, reproducibility, or accountability. In all aspects of the present academic work, GenAI tools like ChatGPT are therefore treated as assistive tools –comparable to statistical software (like R) or calculators– not as autonomous analysts or skilled authors.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/ai_policy.html#core-principles-of-responsible-use-of-genai",
    "href": "chapters/ai_policy.html#core-principles-of-responsible-use-of-genai",
    "title": "5  GenAI Use Guidelines",
    "section": "5.3 Core principles of responsible use of GenAI",
    "text": "5.3 Core principles of responsible use of GenAI\n\nAccountability: The human researcher retains full responsibility for –and is held accountable for– all analytical decisions, interpretations, text and code, and inference. The use of GenAI does not relieve any burdens of authorship, responsibility, or liability. Importantly, this means that the human researcher is allowed –and, in most cases, is encouraged– to use AI-produced code, as long as the research has vetted and error-checked. This likewise assumes that the researcher accepts responsibility for any and all errors arising from AI assistance. That is, GenAI may accelerate work, but it may not replace understanding. If a researcher cannot defend a decision without the GenAI present, the decision is invalid.\nScientific Primacy: Scientific reasoning precedes and constrains GenAI use. Hypotheses, data-generating assumptions, and model structures must be defined by the researcher. GenAI may clarify or critique these decisions but may not create them from scratch without human justification.\nTransparency: All GenAI use must be documented in a way that allows another scientist to understand how GenAI influenced the work. GenAI-assisted reasoning must be distinguishable from original analysis.\nReproducibility: All results must be reproducible without access to GenAI tools. Data, code, and associated documentation must be sufficient to reproduce results independently. GenAI may assist development but must not be a hidden dependency.\nProportionality: The level of documentation naturally should be proportional to the influence of GenAI. Minor stylistic assistance requires minimal logging, while conceptual or analytical assistance requires a larger volume of explicit documentation and justification.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/ai_policy.html#what-is-allowed-and-what-is-not-allowed",
    "href": "chapters/ai_policy.html#what-is-allowed-and-what-is-not-allowed",
    "title": "5  GenAI Use Guidelines",
    "section": "5.4 What is allowed, and what is not allowed?",
    "text": "5.4 What is allowed, and what is not allowed?\n\nPermitted Uses of GPTs in Scientific Work\n\nConceptual Clarification: GenAI may be used to explain statistical concepts, examine modeling assumptions, and interpret model diagnostics (at a broad level).\nPlanning and Reflection: GenAI may assist with refining research questions, generating assumption checklists, stress-testing interpretations, and identifying alternative explanations that are then vetted for ecological sanity by the human researcher.\nWriting Support: GenAI may be used to improve clarity, organization, and tone, and to identify points of ambiguity or inferential overreach. AI absolutely must not invent methods, results, or citations.\nCode Understanding: GenAI may explain what existing code does, diagnose warnings or errors conceptually, and suggest stylistic or reproducibility improvements. AI may not replace independent code comprehension.\n\n\n\n\nProhibited or Restricted Uses of GenAIin Scientific Work\n\nReplacement of Scientific Judgment: GenAI must not be used to select models, error distributions, priors, or random-effects structures without independent human justification, nor to interpret results without verification.\nUndocumented Analysis Generation: GenAI must not generate full end-to-end analytical pipelines without explanation, or produce black-box code whose logic is not understood by the researcher.\nFabrication: GenAI must not be used to invent data, methods, results, or citations, or to create post-hoc justifications unsupported by the analysis.\nOutcome Optimization: GenAI must not be used to iteratively prompt for statistically significant results, improved AIC values, or exaggerated certainty.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/ai_policy.html#genai-interaction-log",
    "href": "chapters/ai_policy.html#genai-interaction-log",
    "title": "5  GenAI Use Guidelines",
    "section": "5.5 GenAI Interaction Log",
    "text": "5.5 GenAI Interaction Log\n\n5.5.1 Purpose of the Log\nResearchers should maintain an AI Interaction Log alongside their analysis notebook to document meaningful AI involvement in the research process. (This is a required component for students in ZOO/ECOL-5500 starting Spring 2026)\n\nRequired Information: Entries record the date, GenAI tool used, purpose of interaction, nature of assistance provided, key takeaway, and the verification step performed.\nWhen Logging Is Required: Logging is required when GenAI influences model choice, interpretation, methodological justification, or scientific claims. Minor stylistic or grammatical use does not require detailed logging.\nVerification Obligations: Any GenAI-suggested content must be independently verified using primary literature, software documentation, diagnostic checks, or independent reasoning. Unverified AI output must not appear in final analyses.\nAuthorship and Attribution: GenAI tools are not authors and do not receive citation credit. If required by journals or funders, GenAI use may be acknowledged in a neutral Methods or Acknowledgments statement.\nEthical Safeguards: GenAI use must not obscure uncertainty, inflate confidence, reduce methodological transparency, or disadvantage collaborators or students with limited access to AI tools.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GenAI Use Guidelines</span>"
    ]
  },
  {
    "objectID": "chapters/preparing_yourself.html",
    "href": "chapters/preparing_yourself.html",
    "title": "6  Preparing Yourself",
    "section": "",
    "text": "6.1 What do I need to do to get started in this class?\nTo prepare for the course, here is what you are expected to do by the end of the first week (specific information is below this list):",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparing Yourself</span>"
    ]
  },
  {
    "objectID": "chapters/preparing_yourself.html#what-do-i-need-to-do-to-get-started-in-this-class",
    "href": "chapters/preparing_yourself.html#what-do-i-need-to-do-to-get-started-in-this-class",
    "title": "6  Preparing Yourself",
    "section": "",
    "text": "Join the ZOO-5500 Slack (private link sent via email)\nDownload, Install, and set up RStudio Desktop\nGather your dataset files for your analysis\n\n\n6.1.1 Join the ZOO-5500 Slack\nJust click on the list and follow instructions. We will use Slack to encourage ongoing, low-stakes discussion within the class. This will be for sharing questions, ideas, and insights in real time so that learning can continue outside of scheduled class meetings. This allows everyone (even me) to get answers to questions that I might be thinking about at odd times. It also allows for problems to be solved faster; rather than waiting until our synchronous Zoom session to ask about an issue, we can group-think over the course of the week. Remember that this is set up to improve collaboration; it does not mean that anyone should be responding outside of work hours or on the weekends. Below are the preferred channels for different kinds of communication:\n\n#announcements &gt;Read-only channel for important course information like deadlines, corrections, and post-class clarifications. Check this regularly.\n#course-questions &gt;For general course or assignment questions (due dates, unclear wording on websites) that others might also have. Expect peer responses and occasional instructor follow-ups.\n#data-help &gt;For data wrangling and technical data issues (imports, joins, NAs, plots). Not for model choice or interpretation.\n#model-talk &gt;For conceptual questions about models and assumptions. Discussion-focused; no code debugging.\n#analysis-workflow &gt;For questions about reproducibility, organization, reporting, and good analytical habits.\n#ai-use-and-logs For discussing allowed AI use and posting AI interaction logs when requested. No private communication or graded work.\n#papers-good-bad-ugly &gt;For highlighting examples of good and bad analyses in published work.\n#random &gt;For off-topic chat, random links, and fun stuff. This will help keep everything else focused.\n\n\n\n6.1.2 Download, Install, and set up RStudio Desktop\nInstall on your local machine. All coding and file organization will be done in RStudio. Note that the following is a very basic way of creating a set of project folders. You will see boxes with R code. Feel free to copy and run these, if necessary. Here are the basic steps:\n\nDownload and install RStudio Desktop. Navigate to your folder (in this case, your student folder).\nIn “Files” window (lower right panel in RStudio), click “More/Set as Working Directory”. This sets your working directory; this is always a good idea to do.\nCreate a New R Project in your student folder: “File/New Project/Existing Directory”. Name this project appropriately.\nCreate your File structure: Refer to this RStudio tutorial. In this class, we will modify this set of directories to make things a bit easier and more transparent for you. After completing the following steps, place your dataset in your “data/raw_data” folder (after you create it below). Navigate to this folder on your computer and drag-and-drop or copy-paste your data file(s) into this folder.\n\nCreate three folders (one suggested framework):\n\n/data (for raw, processed, clean datasets)\n/r_scripts (for R scripts, each containing a set of related functions)\n/output output (for models, graphs, tables)\n\nWe can do this by staying in our directory (within RStudio) and then using the dir.create function.\n\ndir.create(\"data\")\n\nYou can see your new folder (“data”) appear in the Files in the lower right window within RStudio. Now let’s create the other two folders.\n\ndir.create(\"r_scripts\")\ndir.create(\"output\")\n\nTo help with data processing (i.e. cleaning and organization)–something that we will discuss in coming days within the context of your analytical workflow–, we will create some subfolders within your newly created “data” folder. Let’s do this now. Note how you create a folder within the “data” folder by specifying the path:\n\ndir.create(\"data/raw_data\")\ndir.create(\"data/processed_data\")\ndir.create(\"data/metadata\")\n\nAnd that is how you create a basic R Project in RStudio! What you have done is create a set of folders and subfolders that have names that can be easily understood by other users. Feel free to create other subfolders now. Or, should you not like the file nomenclature used above, change the names to whatever you wish. Just be sure that this structure is as simple as possible so that other users understand what you have done.\nThere is an added benefit to creating an R Project in this way. What you have done is create a minimally reproducible data structure. There are two primary ways this can be accomplished (if you are not familiar with this. First method: A collaborator (or instructor or classmate) can simply navigate to your subfolder, set that as their working directory from within RStudio, and then open the .RProj file. Alternatively, you can create zip your root folder (the one labelled as your name, in this case) and then share your entire analysis, etc. All the recipient would need to do is unzip the folder, open the .Rproj file, and then “Set as Working Directory.”\n\n\n6.1.3 Gather your dataset files for your analysis\nPlace your data in your “data/raw_data” subfolder and then confirm that your files are present by running the list.files function.\n\nlist.files(\"data/raw_data\")\n\n\n\n\n\n\n\nNote\n\n\n\nWorking order means more than a spreadsheet or two.\nA dataset is only complete if it includes metadata. Metadata describe: - what the variables are\n- how the data were collected\n- under what conditions\n- with what assumptions and limitations\nIf you have questions about metadata, I have an old, dedicated lecture on metadata structures, and I strongly encourage you to reach out.\n\n\nYou are now ready to move to the next steps of loading necessary R packages and beginning your journey of data exploration!",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparing Yourself</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html",
    "href": "chapters/assignment_grades.html",
    "title": "Assignments & Grades",
    "section": "",
    "text": "Milestone Assignment descriptions\nThis page first describes the Milestone Assignments referenced in the course calendar. Each section below explains the purpose of the milestone, what to submit, and what “good” typically looks like. Grading rubrics are described at the end of this page.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html#milestone-assignment-descriptions",
    "href": "chapters/assignment_grades.html#milestone-assignment-descriptions",
    "title": "Assignments & Grades",
    "section": "",
    "text": "NoteAnalysis Concept Note (Week 2)\n\n\n\n\n\nPurpose:\nLock in a feasible analysis plan early, so you’re not improvising later. This is about scope, clarity, and workflow—not results.\n\nWhat to include\nDownload the Analysis Concept Note template (.qmd)\n\nWorking title (can change later)\nYour focal question (1–3 sentences; plain language)\nStudy system and units of analysis (what is one “row,” conceptually?)\nResponse variable(s) and predictor(s) you expect to use (draft list is fine)\nHypothesized relationships\n\nShort bullets; include directionality when possible\n\nData source and file plan\n\nWhere the data live\n\nWhat you will name files\n\nWhat the folder structure will be\n\nPlanned model family (tentative)\n\ne.g., GLM / GLMM / GAM / GAMM, and why that’s your first guess\n\nPotential complications you already suspect\n\nMissing data, non-independence, zero inflation, seasonality, etc.\n\n\n\n\nSubmission format\n\n1–2 pages (PDF) or a short Quarto/Markdown page\nClear headings (don’t bury the key information)\n\n\n\n“Good” looks like\n\nA question that is answerable with the data you actually have\nReasonable scope for one semester\nA workflow plan that makes your project reproducible from day one\n\n\n\n\n\n\n\n\n\n\n\nNoteData Readiness Note (Week 4)\n\n\n\n\n\nPurpose: For your Data Readiness Note, you will build upon your Analysis Concept Note. (Note that this does not mean that you should simply paste the information below at the end of your Analysis Concept Note; it means that you should recycle.reuse the materials from your Analysis Concept Note as a foundation for your metadata and data readiness statement.)\nThe primary goal of the Data Readiness Note is to demonstrate that your dataset is analysis-ready—or identify exactly what still blocks you. This is your chance to be honest about limitations before modeling.\n\nSubmission format\nUnlike Assignment #1 (Analysis Concept Note), there is flexibility in the submission format. As you will see below, I recommend using the YAML template for Part 1 (metadata), but the format is otherwise up to you. If you submit in a rendered PDF or HTML, I will provide general feedback in text form (Word document, etc.); if you submit as QMD file, I will respond within callout boxes. Whatever works best for your workflow also works for me!\n\n\nWhat to include\nThe components in the list below should be included in your assignment submission; keep these in separate files for your own records, but please –if you can– combine them all into a single file before submitting via email.\nPart 1: Metadata file It is strongly suggested you use YAML Metadata Template, but you may also create a temporary metadata document using a spreadsheet. Your submission should include a very good initial attempt at making a formal metadata file that includes the following kinds of information (but could include more):\n\nDataset summary: Number of rows/observations, key grouping structure (site, individual, date, etc.)\nData dictionary (lightweight): Variable name → meaning → units → type (numeric, factor, date)\nMissingness and exclusions: what is missing, where is it missing from, and what you did about it (if anything)\nMeasurement and uncertainty notes: Known measurement error, detection limits, observer effects, instrument drift, etc.\n\nPart 2: Basic Exploratory Data Analysis outputs This component should include minimal but informative exploratory data analysis. You do not need to show density plots of all 1000+ variables. Highlight a few approaches that detected problematic issues, if possible. Again, make this useful for your own documentation purposes. This short section should include:\n\nBasic graphs or tables\n\n1–3 plots that reveal structure (distributions, relationships, time trends)\nA short paragraph interpreting what matters\n\nReadiness decision\n\n“Ready to model” or “Not ready yet,” with a clear plan to get there\n\n\nPart 3: AI-based Evaluation Table A document showing that you used JackalopeGPT to include the final Evaluation Table (i.e. rubric-tested) output from JackalopeGPT.\n\n\n“Good” looks like\nA submission of high quality will demonstrate: - that you have a clear idea about (and can explain) what each variable means and whether it is trustworthy - that you recognize your responsibility to document your metadata in a way that can be understood by future users - that you purposefully applied basic approaches of Exploratory Data Analysis (and have not used this as a simple plot dump) - that you are ready to move on to data analysis!\n\n\n\n\n\n\n\n\n\n\nNoteWorking Model — Draft (Week 7)\n\n\n\n\n\nPurpose:\nPut a real model on the table. This is a draft: imperfect is fine, but it must be runnable and defensible.\n\nWhat to include\n\nYour current model formula(s) (plain language and/or code notation)\nModel type (GLM, GLMM, GAM, etc.) and why it fits the data structure\nKey diagnostics or checks (draft level)\n\nAnything that suggests it’s “in the ballpark” or clearly broken\n\nOne short interpretation paragraph\n\nWhat the model suggests (careful language; no over-claiming)\n\nA short “known issues” list\n\nWhat you know is wrong or incomplete\n\nWhat you plan to try next\n\n\n\n\nSubmission format\n\nQuarto page (preferred) or PDF with model description and outputs\nInclude enough detail that someone else could reproduce the model from your write-up\nIn your Quarto-formatted (.qmd) submission, please include the final Evaluation Table (i.e. rubric-tested) output from JackalopeGPT.\n\n\n\n“Good” looks like\n\nA runnable model aligned with your question\nHonest diagnostics and a clear plan to improve\n\n\n\n\n\n\n\n\n\n\n\nNoteWorking Model — Final Lock (Week 8)\n\n\n\n\n\nPurpose:\nFreeze the core model so the rest of the semester can focus on prediction, interpretation, and reporting instead of endless tinkering.\n\nWhat to include\n\nFinal model specification\n\nFinal formula, family/link, random effects or smooths, correlation structures if used\n\nJustification\n\nWhy this structure is appropriate for the data and the question\n\nModel checks\n\nA small set of diagnostics you can defend\n\nNotes on remaining limitations\n\nWhat changes are no longer allowed\n\nAfter the lock, presentation and interpretation may improve, but model shopping should stop without compelling justification\n\n\n\n\nSubmission format\n\nQuarto page (preferred) or PDF\nInclude key outputs you will rely on later (even if figures improve)\nIn your Quarto-formatted (.qmd) submission, please include the final Evaluation Table (i.e. rubric-tested) output from JackalopeGPT.\n\n\n\n“Good” looks like\n\nA stable model you can explain to a skeptical reader\nEvidence you tested assumptions (rather than hoping)\n\n\n\n\n\n\n\n\n\n\n\nNoteInterpretation Memo (Week 12)\n\n\n\n\n\nPurpose:\nPractice disciplined interpretation and uncertainty-aware reasoning. This is where you show that you can tell the truth about what the model does—and does not—say.\n\nWhat to include\n\nMain result(s) in plain language\nUncertainty\n\nWhat is uncertain and why\n\nConfidence intervals, credible intervals, prediction intervals, or other appropriate summaries\n\nPrediction vs. inference clarity\n\nAre you predicting new cases, explaining mechanisms, or both?\n\nSensitivity or robustness (lightweight)\n\nOne small check that increases confidence (or reveals fragility)\n\nLimitations\n\nThe most important threats to inference and how they affect interpretation\n\n\n\n\nSubmission format\n\n~1–2 pages (PDF) or Quarto page\nEmphasis on writing quality and intellectual restraint\nIn your Quarto-formatted (.qmd) submission, please include the final Evaluation Table (i.e. rubric-tested) output from JackalopeGPT.\n\n\n\n“Good” looks like\n\nClear, honest claims tied directly to model output\nUncertainty is centered, not hidden\nNo story time beyond what the evidence supports\n\n\n\n\n\n\n\n\n\n\n\nNoteResults Section (Week 14)\n\n\n\n\n\nPurpose:\nDraft the Results section you would submit in a real paper or report: concise, structured, and supported by tables and figures.\n\nWhat to include\n\nShort orienting paragraph\n\nWhat you modeled and what the reader should look for\n\nCore results\n\n2–4 key findings, depending on project scope\n\nTables and/or figures\n\nClean, interpretable, labeled, and referenced in text\n\nMinimal interpretation\n\nState what you found; deeper interpretation comes later or is clearly separated\n\n\n\n\nSubmission format\n\nQuarto page (ideal) or PDF\nInclude figure and table captions\nIn your Quarto-formatted (.qmd) submission, please include the final Evaluation Table (i.e. rubric-tested) output from JackalopeGPT.\n\n\n\n“Good” looks like\n\nFindings are understandable without reading your code\nFigures and tables do real work\nClaims match the evidence shown\n\n\n\n\n\n\n\n\n\n\n\nNoteFull Draft (Week 15)\n\n\n\n\n\nPurpose:\nAssemble a complete, coherent draft that reads like a real scientific product. Structure and flow matter here.\n\nWhat to include\n\nAt minimum: Introduction, Methods, Results, Discussion (or equivalent)\nConsistent terminology\n\nVariable names, units, and definitions do not drift\n\nAll figures and tables in near-final form\nReferences and citations, as appropriate\n\n\n\nSubmission format\n\nQuarto render (HTML or PDF preferred)\nEnsure figures and tables are legible and captioned\nIn your Quarto-formatted (.qmd) submission, please include the final Evaluation Table (i.e. rubric-tested) output from JackalopeGPT.\n\n\n\n“Good” looks like\n\nA document that could be revised into something publishable\nClear logic and honest uncertainty\nNo missing major components\n\n\n\n\n\n\n\n\n\n\n\nNoteRevision Plan (Not Executed) (Week 16)\n\n\n\n\n\nPurpose:\nDemonstrate that you can revise strategically. You will not execute revisions; you will propose them.\n\nWhat to include\n\nTop five revision priorities\n\nRanked, with a sentence explaining why each matters\n\nWhat you would change (specific)\n\n“Rewrite paragraph X to clarify Y”\n\n“Replace Figure 2 with a plot that shows Z”\n\nWhat evidence you would need\n\nAdditional checks, plots, sensitivity analyses, etc.\n\nIf you had 10 more hours\n\nWhat would you do first?\n\nIf you had 40 more hours\n\nWhat would you do that you cannot do now?\n\n\n\n\nSubmission format\n\n1 page (PDF) or short Quarto page\nIn your Quarto-formatted (.qmd) submission, please include the final Evaluation Table (i.e. rubric-tested) output from JackalopeGPT.\n\n\n\n“Good” looks like\n\nSpecific, actionable revisions\nClearly prioritized and realistic\nDemonstrates mature judgment about improving inference and communication",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html#general-assignment-submission-requirements",
    "href": "chapters/assignment_grades.html#general-assignment-submission-requirements",
    "title": "Assignments & Grades",
    "section": "General assignment submission requirements",
    "text": "General assignment submission requirements\nFor many assignments, at least a partial template is provided (e.g. YAML or QMD). You may choose to submit a raw Quarto Markdown file or submit a version of it rendered as a PDF or in HTML (viewable in a browser). As long as it is legible and, more importantly, makes sense within your own workflow, it works for me.\nHere is a brief note about Quarto Markdown, which is preferred to R Markdown. While R Markdown is not obsolete by any means, it has a number of disadvantages compared to Quarto; these are summarized in the table below.\n\n\n\nDimension\nR Markdown\nQuarto\n\n\n\n\nage\nolder\nnewer\n\n\nprimary language\nR\nmulti-language\n\n\ndocument types\nreports\ndocs, books, sites, slides\n\n\ncross-referencing\nmanual\nbuilt-in\n\n\nextensibility\nconstrained\nmodular\n\n\n\nSo, in your own work, if you ever anticipate wanting to streamline your workflow, I recommend migrating your old R Markdown files to Quarto. Note that there are a few steps to do so, steps that involve changing file headers and syntax. I have had to do this with my older R Markdown files, and it doesn’t take much time. Quarto can really add huge advantages, including reproducible creation of scientific presentations (sometimes in minutes and not hours).\nFor this course, please use Quarto.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html#grading",
    "href": "chapters/assignment_grades.html#grading",
    "title": "Assignments & Grades",
    "section": "Grading",
    "text": "Grading\n\n\nNumerical grades are downright loathsome. For all assignments, I use the Traffic Signal Grading Scale:\n\n\n\n\n\n\n\n\nGrade\nSimple meaning\nScientific judgement\n\n\n\n\nGreen\nGood to Proceed\nScientifically coherent for this milestone\n\n\nYellow\nExercise Caution\nViable, but revision is required\n\n\nRed\nStop and Rework\nNot yet ready for scientific judgment\n\n\n\nYellow is not bad. It just means there are certain elements that you should tighten up before moving on. Red is also not bad. It just means there are more elements that you should tighten up before moving on. Hmm…see a pattern here? Maybe top-tier science is built upon iterative improvements?",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  },
  {
    "objectID": "chapters/assignment_grades.html#universal-milestone-grading-rubric",
    "href": "chapters/assignment_grades.html#universal-milestone-grading-rubric",
    "title": "Assignments & Grades",
    "section": "Universal Milestone Grading Rubric",
    "text": "Universal Milestone Grading Rubric\nThe Traffic Signal Grading Scale applies to all Milestone Assignments. Grading reflects scientific readiness relative to the purpose of the milestone.\n\n\n\n\n\n\n\n\n\nDimension\n🟢 GREEN: Good to Proceed\n🟡 YELLOW: Exercise Caution\n🔴 RED: Stop and Rework\n\n\n\n\nPurpose & intent\nClearly meets the core intent of the milestone\nPartially meets intent; key elements need revision\nDoes not meet the core intent of the milestone\n\n\nQuestion or objective\nClear, focused, and appropriate for this stage\nPresent but vague, drifting, or overly broad\nUnclear, unfocused, or missing\n\n\nAlignment (question–data–methods)\nQuestion, data, and approach are coherently aligned\nPartial alignment; gaps or weak links remain\nFundamental mismatch between components\n\n\nReasoning & assumptions\nReasoning is coherent; assumptions or limitations acknowledged\nReasoning incomplete or assumptions implicit\nReasoning unclear or unsupported\n\n\nClaims & language\nClaims are appropriate for the stage of analysis\nSome overreach or ambiguous language\nClaims exceed what the work can support\n\n\nCourse GPT self-check\nUsed appropriately; issues addressed or justified\nUsed, but issues only partially addressed\nLittle or no meaningful evidence of self-checking\n\n\nScientific readiness\nWork is ready to move to the next stage\nViable but requires targeted revision\nNot yet in a usable scientific state\n\n\n\nThis universal grading scale is meant to help you quickly understand where your work stands, not to highlight small mistakes. My goal here is to make expectations clear, keep grading transparent, and help you focus on doing solid, defensible science –all without unnecessary stress over points or formatting (which we will let GenAI work on for you). The scale’s primary benefits include:\n\nIt is easy to read. The traffic light color scheme (green, yellow, and red) tell you right away whether your work is ready to move forward, needs some fixing, or needs a more substantial rethink (the majority of good scientists’ ideas)\nIt is the same for every assignment. I use this scale for all milestone assignments. Therefore, the standards are predictable for every assignment.\nIt centers on scientific readiness, not scientific perfection (which, technically, does not exist). You are not being graded on having the “right” answer. What matters most to me is whether your question, data, and reasoning co-exist coherently together.\nIt matches how real science works. Research almost never gets it right on the first try; drafts, checks, and revisions are normal.\nIt allows for rapid feedback. GenAI will do some of the busy work associated with troubleshooting coding issues or improving clarity, which will give me more time to gauge the scientific readiness of your work. A win-win for us all!",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Assignments & Grades</span>"
    ]
  },
  {
    "objectID": "chapters/project_scoping.html",
    "href": "chapters/project_scoping.html",
    "title": "Project Scoping",
    "section": "",
    "text": "The big idea: pick a data analysis project that benefits from the course topics\nThis course is designed so that your final product (Methods + Results + start of Discussion / primary inferences) is built incrementally across Milestone Assignments. The fastest way to struggle in this course is to choose a project that is “interesting” but not workable (either because it is too big, or it is beyond your current experience level). The fastest way to thrive is to choose a project that is both meaningful to you and also has a clearly defined scope.\nBelow are a few concrete recommendations for scoping a project that is:\nEvery milestone is a checkpoint that asks you to refine the same project:\nA well-scoped project means each milestone feels like a natural next step, not a complete and uncomfortable reset.\nBelow are five Principles of Scoping for projects in this course (click on the boxes to expand):",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Scoping</span>"
    ]
  },
  {
    "objectID": "chapters/project_scoping.html#the-big-idea-pick-a-data-analysis-project-that-benefits-from-the-course-topics",
    "href": "chapters/project_scoping.html#the-big-idea-pick-a-data-analysis-project-that-benefits-from-the-course-topics",
    "title": "Project Scoping",
    "section": "",
    "text": "Week 2: Analysis Concept Note (scope + design clarity + workflow plan)\nWeek 4: Data Readiness Note (data trustworthiness + Exploratory Data Analysis with purpose)\nWeek 7–8: Working Model (runnable → defensible → locked)\nWeek 12: Interpretation Memo (uncertainty-aware reasoning)\nWeek 14: Results Section (clear, concise quantitative reporting)\nWeek 15: Full Draft (coherent paper-like product)\nWeek 16: Revision Plan (strategic improvement thinking)\n\n\n\n\n\n\n\n\n\nTipScoping Principle #1: “You only get one semester of attention”\n\n\n\n\n\nChoose a question that you will be happy thinking about repeatedly but only for this finite, semester-long period. If the project is too small, there is a danger that you may become bored. If the project is too big and ambitious, you will likely drown in a turbulent sea of discontent (and then blame me or your housemate).\nA good scope has:\n\nOne central question (plain language; 1–3 sentences)\nOne primary response variable (or a tightly related pair)\nA manageable predictor set (start small; justify later additions)\nA clear unit of analysis (what is one “row,” conceptually?)\nA realistic path to one defensible model by Week 8\n\n\n\n\n\n\n\n\n\n\nTipScoping Principle #2: “Your project should teach you the course”\n\n\n\n\n\nThis course covers:\n\nMetrology, uncertainty, and data quality\nExploratory Data Analysis for messy data\nGLMs → GLMMs → GAMs / GAMMs\nSpatial / temporal heterogeneity\nModel comparison (AIC)\nPrediction and validation\nStructural causal modeling (conceptual level; DAGs)\nWriting results with restraint (defensible claims)\n\nA strong project doesn’t need to use every tool — but it should naturally connect to several of them. Ideally, your project has at least one “real” complication that forces you to think like a scientist:\n\nnon-independence (repeated measures; clustered sampling)\nzeros (many true zeros or detection problems)\nunequal effort / detectability issues\nseasonality or temporal structure\nspatial clustering / site heterogeneity\nmeasurement uncertainty or instrument drift\n\n\n\n\n\n\n\n\n\n\nTipScoping Principle #3: Use this to build an reusable infrastructure\n\n\n\n\n\nThe biggest hidden benefit of a good scope is that it rewards you for building a clean workflow early:\n\nclear folder structure\nreproducible Quarto document(s)\na lightweight data dictionary\nstable variable names and units\nAI interaction logging for troubleshooting and drift tracking\n\nIf your project is well-scoped, each improvement you make in Week 2–4 pays dividends in Weeks 7–15.\n\n\n\n\n\n\n\n\n\nTipScoping Principle #4: pick something that advances your degree\n\n\n\n\n\nIf possible, choose a dataset that:\n\nis from your lab, thesis, dissertation, or a collaborator\nconnects to a real paper/report you could write\nhas a real audience beyond this course (advisor, lab group, agency, etc.)\n\nEven if the final product is “only” a course draft, you want it to be a useful artifact you can revise later.\n\n\n\n\n\n\n\n\n\nTipScoping Principle #5: be honest about what you have right now\n\n\n\n\n\nA project is not “good” because it is fascinating; it is good because you can answer something with the data you actually possess.\nBefore committing, you should be able to answer:\n\nDo I have the dataset in-hand by Week 2?\nDoes the dataset need cleaning, and can this be done by Weeks 2-3?\nAre the key variables already measured?\nCan I explain each variable’s meaning and units by Week 4?\nIs the sampling design understandable enough to model by Week 7?\nCan I reasonably lock a core model by Week 8?\n\nIf the answer is no to any of these, you can still proceed, but you must scope down to what is truly feasible.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Scoping</span>"
    ]
  },
  {
    "objectID": "chapters/project_scoping.html#a-practical-scoping-checklist",
    "href": "chapters/project_scoping.html#a-practical-scoping-checklist",
    "title": "Project Scoping",
    "section": "A practical scoping checklist",
    "text": "A practical scoping checklist\nChoose a project that meets most of these (given your current knowledge):\n\nOne question: you can state clearly in plain language\n\nOne primary response: that matches the question\n\nA known sampling structure: (site / individual / time / observer, etc.)\n\nA plausible model family: (GLM / GLMM / GAM / GAMM) you can defend\n\nOne major complication: you can address explicitly (zeros, clustering, etc.)\n\nA results story you can tell honestly: without over-claiming\n\nA dataset you can understand and trust: enough to write Methods",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Scoping</span>"
    ]
  },
  {
    "objectID": "chapters/project_scoping.html#examples-of-good-vs.-bad-scope",
    "href": "chapters/project_scoping.html#examples-of-good-vs.-bad-scope",
    "title": "Project Scoping",
    "section": "Examples of good vs. bad scope",
    "text": "Examples of good vs. bad scope\nBelow are examples you can use as patterns. These are collapsed so you can skim quickly.\n\n\n\n\n\n\n\n\nNoteGood scope example (light green)\n\n\n\n\n\nProject title (working):\nCanopy structure and bird counts in repeated point counts\nCentral question (plain language):\nDoes canopy cover predict bird counts per visit, after accounting for site-to-site variation?\nData/design clarity:\n- 30 sites, 4 visits per site\n- Response: count per visit\n- Predictors: canopy cover, wind, observer\n- Grouping: site (random intercept)\nComplication (one is enough):\n- detection likely declines with canopy and wind\n- repeated measures → non-independence\nWhy this is well-scoped:\n- runnable GLMM by Week 7\n- lockable model by Week 8\n- meaningful uncertainty-aware interpretation by Week 12\n- produces a clean Results section by Week 14\nWhat you don’t try to do:\n- no “global biodiversity mechanisms”\n- no causal claims without design support\n- no multi-species hierarchy unless truly necessary\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBad scope example (light red)\n\n\n\n\n\nProject title (vague):\nWhat drives biodiversity in tropical forests?\nCentral question (problem):\nToo broad to be answerable in one semester\nData/design issues:\n- response variable not defined\n- sampling design unclear (“multiple sites and years”)\n- predictors not specified\n- unit of analysis unknown (plot? site? species? time?)\nComplications (too many, unbounded):\n- spatial structure + temporal trends + detection + species turnover\n- multiple outcomes (richness + abundance + composition + traits)\nWhy this scope probably fails:\n- you cannot write Methods early\n- Week 4 becomes endless data wrangling\n- Week 7 has no runnable model (or 10 models with no rubric)\n- Week 8 “lock” is impossible\n- Results become unfocused and hard to defend\nWhat this needs to become viable:\n- choose one response, one scale, one question, and one model family",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Scoping</span>"
    ]
  },
  {
    "objectID": "chapters/project_scoping.html#scope-down-moves-that-work-and-feel-good",
    "href": "chapters/project_scoping.html#scope-down-moves-that-work-and-feel-good",
    "title": "Project Scoping",
    "section": "“Scope down” moves that work (and feel good)",
    "text": "“Scope down” moves that work (and feel good)\nIf your idea is too big, these moves are almost always helpful (but we can talk about what is best for your particular goals):\n\nPick one response variable (one outcome, not five)\nChoose one spatial scale (plots or sites, not a whole hierarchy)\nChoose one time window (e.g., one season or one year)\nStart with one model family (GLMM or GAMM — justify later)\nLimit the predictor set to a small set you can defend\nTreat extra complexity as a sensitivity check, not the core project",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Scoping</span>"
    ]
  },
  {
    "objectID": "chapters/project_scoping.html#a-final-recommendation-choose-the-project-youll-revisit-after-the-course",
    "href": "chapters/project_scoping.html#a-final-recommendation-choose-the-project-youll-revisit-after-the-course",
    "title": "Project Scoping",
    "section": "A final recommendation: choose the project you’ll revisit after the course",
    "text": "A final recommendation: choose the project you’ll revisit after the course\nPick something you would be proud to show:\n\nyour advisor\nyour lab group\na collaborator\na future committee member\nyour future self\n\nIf you choose well, ZOO/ECOL-5500 will not simply teach methods; it will produce a real, useful product that helps you in the future.",
    "crumbs": [
      "Course Overview",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Scoping</span>"
    ]
  },
  {
    "objectID": "chapters/ai_prompting.html",
    "href": "chapters/ai_prompting.html",
    "title": "9  AI Prompting for Analysis",
    "section": "",
    "text": "9.1 Overview\nThis page covers the basics of Large Language Models (LLM), the fundamentals of good prompt engineering and design, and the specifics of how to interact with the JackalopeGPT, the custom GPT for this course.",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI Prompting for Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ai_prompting.html#overview",
    "href": "chapters/ai_prompting.html#overview",
    "title": "9  AI Prompting for Analysis",
    "section": "",
    "text": "9.1.1 What are Large Language Models (e.g. ChatGPT, Gemini, Claude)\nJackalopeGPT is a customized GPT assistant tailored specifically for this course using OpenAI’s ChatGPT, a Large Language Model (LLM). Large Language Models (LLMs) like ChatGPT are advanced generative artifical intelligence systems trained on absolutely massive collections of text, enabling them to answer complex questions, summarize information, write in particular styles, and interact using natural language. Large Language Models are trained in two distinct steps that intentionally put humans in the loop:\n\nSupervised fine-tuning (SFT) teaches the model what to say by training on human-written input–output examples.\nReinforcement learning from human feedback (RLHF) teaches the model how to say it by rewarding outputs people judge as better (clearer, safer, more helpful, etc.).\n\nThe key takeaway is very practical for our purposes in this course: these models are optimized for interaction (between the user and LLM). The trade-off is that they are not necessarily built for accuracy. Model responses to users’ prompts (see Section 8.1.2) can sound amazingly confident and authoritative even when their information is grossly incorrect.\n\n\n9.1.2 What are prompts?\nAs stated above, you begin each interaction with an LLM using a prompt. A prompt is simply your input to the trained model; it is your question or request. There are two broad classes of prompts that are operationally differentiated by their purpose:\n\nEvaluative prompts: Models that produce content (e.g., text, code, summaries, or ideas).\nGenerative prompts: Models that assess or critique ingested data.\n\nIn this course, your goal for JackalopeGPT is not to “get an answer,” so we primarily use evaluative prompts. This prompting type keeps you in control of important scientific decisions and constrains the model’s role to that of a friendly reviewer or sounding board (rather than an autonomous data scientist).\n\n\n\n\n\n\nNoteGood prompt engineering takes practice\n\n\n\nPlease note that it may take a bit of practice to concisely write prompts that contain all of this information and yield high-quality responses, but this is a good framework to get you started.",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI Prompting for Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ai_prompting.html#engineering-successful-prompts",
    "href": "chapters/ai_prompting.html#engineering-successful-prompts",
    "title": "9  AI Prompting for Analysis",
    "section": "9.2 Engineering Successful Prompts",
    "text": "9.2 Engineering Successful Prompts\n\n9.2.1 The iterative prompting loop\nSuccessful prompting is usually iterative, involving the following basic steps:\n\nState expectations clearly. Your goal is to produce an output you can evaluate.\nEvaluate the response using a documented rubric. You should always have an implicit (or explicit) rubric in mind: a short set of criteria that defines what a good response should contain.\nRevise the prompt to reduce ambiguity and increase verifiability. Use the rubric to improve the prompt by clarifying the task/topic/context, adding constraints, and requiring formal checks so the response is easier to evaluate.\nRepeat (briefly, yet thoughtfully). until the output is actionable.\n\n\n\n9.2.2 Why good prompting matters (and why you should verify outputs)\nLarge Language Models can be extremely helpful, but they are not a scientific authority, and its responses are not “timeless.” That is, the quality of the responses depends heavily on what you ask and when you ask it (given what information has been recently folded into the LLM training).\n\n\n9.2.3 Actions for writing strong prompts\n\nLeverage your expertise: Use your biological knowledge to define what matters: structure of your study design, sources of bias (detectability of individuals), confounding variables, scale, or non-independence.\nBe challenging and specific. Remove the escape routes. Provide the response type, design, predictors, and what you want back (models, diagnostics, interpretation template).\nUse explicit constraints: Tell it how to answer (format, scope, evidence requirements), not just what to answer.\nForce juggling: Require it to handle multiple issues at once (e.g., zero inflation + nesting + spatial clustering).\nKeep it realistic: Ask for defensible inference, not magic. Explicitly ask it to flag overreach.\nMake prompts reusable: Favor principles and reasoning over tool/version-specific instructions (unless needed).\nMake prompts unambiguous and evaluable: Avoid “this/that/the above” references that only make sense outside the current chat.\nAvoid broad prompts; make outputs verifiable. Interaction GPTs love to spread their wings when given some wiggle room.\nWrite original prompts. Writing original prompts grounds the model in how you think about a scientific question. The model can help refine language or suggest options, but it should not invent the prompt’s goals.\n\n\n\n\n9.2.4 TRACE: a framework for writing strong prompts\nUse TRACE as a memory hook (technically, a acronymic mnemonic) for what to include in high-quality, repeatable prompts:\n\nT (Task/Topic, aka Context): What problem are you working on? What’s the data/design?\nR (Role): What kind of helper should JackalopeGPT be?\nA (Audience): What level should it explain to?\nC (Criteria): What must a good answer include?\nE (Exclusions): What should it avoid? What is out-of-scope?\n\nHere is a more detailed description. The bracketed, italicized components indicate places where you enter your original prompt components. Examples are given in the Description column.\n\n\n\n\n\n\n\n\n\nLetter\nComponent\nDescription\n\n\n\n\nT\nTask / Topic\nI am working on: [system + response + sampling design + sample sizes]. Known issues: [zeros / detectability / spatial clustering / non-independence / confounding].   Example: I have repeated point counts at 30 sites (4 visits per site). The response variable is bird counts per visit. Predictors include canopy density, wind, and observer. I expect detection of individuals to decline as a function of canopy density and wind.\n\n\nR\nRole\nAct as a [R tutor / methods advisor / model debugger].   Example: Act as an ecology methods advisor and R tutor.\n\n\nA\nAudience\nExplain to a [specified knowledge level].   Example: Explain to a 1st–2nd year ecology graduate student with basic statistics knowledge, who has 5–6 months’ experience working with Generalized Linear Models (GLMs) in R.\n\n\nC\nCriteria\nA good answer must include: [(1) restate the design, (2) assumptions, (3) 2–3 candidate approaches, (4) diagnostics and checks, (5) interpretation limits and what would change the recommendation.]   Example: Propose two scientifically defensible modeling approaches. For each, state assumptions, specify the model structure (in words or formula), list at least two scientifically defensible model diagnostics, and explain what result would be misleading if detection bias is not accounted for.\n\n\nE\nExclusions\nDo not: [write my full assignment / invent data / claim causality / skip assumptions / provide code if concepts only were requested].   Example: Do not claim causal effects. Do not invent data. Do not write end-to-end analysis code. If information is missing, list what is needed rather than guessing.\n\n\n\n\n\n\n\n\n\n\nTipTRACE templates for your convenience\n\n\n\n\n\nGoogle Sheet template (Read-only; you can copy and paste)\nGoogle Doc template (Read-only; you can copy and paste)",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI Prompting for Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ai_prompting.html#evaluating-llm-output",
    "href": "chapters/ai_prompting.html#evaluating-llm-output",
    "title": "9  AI Prompting for Analysis",
    "section": "9.3 Evaluating LLM Output",
    "text": "9.3 Evaluating LLM Output\n\n9.3.1 The Rubric-Prompt Synergy\nA rubric is your short checklist for what a complete and accurate response must include. Do not skip it or underestimate its utility: LLMs can sound confident while being downright wrong and incomplete. A good rubric –derived from your own expertise– is how you keep yourself in control of the LLM’s behavior.\nThink of your prompt as a cake recipe and the rubric as the taste test. (I am already thinking this analogy isn’t worth its salt.) The cake might look finished and ready to bring to the party, but the rubric if what tells you whether the cake is actually edible. In other words, you and your rubric must:\n\nNOT ask “Is this answer impressive and sounds smart?”\n\nDefinitely ask “Is this answer complete, appropriately constrained, and testable or verifiable?”\n\n\n\n9.3.2 Rubric criteria\nAfter developing and running a conceptually good prompt, you can be satisfied that a response is worth using if it satisfies the following criteria:\n\nGets the problem right\n\nStates its assumptions\n\nExplains its reasoning\n\nShows how to check itself\n\nKnows where it could be wrong\n\nIf any one of these is missing, revise the prompt before revising your analysis. To see details about each of the above criteria, click on the green box below to expand.\n\n\n\n\n\n\nTipDetailed criteria for assessing LLM responses\n\n\n\n\n\n\nDoes it get the problem right?\n\nCheck whether the response correctly restates your system, data, and task.\nLook for invented details or a mischaracterized study design.\nIf this fails, stop and revise the prompt (not your interpretation).\n\n\n\n\nDoes it state its assumptions?\n\nIdentify whether assumptions are explicit (e.g., independence, sampling, detection, distributional form, causal limits).\nMissing or vague assumptions make recommendations hard to evaluate.\nTreat missing assumptions as a prompt failure, not a model error.\n\n\n\n\nDoes it explain its reasoning?\n\nLook for a clear, step-by-step chain from design → model → inference.\nExplanations should justify why recommendations follow from the setup.\nWithout step-by-step reasoning, we cannot evaluate conclusions.\n\n\n\n\nIs it verifiable?\n\nThe response should include at least one diagnostic, stress test, or sensitivity check (which we will discuss later in the course)\nIt should explain how the recommendation could fail or mislead.\nAdvice without checks is not trustworthy.\n\n\n\n\nDoes it know where it could be wrong?\n\nCheck for limits: what cannot be concluded, what would be overreach, what remains uncertain.\nLook for acknowledgment of uncertainty.\nOverconfidence is a severe red flag.\n\n\n\n\n\n\n\n9.3.3 Quantifying Model Drift: Another way to put rubrics to work\nLarge Language Models are updated and adjusted over time. That means the same prompt can yield different answers across weeks or months—sometimes subtly, sometimes dramatically. To stay scientifically grounded, you need a habit of validation, a way to detect when the LLM’s behavior has changed and when outputs should no longer be trusted. Consider the following scenario. Imagine you weigh your study subjects on a scale every morning. A lab jackanape recalibrates the scale every few weeks without telling you. After discovering that this has occurred, would you trust that the measurements are directly comparable across time? However, if your lab protocol included a daily measurement of a standardized set of weights, you could determine exactly when recalibrations were back, and, what’s even better, you could adjust your measurements accordingly. Science would be saved!\n\nA practical –and easy– way to assess “model drift” in your own work\n\nKeep 3–5 benchmark prompts you reuse all semester (e.g., “fit a GLMM with random intercept for site, explain assumptions, propose diagnostics”). Ideally, you should document and store these as metadata, so that the phrase(s) can always be connected to a project and also be easily accessed.\nRe-run the same benchmark prompt occasionally (at a predefined interval or whenever you begin a new interaction or project). Save (copy/paste) the prompt + output into your AI Interaction Log.\nExamine the output for changes in:\n\nwhether it correctly restates the design,\nwhether it flags assumptions and/or constraints,\nwhether it proposes diagnostics that are scientifically defensible,\nwhether it starts inventing details or over-claiming.\n[other criteria that you can think of?]",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI Prompting for Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ai_prompting.html#synthesis-examples-of-improving-prompts",
    "href": "chapters/ai_prompting.html#synthesis-examples-of-improving-prompts",
    "title": "9  AI Prompting for Analysis",
    "section": "9.4 Synthesis: Examples of improving prompts",
    "text": "9.4 Synthesis: Examples of improving prompts\nInstructions to students: Read the excessively broad-scope prompt first. Pause and think:\n\nWhat information is missing (hint: use the TRACE framework)?\nWhat verbiage could be added to fill most or all of those missing components? Then expand the box to see a stronger version (based in the TRACE framework)\n\nRubric tag key (TRACE):\n\n[T] Task/Topic context missing or underspecified\n[R] Role missing\n\n[A] Audience missing\n\n[C] Criteria missing (what a “good” answer must include)\n\n[E] Exclusions missing (scope/safeguards)\n\n\n\n9.4.1 Example 1: Debugging R code\n\n“My model isn’t working. Help.”\n\nThis prompt lacks all components of a useful prompt. This would lead to an overly broad response from the Large Language Model. Before clicking to expand this example, think about ways you could improve this prompt.\n\n\n\n\n\n\nTipClick to reveal a stronger prompt (TRACE)\n\n\n\n\n\nMissing tags: [T] [R] [A] [C] [E]\nRubric tags addressed: [T] [R] [A] [C] [E]\n\nAct as an R debugger. I’m a beginner–intermediate R user. I’m fitting a GLMM for counts with a random intercept for site. Here is my code and the exact error message:. A good answer must: (1) identify the likely cause, (2) show the minimal fix, (3) explain why it failed, (4) suggest one diagnostic check. Do not rewrite my entire analysis—just fix the error and explain.\n\n\n\n\n\n\n\n9.4.2 Example 2: Choosing a model strategy (polished but incomplete)\n\n“I have ecological count data collected over multiple sites and years, and I want to choose an appropriate statistical model that accounts for structure in the data. Can you recommend a suitable modeling approach and explain why?”\n\nThis prompt sounds careful and scientific, but it fails to bound the answer and require checks, inviting a vague or overbroad response. Before clicking to expand this example, think about ways you could improve this prompt.\n\n\n\n\n\n\nTipClick to reveal a stronger prompt (TRACE)\n\n\n\n\n\nMissing tags: [C] [E]\nRubric tags addressed: [T] [R] [A] [C] [E]\n\nAct as an ecology methods advisor. I am a graduate student in ecology with variable experience in statistical modeling. I have count data with many zeros; samples nested in plots within sites; repeated measures over time. Compare 2–3 modeling strategies (e.g., negative binomial GLMM vs zero-inflated vs hurdle), state assumptions, and give diagnostics that would falsify each. Do not claim causality; focus on defensible inference.\n\n\n\n\n\n\n\n9.4.3 Example 3: Interpreting a model result\n\n“How do I interpret this coefficient?”\n\nThis is another example of a prompt that is vague even by the standards of a simple web search. This approach invites a vague response or evil hallucinations by the Large Language Model. Before clicking to expand this example, think about ways you could improve this prompt.\n\n\n\n\n\n\nTipClick to reveal a stronger prompt (TRACE)\n\n\n\n\n\nMissing tags: [T] [R] [A] [C] [E]\nRubric tags addressed: [T] [R] [A] [C] [E]\n\nAct as a statistical interpreter for ecology graduate students. I am a graduate student learning how to interpret fitted models rather than build them. I’m fitting a negative binomial GLMM for bird counts with canopy cover as a predictor and site as a random effect. Explain how to interpret the canopy coefficient on the link scale and response scale, state the assumptions required for this interpretation, and describe one way this interpretation could be misleading. Do not claim causal effects.\n\n\n\n\n\n\n\n9.4.4 Example 4: Checking model assumptions\n\n“I’m using a Poisson mixed-effects model for count data with repeated measurements across sites. Can you explain the assumptions of this model and whether it’s appropriate for my analysis?”\n\nThis prompt is very polished but incomplete as it lacks a set of criteria for which a good answer must include. Adding such criteria helps the model craft a more useful response. Before clicking to expand this example, think about ways you could improve this prompt.\n\n\n\n\n\n\nTipClick to reveal a stronger prompt (TRACE)\n\n\n\n\n\nMissing tags: [C]\nRubric tags addressed: [T] [R] [A] [C] [E]\n\nAct as an ecology methods advisor. I am a graduate student in ecology with limited but growing experience using mixed-effects models. I’m using a Poisson GLMM for count data with repeated measures per site. List the key assumptions, propose 2–3 diagnostics to evaluate them, and explain what pattern in the diagnostics would indicate a serious problem. If information is missing, list what you need rather than guessing.",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI Prompting for Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html",
    "href": "chapters/using_jackalope_gpt.html",
    "title": "10  Using JackalopeGPT",
    "section": "",
    "text": "10.1 A Reminder: What is JackalopeGPT?\nJackalopeGPT is a learning assistant designed to support quantitative and methodological reasoning in this course. Its primary purpose is to help you improve scientific rigor and streamline your analytical workflow while allowing you to maintain full responsibility for:\nJackalopeGPT does not assign grades (except for the Traffic-Signal readiness lights), make final evaluative decisions, or fill in missing information. Instead, JackalopeGPT has been customized to emphasize:\nIn other words, JackalopeGPT has been designed to help you think better, not to think less.",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#a-reminder-what-is-jackalopegpt",
    "href": "chapters/using_jackalope_gpt.html#a-reminder-what-is-jackalopegpt",
    "title": "10  Using JackalopeGPT",
    "section": "",
    "text": "Reasoning\n\nInterpretation\n\nvalidation\n\nAll final assignment submissions\n\n\n\nExplicit assumptions\nTransparent reasoning\n\nScope control\n\nReproducible workflows",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#how-you-can-start-using-it-important",
    "href": "chapters/using_jackalope_gpt.html#how-you-can-start-using-it-important",
    "title": "10  Using JackalopeGPT",
    "section": "10.2 How You Can Start Using It (Important)",
    "text": "10.2 How You Can Start Using It (Important)\nIf you are first using JackalopeGPT to optimize your prompts or evaluate an assignment’s completeness before submission, you should use the Main Menu. However, you do not need to choose anything from the Main Menu to begin. The Main Menu exists only for structured actions (such as readiness checks or AI interaction logging). Most learning happens without it. It is only provided for a bit more convenience. You may simply start prompting JackalopeGPT naturally, as you would with any chat-based GPT.",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#operating-modes",
    "href": "chapters/using_jackalope_gpt.html#operating-modes",
    "title": "10  Using JackalopeGPT",
    "section": "10.3 Operating Modes",
    "text": "10.3 Operating Modes\nJackalopeGPT operates in three distinct modes:\n\n10.3.1 Learning & Practice (default)\n\nConcept clarification\n\nCode debugging or refactoring\n\nTranslating between modeling frameworks\n\nMethodological explanations\n\nThis mode is informal, exploratory, and never evaluative.\n\n\n10.3.2 Scientific Readiness Evaluation\n\nStructured, rubric-based checks of specific artifacts\n\nExecuted only when explicitly invoked (via menu or trigger phrases)\n\nUsed to assess whether work appears ready for submission, not to grade on scientific quality.\n\n\n\n10.3.3 AI Interaction Logging (To be launched on Monday, 26-Jan-2026!!)\n\nMinimal documentation of AI assistance\n\nTriggered only by the exact command: log this\n\nDesigned for transparency, not surveillance",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#safeguards-and-constraints",
    "href": "chapters/using_jackalope_gpt.html#safeguards-and-constraints",
    "title": "10  Using JackalopeGPT",
    "section": "10.4 Safeguards and Constraints",
    "text": "10.4 Safeguards and Constraints\nJackalopeGPT is intentionally constrained:\n\nNon-evaluative by default (but has the Main Menu to allow evaluation)\nAsks at most one clarifying question at a time\nNever guesses or silently fills gaps\n\nDoes not hallucinate criteria or requirements\nDoes not combine Main Menu actions\nGenerally limits verbosity unless you request detail explicitly\n\nAny evaluation or logging action requires explicit transparency and user consent. As a last note, if you are unsure whether your prompt is appropriate, add this line: “If this prompt is likely to cause me to overreach in inference or bypass learning, suggest a safer, learning-focused version.”",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#menu-commands-and-triggers",
    "href": "chapters/using_jackalope_gpt.html#menu-commands-and-triggers",
    "title": "10  Using JackalopeGPT",
    "section": "10.5 Menu Commands and Triggers",
    "text": "10.5 Menu Commands and Triggers\nTyping main menu (or Main Menu) displays all available structured actions (with no analysis). This means that JackalopeGPT has all the course assignments and rubrics uploaded into its working knowledge.\nIf you enter a menu number or a similar matching instruction, JackalopeGPT will:\n\nconfirm the action\n\nexecute the instruction file exactly as specified\n\nCertain phrases (e.g., assignment-check intent or log this) automatically trigger workflows before menu selection.",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#traffic-signal-rubric-how-evaluations-work",
    "href": "chapters/using_jackalope_gpt.html#traffic-signal-rubric-how-evaluations-work",
    "title": "10  Using JackalopeGPT",
    "section": "10.6 Traffic-Signal Rubric (How Evaluations Work)",
    "text": "10.6 Traffic-Signal Rubric (How Evaluations Work)\nA universal Traffic-Signal rubric is pre-loaded and applied consistently across all readiness checks.\nEvaluations return the follwing ratings of scientific readiness:\n\nGreen: Ready to proceed\n\nYellow: Exercise caution (revisions necessary)\n\nRed: Stop and Rework\n\nThis rubric provides standardized, comparable feedback without grading.",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#how-to-generate-an-evaluation-table-required-for-assignments",
    "href": "chapters/using_jackalope_gpt.html#how-to-generate-an-evaluation-table-required-for-assignments",
    "title": "10  Using JackalopeGPT",
    "section": "10.7 How to Generate an Evaluation Table (Required for Assignments)",
    "text": "10.7 How to Generate an Evaluation Table (Required for Assignments)\nBefore submission, you must evaluate assignments using JackalopeGPT.\nSteps:\n\nSelect the focal assignment from the JackalopeGPT menu\n\nCopy and paste the your completed .qmd file (not rendered output)\n\nUse the model output to revise your file until it meets the assignment standard\n\nPaste the complete Evaluation Table output directly from the JackalopeGPT windo (it is already in .qmd format) into the section at the end of each assignment titled JackalopeGPT Pre-Submission Evaluation.\n\n(Tip: hover over the table until the file icon appears at the upper right, then copy.)",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#important-usage-notes",
    "href": "chapters/using_jackalope_gpt.html#important-usage-notes",
    "title": "10  Using JackalopeGPT",
    "section": "10.8 Important Usage Notes",
    "text": "10.8 Important Usage Notes\n\n10.8.1 Interaction Limits\nIf you are using the free tier (recommended), be aware of interaction limits. Plan longer evaluation workflows accordingly.\n\n\n10.8.2 Memory and Sessions\nJackalopeGPT does not retain memory across sessions.\nIf you start a new session, you must re-provide context about:\n\nyour study system\n\nyour data\n\nyour analytical goals\n\nDo not assume continuity across chats. It is strongly recommended that you keep a file with your updated and optimized prompts so that you can reuse.",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/using_jackalope_gpt.html#a-reminder-about-the-appropriate-use-in-this-course",
    "href": "chapters/using_jackalope_gpt.html#a-reminder-about-the-appropriate-use-in-this-course",
    "title": "10  Using JackalopeGPT",
    "section": "10.9 A reminder about the appropriate use in this course",
    "text": "10.9 A reminder about the appropriate use in this course\n\n10.9.1 You may use JackalopeGPT to:\n\nclarify concepts\n\ndebug R code\n\ncompare modeling approaches (with explanation)\n\nlearn how to validate models\n\nimprove analytical workflow and reproducibility\n\n\n\n10.9.2 You may not use JackalopeGPT to:\n\ngenerate full assignment submissions\n\nwrite end-to-end analyses, methods, or results\n\nreplace your own reasoning\n\noutsource scientific judgment\n\nproduce claims without validation",
    "crumbs": [
      "Part ∅: AI preparation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using JackalopeGPT</span>"
    ]
  },
  {
    "objectID": "chapters/reproducible_workflow_intro.html",
    "href": "chapters/reproducible_workflow_intro.html",
    "title": "11  Reproducible Analyses",
    "section": "",
    "text": "11.1 Why start with reproducible analysis workflows?\nAlthough ZOO/ECOL-5500 focuses on quantitative analysis and modeling, those tools are only as reliable as the infrastructure that supports them. Establishing a clear, reproducible workflow creates a solid foundation that can scale appropriately as new data arrive –whether that means new sensor downloads, ongoing field observations, or ever-expanding datasets– without requiring you to constantly restructure, reinvent, or debug your analysis.\nThis chapter provides the broad conceptual map for the rest of Part I. Subsequent chapters focus on individual components of this workflow.\nEverything up to (and including) production of figures and tables can and should be reproducible. That is, (1) you should be able to rerun an entire analysis and produce tables and figures (and fill in values in a Results section) with a simple click of a button, and (2) another user should be able to reproduce all steps in your analysis pipeline, even with minimal code. Framing the workflow as a casual path makes obvious how each result is causally linked to specific inputs, transformations, and assumptions.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reproducible Analyses</span>"
    ]
  },
  {
    "objectID": "chapters/reproducible_workflow_intro.html#analytical-workflow",
    "href": "chapters/reproducible_workflow_intro.html#analytical-workflow",
    "title": "11  Reproducible Analyses",
    "section": "11.2 Analytical Workflow",
    "text": "11.2 Analytical Workflow\nA statistical philosophy only works if it is supported by a deliberate, reproducible analytical workflow. This workflow description is doing a lot of work, so let us be very clear about the meaning.\n\nAnalytical workflow: The data and computational infrastructure that translates raw observations into scientifically defensible results and inferences.\n\nA workflow include everything about how datasets are stored, cleaned, explored, modeled, and reported. Importantly, it also includes how decisions are documented and how analyses can be reproduced by someone else (including your future, more frenetic self).\nDesigning a good analytical workflow is hardly a casual endeavor. In fact, it should be a target of formal and toughtful analysis itself, answering questions like:\n\nIs your workflow efficient in terms of your valuable time?\nIs your workflow efficient in terms of your computational infrastucture and abilities?\nIs your workflow clear, or does it create confusion and induce mission/scope creep?\nDoes your workflow allow you to stop and restart without losing your place?\n\nA good workflow constrains your behavior in productive ways. It limits wandering, enforces transparency, and makes progress visible. That is, there is no single correct workflow. As Rudyard Kipling wrote (In the Neolithic Age):\n\nHere’s my wisdom for your use, as I learned it when the moose And the reindeer roamed where Paris roars to-night: “There are nine and sixty ways of constructing tribal lays, And—every—single—one—of—them—is—right!\n\nThere are many ways of constructing something useful and meaningful, and every single one of them can be right. What matters is that your workflow is intentional, scientifically defensible, and aligned with your scientific goals. So, I encourage you to start formalizing a philosophical workflow that reduces bias, controls scope, and supports efficient thinking. Then stick to it until you learn something better. That moment will come, and when it does, simply update your philosophy through slow iteration rather than abandoning it.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reproducible Analyses</span>"
    ]
  },
  {
    "objectID": "chapters/reproducible_workflow_intro.html#what-is-a-reproducible-analytical-workflow",
    "href": "chapters/reproducible_workflow_intro.html#what-is-a-reproducible-analytical-workflow",
    "title": "11  Reproducible Analyses",
    "section": "11.3 What is a reproducible analytical workflow?",
    "text": "11.3 What is a reproducible analytical workflow?\nA common misconception—especially early in graduate training—is that reproducibility means sharing files and code on GitHub, Dryad, or similar repositories. Those components are necessary, but they is not sufficient.\nA reproducible analytical workflow is a structured system that explicitly and completely describes how scientific results are generated. Such a workflow:\n\nstarts with observations of the world, not the computer\ntreats measurement as a translation (or signal-transduction) process\nclearly and explicitly documents where data come from\nseparates data, metadata, processing, and inference\nrecords inputs → transformations → outputs at each step (this is key!)\nmakes explicit where assumptions or biases are introduced\nallows another person (or you, six months later) to reconstruct the results\n\nTogether, these features ensure that analyses can be understood, repeated, extended, and scaled as new data arrive.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reproducible Analyses</span>"
    ]
  },
  {
    "objectID": "chapters/reproducible_workflow_intro.html#the-reproducible-analytical-workflow-as-a-causal-system",
    "href": "chapters/reproducible_workflow_intro.html#the-reproducible-analytical-workflow-as-a-causal-system",
    "title": "11  Reproducible Analyses",
    "section": "11.4 The reproducible analytical workflow as a causal system",
    "text": "11.4 The reproducible analytical workflow as a causal system\nThe reproducible analytical workflow can be understood as a causal system in which each step produces downstream consequences: measurement choices shape the raw data, data processing constrains what can be modeled, and modeling assumptions determine what can be inferred. Thinking causally about the workflow makes it clear that results do not simply “come from the data,” but from a chain of decisions that can be traced, tested, and reproduced. Generally, this pipeline looks like this:\n\nWorld → Data → Models → Inference → Interpretation\n\nMore explicitly, as seen in the diagram at right, we have the following general steps:\n\nWorld / phenomena: Processes exist whether or not we observe them (or like them!)\nMeasurement & observation: Instruments and observers translate phenomena into recorded values.\nRaw data + metadata: Raw data must have metadata as context.\nProcessing & derivation: Filtering, organizing, feature extraction, etc.\nModels & inference rules: Assumptions can be documented and coded explicitly.\nInterpretation: Human judgment (which is usually not reproducible, unless codified).",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reproducible Analyses</span>"
    ]
  },
  {
    "objectID": "chapters/reproducible_workflow_intro.html#why-workflows-matter-scientifically",
    "href": "chapters/reproducible_workflow_intro.html#why-workflows-matter-scientifically",
    "title": "11  Reproducible Analyses",
    "section": "11.5 Why workflows matter scientifically",
    "text": "11.5 Why workflows matter scientifically\nReproducibility in a scientific workflow is not primarily about convenience, though it often becomes convenient over time. A convenient –but non-reproducible– workflow focuses on rerunning an analysis quickly, often relying on memory, manual steps, and undocumented choices. That approach may work in the short term, but it does not scale and rarely holds up weeks, months, or years later. A reproducible workflow, by contrast, is designed so that every result can be traced back to its origins, including the data, code, assumptions, and decisions that produced it.\nAt a broader scale, reproducible workflows allow us to:\n\ndiagnose errors\nunderstand sensitivity to assumptions\nreuse data responsibly\nbuild on previous work without re-guessing decisions\nreduce long-term computational and cognitive overhead\n\nIn short:\n\nConvenience: “Can I run this analysis again?”\nReproducibility: “Can I clearly show how these results were produced?”",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reproducible Analyses</span>"
    ]
  },
  {
    "objectID": "chapters/reproducible_workflow_intro.html#how-part-i-reproducible-analyses-is-organized",
    "href": "chapters/reproducible_workflow_intro.html#how-part-i-reproducible-analyses-is-organized",
    "title": "11  Reproducible Analyses",
    "section": "11.6 How Part I: Reproducible Analyses is organized",
    "text": "11.6 How Part I: Reproducible Analyses is organized\nThis Part follows the workflow in stages:\n\nNature of data: What data are, how measurement works, and what raw data means.\nMetadata (low friction): How to document context without drowning in formatting standards.\nReproducible workflows: Why multiple step-by-step scripts are not enough; the need for explicit input–output mapping.\nInference and interpretation: Where assumptions enter and where reproducibility ends.\n\nEach chapter is responsible for a specific region of the workflow.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reproducible Analyses</span>"
    ]
  },
  {
    "objectID": "chapters/reproducible_workflow_intro.html#a-guiding-principle",
    "href": "chapters/reproducible_workflow_intro.html#a-guiding-principle",
    "title": "11  Reproducible Analyses",
    "section": "11.7 A guiding principle",
    "text": "11.7 A guiding principle\n\nIf you cannot say what a result depends on, it is not reproducible.\n\nKeep this sentence in mind as you move forward. The next chapter starts where all workflows begin: with the nature of data themselves.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reproducible Analyses</span>"
    ]
  },
  {
    "objectID": "chapters/reproducible_workflow_intro.html#a-parting-note",
    "href": "chapters/reproducible_workflow_intro.html#a-parting-note",
    "title": "11  Reproducible Analyses",
    "section": "11.8 A parting note",
    "text": "11.8 A parting note\nIn the next section, I take a step back and offer a set of statistical aphorisms that can help us remain vigilane about workflow weaknesses, leaky assumptions, incorrect modeling approach, or cryptic biases.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reproducible Analyses</span>"
    ]
  },
  {
    "objectID": "chapters/nature_of_data.html",
    "href": "chapters/nature_of_data.html",
    "title": "12  The Nature of Data",
    "section": "",
    "text": "12.1 Introduction\nUnderstanding the nature of your data is the first real step toward defensible ecological inference.You were asked to come into this course with a dataset in hand. Some of you have undoubtedly arrived with very well-organized spreadsheets. Others have long lists of folders full of raw sensor outputs, GIS layers, audio files, camera-trap images, or field notes that only you can currently interpret. All of that is perfect for this course.\nBefore we analyze anything, however, we need to be precise about three things:\nMost scientific confusion comes from mixing these three up. If you do not understand these three critical components, no amount of fancy R coding will save your analysis.\nSo, we begin where all data originate: measurement. And, for that, it helps to start with a brief introduction to the field of metrology.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Nature of Data</span>"
    ]
  },
  {
    "objectID": "chapters/nature_of_data.html#introduction",
    "href": "chapters/nature_of_data.html#introduction",
    "title": "12  The Nature of Data",
    "section": "",
    "text": "What measurements represent\n\nWhat data are\n\nWhat we are actually trying to estimate",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Nature of Data</span>"
    ]
  },
  {
    "objectID": "chapters/nature_of_data.html#metrological-terminology",
    "href": "chapters/nature_of_data.html#metrological-terminology",
    "title": "12  The Nature of Data",
    "section": "12.2 Metrological Terminology",
    "text": "12.2 Metrological Terminology\nMetrology is simply the science of measurement. Whereas it has traditionally focused on the physical sciences and engineered systems, it applies just as much to biology and its messier subfields (e.g. ecology) as it does to physics or chemistry.\nMetrology establishes a formal framework for ensuring that scientific measurements are traceable to standards, reported with uncertainty, and reproducible across space, time, and observers. Crucially, this framework operates upstream of analysis—at the moment when information about the world are collected an raw data are generated. Metrology naturally integrates with our discussion of reproducible analyses because, instead of a number standing alone, a measurement comes bundled with information about what it was trying to measure, how reliable it is, and what assumptions were made along the way. That lets future researchers decide whether datasets are truly comparable, whether old data can answer new and exciting questions, and how much trust researchers should place in a result without having to guess what the original researcher meant.\nTwo term must be clearly differentiated from measurements and estimates. These terms are:\n\nmeasurand\nestimand\n\n\n\n\n\n\n\nNoteEtymology of measurand and estimand\n\n\n\n\n\nThe suffix –and comes from Latin –andus\n(“that which is to be acted upon [a target]”)\n\n\n\nTarget (–and)\nResult\n\n\n\n\nMeasurand\nMeasurement\n\n\nEstimand\nEstimate\n\n\n\n\n\n\n\n\n\n12.2.1 Measurand!\n\nThe theoretical quantity of interest in the world to be measured.\n\nA measurand:\n\nexists independently of observers\n\nis defined conceptually, not instrumentally\n\nis the quantity you wish you could measure directly\n\nExamples:\n\ntrue wing length\n\nactual activity level\n\nreal population abundance\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe measurand is defined before measurement and never directly observed.\n\n\n\n\n\n\n\n\n\n\nNoteCan we ever directly achieve the measurand?\n\n\n\nShort answer: Almost never—and that is normal science.\n\n\n\n\n\n\n12.2.2 Estimand! (the missing precision tool)\n\nThe quantity that a statistical model is designed to estimate, given the available data.\n\nThough we will discuss estimands at length later in the course during the modeling exercises, it helps to discuss it in the context of measurands. An estimand:\n\nis defined by both the scientific question and the analysis model\n\ndepends on the data, measurement process, and assumptions\n\nis the quantity you can actually estimate from the data\n\nExamples:\n\nmean wing length in the sampled population\n\nexpected activity level given detection and sampling design\n\nestimated population abundance under a specified model\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe estimand is defined before analysis, depends on modeling choices, and is not the same as the measurand.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Nature of Data</span>"
    ]
  },
  {
    "objectID": "chapters/nature_of_data.html#summary-measurand-vs-estimand",
    "href": "chapters/nature_of_data.html#summary-measurand-vs-estimand",
    "title": "12  The Nature of Data",
    "section": "12.3 Summary: Measurand vs Estimand",
    "text": "12.3 Summary: Measurand vs Estimand\n\n\n\nConcept\nMeasurand\nEstimand\n\n\n\n\nNature\nConceptual / scientific\nFormal / statistical\n\n\nExists without data?\nYes\nNo\n\n\nDepends on model?\nNo\nYes\n\n\nDefined when?\nBefore measurement\nBefore estimation\n\n\n\n\n\n\n\n\n\n\nNoteWhere does each concept belong in a scientific paper?\n\n\n\n\n\n\n\n\nConcept\nSection\n\n\n\n\nMeasurand\nIntroduction\n\n\nEstimand\nMethods\n\n\nEstimate\nResults\n\n\nInterpretation (circling back to measurand)\nDiscussion",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Nature of Data</span>"
    ]
  },
  {
    "objectID": "chapters/nature_of_data.html#what-are-data",
    "href": "chapters/nature_of_data.html#what-are-data",
    "title": "12  The Nature of Data",
    "section": "12.4 What are data?",
    "text": "12.4 What are data?\nOnly after measurement do we get data, the recorded outputs of those measurement processes. To understand what is meant by data, let us consult a dictionary first. Two definitions from the Merriam-Webster Dictionary—188 years apart—are especially revealing:\n\n\n\n\n\n\n\n\nNoteMerriam-Webster (2016)\n\n\n\n“Facts or information used usually to calculate, analyze, or plan something.”\n\n\n\n\n\n\n\n\n\n\n\nNoteMerriam-Webster (1828)\n\n\n\n“Quantities, principles, or facts given or admitted, by which to find things or results unknown.”\n\n\n\n\nModern usage treats data and information as synonymous. But scientifically, this is misleading.\nData are observed facts. Information is what we extract from those facts using models and assumptions.\nWe do not collect information. We collect data, and then we extract or create information. This distinction is critical for reproducibility.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Nature of Data</span>"
    ]
  },
  {
    "objectID": "chapters/nature_of_data.html#raw-data",
    "href": "chapters/nature_of_data.html#raw-data",
    "title": "12  The Nature of Data",
    "section": "12.5 Raw data",
    "text": "12.5 Raw data\nBecause measurands cannot be observed directly and estimands depend on how data are generated and modeled, raw data are the critical link between scientific questions (filled with measurands) and strong inference. Raw data preserve the closest recorded traces of the measurement process, making it possible to understand how observations relate to the underlying measurand and what estimands the data can actually support.\nBy now, we have all been conducting formal scientific inquiry for many years. It is common to see folders labeled /raw_data/ that contain offloaded sensor files or spreadsheets transcribed from field notebooks. These data may feel “raw,” but often they are not.\n\nRaw data are the facts produced by an instrument or observer through a domain-specific translation, recorded at the time of observation, and preserved without post-hoc modification.\n\n\n\nWhen we offload data from a sensor or transcribe notes into a spreadsheet, our actions have a non-zero probability of influencing the data.\nExamples of raw data include:\n\naudio waveforms recorded at a study site (on the device)\n\nGPS locations logged by a collar (not yet downloaded)\n\ncounts written on a datasheet (not yet transcribed)\n\npixel values collected by a camera sensor\n\n\n\n\n\n\n\n\n\n\n\n\nWarningCommon misconceptions about raw data\n\n\n\n\n\n\nRaw data are not clean, tidy, or analysis-ready\n\nProcessed tables and summaries are not raw data\n\nRaw data are not the truth—they are translated observations\n\nRaw data are not limited to numbers\n\nMessiness does not imply poor data quality\n\nEarly processing choices permanently change the data\n\n\n\n\n\nRaw data are the foundation of all scientific analysis. On their own, however, they are incomplete. To be interpretable, reusable, and reproducible, raw data require context.\nThat context comes from metadata, which are introduced on the next page. Keeping the focus here on raw data helps avoid mixing observations of the world with explanations about how those observations were produced. Metadata play a different conceptual role, and treating them separately keeps that distinction clear.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Nature of Data</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html",
    "href": "chapters/metadata_low_friction.html",
    "title": "13  Metadata",
    "section": "",
    "text": "13.2 Why metadata are essential\nMetadata record the information needed to interpret measurements correctly. They tell us:\nMetadata are not bureaucracy. They are scientific memory. Without metadata, data quickly lose meaning—especially once they leave the hands of the person who collected them.\nRaw data without metadata are often difficult—or impossible—to interpret, even by the original researcher. In practice, many irreproducible analyses fail before modeling begins, because key contextual information was never recorded. A column of numbers without units, provenance, or spatial and temporal context is just a sequence of values. It is not reusable scientific data.\nAt a broad level, good metadata allow others (and future you) to:\nMetadata do not guarantee good science, but their absence almost guarantees future confusion.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#what-metadata-are-and-are-not",
    "href": "chapters/metadata_low_friction.html#what-metadata-are-and-are-not",
    "title": "13  Metadata",
    "section": "13.1 What metadata are (and are not)",
    "text": "13.1 What metadata are (and are not)\n\nMetadata are data about data.\n\nThis definition is technically correct, but not very useful on its own. A more practical way to think about metadata is this:\n\nMetadata describe the context in which data were generated.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#why-metadata-are-essential",
    "href": "chapters/metadata_low_friction.html#why-metadata-are-essential",
    "title": "13  Metadata",
    "section": "",
    "text": "understand what the data actually represent, and what their limitations are\n\njudge whether the data are appropriate for a new purpose or analysis\n\nreconstruct and evaluate analytical decisions made downstream",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#metadata-within-a-reproducible-analytical-workflow",
    "href": "chapters/metadata_low_friction.html#metadata-within-a-reproducible-analytical-workflow",
    "title": "13  Metadata",
    "section": "13.3 Metadata within a reproducible analytical workflow",
    "text": "13.3 Metadata within a reproducible analytical workflow\nReproducibility is not just about being able to rerun code indefinitely. It also requires being able to answer core questions about the data themselves, such as:\n\nWhat exactly was measured?\n\nUnder what conditions were the measurements taken?\n\nWhat instruments or protocols were used?\n\nWhat were the data’s limitations or sources of error?\n\nMetadata provide the necessary bridge between raw data and scientific inference. They explain how measurements came to exist and clarify which kinds of estimands the data can —and cannot— support.\nWithin the broader analytical workflow, raw data and metadata together form a complete dataset. Complete datasets can be verified, versioned, shared, and reused. All subsequent processing, modeling, and inference depend on this foundation.\n\nTo emphasize, metadata are not an afterthought; they are part of the data.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#types-of-metadata-what-different-layers-do",
    "href": "chapters/metadata_low_friction.html#types-of-metadata-what-different-layers-do",
    "title": "13  Metadata",
    "section": "13.4 Types of metadata: what different layers do",
    "text": "13.4 Types of metadata: what different layers do\nIn practice, metadata operate at several conceptual layers. Each layer answers a different scientific question and constrains how hypotheses and models can be formulated.\n\nTogether:\nStructural metadata define what exists.\nMeasurement metadata define what it represents.\nContextual metadata define what it can be used to infer.\n\n\n\n\n\n\n\nTipStructural Metadata\n\n\n\n\n\nWhat is in the dataset\nStructural metadata describe the literal structure of the data file: variable names, units, data types, allowed values, and how missing values are encoded. This layer tells a reader (and software) what each column represents and how it should be interpreted at a basic level.\nStructural metadata directly constrain hypotheses and models by determining:\n\nwhat variables exist\n\nwhat scales and transformations are meaningful\n\nwhat distributional assumptions are even plausible\n\n\nStructural metadata define what questions can be expressed with the data at all.\n\n\n\n\n\n\n\n\n\n\nTipMeasurement Metadata\n\n\n\n\n\nHow values were generated\nMeasurement metadata describe the process by which values came into existence: instruments or observers, protocols, calibration or standardization, accuracy, resolution, and the role of human judgment versus sensors.\nThis layer determines whether variables are:\n\ndirect measurements or proxies\n\ncomparable across space, time, or observers\n\nsubject to detection error or measurement error\n\nMeasurement metadata shape hypotheses by clarifying what the variables actually represent in the real world, and they constrain model choice by determining whether additional structure (e.g., detection submodels) is required.\n\nMeasurement metadata define what your variables mean.\n\n\n\n\n\n\n\n\n\n\nTipContextual Metadata\n\n\n\n\n\nWhy the dataset looks the way it does\nContextual metadata describe the study design and constraints that shaped the dataset: sampling units, spatial and temporal scope, missingness mechanisms, access limitations, and known biases.\nThis layer determines:\n\nwhich comparisons are valid\n\nwhether observations are independent or repeated\n\nwhether certain hypotheses are testable at all\n\nContextual metadata are where limitations become explicit and where interpretive boundaries are drawn.\n\nContextual metadata define what inferences the data can support.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#what-good-metadata-enable-fair-in-plain-language",
    "href": "chapters/metadata_low_friction.html#what-good-metadata-enable-fair-in-plain-language",
    "title": "13  Metadata",
    "section": "13.5 What good metadata enable (FAIR, in plain language)",
    "text": "13.5 What good metadata enable (FAIR, in plain language)\nAt their core, the FAIR principles (Wilkinson et al. (2016)) recognize that data are only useful if their context travels with them. Metadata are what make this possible. Much of the modern emphasis on metadata comes from the FAIR principles, which aim to ensure that scientific data are:\n\nFindable: Metadata help other people find that the data exist.\n\nAccessible: Metadata explain how to get the data and what the files mean.\n\nInteroperable: Metadata make it clear how the data are formatted and measured, so they work with other data.\n\nReusable: Metadata explain how the data were collected and what their limits are, so others can use them correctly.\n\nCrucially, FAIR is not about ensuring perfection; it is about increasing the probability of future usability.\n\n\n\n\n\n\nNote\n\n\n\nYou are not expected to memorize the FAIR acronym. What matters most is understanding what FAIR is trying to do (specifically, what it is trying to protect)!",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#a-low-friction-approach-to-metadata",
    "href": "chapters/metadata_low_friction.html#a-low-friction-approach-to-metadata",
    "title": "13  Metadata",
    "section": "13.6 A low-friction approach to metadata",
    "text": "13.6 A low-friction approach to metadata\nYou have probably encountered the long list of formal metadata standards used across scientific disciplines. For this course, you do not need to master any of them, and, in fact, you barely need to know them at all.\nOur approach to metadata is intentionally low friction. That means it fits naturally into you (and others) already work: without extra tools or unnecessary extra steps, and without specialized expertise. The goal is not blind compliance to reporting standards; first and foremost, it is about scientific clarity. We therefore adopt a simple guiding principle:\n\nLet repositories handle standardization. Your job is to record clear, honest, human-readable metadata.\n\nRepositories such as Dataverse, Zenodo, Dryad, GBIF, and institutional archives are designed to translate user-supplied documentation into formal metadata schemas. That translation only works, however, if the essential information about your data exists in the first place. What matters most at this stage is clarity and completeness, not adherence to a specific –and often over-specified– standard.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#what-good-enough-metadata-must-accomplish",
    "href": "chapters/metadata_low_friction.html#what-good-enough-metadata-must-accomplish",
    "title": "13  Metadata",
    "section": "13.7 What “good enough” metadata must accomplish",
    "text": "13.7 What “good enough” metadata must accomplish\nAt a minimum, metadata should allow another scientifically literate person—someone outside your project—to answer the following questions:\n\nWhat are these data?\n\nWho created them?\n\nWhen and where were they collected?\n\nHow were the measurements made?\n\nWhat files belong together, and how?\n\nIf these questions cannot be answered from the metadata alone, the dataset is not reusable—no matter how sophisticated the analysis or modeling may be.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#practical-formats-for-early-stage-metadata",
    "href": "chapters/metadata_low_friction.html#practical-formats-for-early-stage-metadata",
    "title": "13  Metadata",
    "section": "13.8 Practical formats for early-stage metadata",
    "text": "13.8 Practical formats for early-stage metadata\nIn everyday research workflows, metadata often begin life in simple, familiar formats such as:\n\nGoogle Sheet (good for coordinating field or lab work)\n\nplain-text or Markdown file (good for small or exploratory projects)\n\nThese formats are perfectly acceptable as starting points. They lower the barrier to documentation and encourage metadata to be written early in the analysis life-cycle rather than postponed. However, these format are usually best treated as temporary representations. As projects grow, metadata need to become more structured, more explicit, and easier to validate and reuse. But, if you are more comfortable beginning your journey using a version-controlled spreadsheet platform (like Google Sheets), you are welcome to do so.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#why-we-use-yaml",
    "href": "chapters/metadata_low_friction.html#why-we-use-yaml",
    "title": "13  Metadata",
    "section": "13.9 Why we use YAML",
    "text": "13.9 Why we use YAML\nTo support that transition from your mind (or Google Sheets), this course adopts YAML as the canonical format for project-level metadata.\n\n\n\n\n\n\nNoteYAML: What’s in a name?\n\n\n\nOriginally, YAML was an abbreviation for “Yet Another Markup Language”. Later, it become “YAML Ain’t Markup Language”.\n\n\n\n\n\n\n\n\nNoteWhat the heck is canonical in this context?\n\n\n\nCanonical simply means that there is exactly one place where your dataset is formally defined; it does not mean that your metadata exists as exactly one file (though it could).\n\n\nThere are several advantages to using YAML, including:\n\nhuman-readable and easy to edit\n\nstructured and explicit about relationships\n\neasy to version-control\n\nsimple to validate and extend (i.e. Quarto markdown files have YAML headers)\n\nstraightforward to convert into repository-specific schemas later\n\nMost importantly, YAML encourages you to think carefully about what varies, what stays constant, and how different pieces of a dataset relate to one another.\nIn the next section, we will walk through the structure of a well-designed YAML metadata file, using concrete examples and expandable templates to show how common research scenarios —such as multiple instruments, deployments, or sampling rates— can be documented clearly and correctly.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#levels-of-metadata-where-information-belongs-and-where-it-does-not",
    "href": "chapters/metadata_low_friction.html#levels-of-metadata-where-information-belongs-and-where-it-does-not",
    "title": "13  Metadata",
    "section": "13.10 Levels of metadata: where information belongs (and where it does not)",
    "text": "13.10 Levels of metadata: where information belongs (and where it does not)\nWhen you document metadata, your main job is to put information at the right level of scope. This keeps metadata clear, avoids duplication, and prevents the most common failure mode: collapsing variation.\nA useful way to think about this is a hierarchy of levels, from broad context to individual files.\n\n\n\n\n\n\n\nNoteDataset-level metadata (the whole project)\n\n\n\n\n\nThis level answers: What is this data set, broadly, and how should it be cited, discovered, and reused?\nWhat belongs at this level\n\ntitle, description, and keywords\n\ncreators, affiliations, persistent identifiers (e.g., ORCID)\n\noverall spatial and temporal scope (broad bounds, if applicable)\n\nlicense and usage rights\n\nrelated identifiers (publications, code repositories, DOIs)\n\nhigh-level description of how the data were generated\n\nprovenance summary (e.g., “data collection + processing workflow”)\n\nWhat does not belong at this level\n\nfile-specific settings or parameters\n\nvalues that vary across observations or files\n\nderived results (means, medians, model outputs)\n\nvague summaries such as “most values were…”\n\n\n\n\n\n\n\n\n\n\n\nNoteInstrument- or system-level metadata (the measurement system)\n\n\n\n\n\nThis level answers: What system, instrument, or process produced the measurements, and what are its stable properties?\nWhat belongs at this level\n\nsystem or instrument type, manufacturer, and model\n\nserial number, asset ID, or logical identifier\n\nsoftware or firmware version (if relevant)\n\nproperties that are stable across use (e.g., resolution, precision, bit depth)\n\nWhat does not belong at this level\n\nsettings that vary across observations unless explicitly declared as defaults\n\ntime- or location-specific information\n\n\n\n\n\n\n\n\n\n\n\nNoteDeployment- or configuration-level metadata (a consistent setup)\n\n\n\n\n\nThis level answers: When and under what conditions was the system used with a consistent configuration?\nWhat belongs at this level\n\nconfiguration or deployment identifier\n\ncontextual identifiers (e.g., site, batch, experiment, run)\n\nstart and end dates or times\n\nparameters that were constant during this configuration\n\ncalibration notes, protocols, or procedural references\n\nWhat does not belong at this level\n\nindividual file or observation exceptions unless explicitly mapped\n\nvalues that vary within the configuration without structure\n\n\n\n\n\n\n\n\n\n\n\nNoteFile- or observation-level metadata (individual data units)\n\n\n\n\n\nThis level answers: What is true about this specific file, record, or observation?\nWhat belongs at this level\n\nfilename or observation identifier\n\ndate and time of acquisition or creation\n\nparameter values that vary (e.g., sampling rate, resolution, settings)\n\ncontextual attributes that differ across observations\n\nlocation information, if variable\n\nWhat does not belong at this level\n\ndataset-wide descriptions\n\ninterpretive judgments (e.g., “high quality”) unless defined by a controlled scheme\n\n\n\n\n\n\n\n\n\n\n\n\nTipTwo rules that prevent most metadata mistakes\n\n\n\n\n\n\nPlace metadata at the highest level where they are constant.\nIf a sampling rate never changes across a deployment, store it at the deployment or instrument level.\nIf a metadata value varies, do not summarize it; map it explicitly by adding another metadata entry.\nNever write “sampling_rate_hz: 48000” plus “some files differ.” Instead, record which files have which values.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/metadata_low_friction.html#wrapping-this-up",
    "href": "chapters/metadata_low_friction.html#wrapping-this-up",
    "title": "13  Metadata",
    "section": "13.11 Wrapping this up",
    "text": "13.11 Wrapping this up\nClear and clean metadata describe what your data are and how they were generated. Just as importantly, the names you give to files, variables, and folders determine whether that information remains interpretable as projects grow. In the next section, we turn to the topic of naming conventions and coding style —small and seemingly trivial decisions that play an outsized role in scientific reproducibility. Let’s proceed!",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metadata</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html",
    "href": "chapters/naming_conventions.html",
    "title": "14  Conventions and Style",
    "section": "",
    "text": "14.1 Why you should care (even if you “just want to model stuff”)\nNaming conventions and coding style are not decoration. They are infrastructure.\nIf your names are inconsistent, your code becomes harder to read, harder to debug, and harder to share—especially once your project grows beyond “one file, one afternoon.”\nIn scientific workflows, this difference is not about elegance. It is about traceability.\nA workflow that depends on unstable or unclear names cannot be audited, reused, or trusted.\nBefore we talk about modeling or automation, we need to ensure that the basic objects in our workflow—variables, files, and folders—are named in a way that makes their roles unambiguous.\nThis is what allows:",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conventions and Style</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#why-you-should-care-even-if-you-just-want-to-model-stuff",
    "href": "chapters/naming_conventions.html#why-you-should-care-even-if-you-just-want-to-model-stuff",
    "title": "14  Conventions and Style",
    "section": "",
    "text": "“It’s the hardest job to come up with new and unique names for a variable every time you create one but this is the difference between an average programmer and a good one.”\n— Vikram Singh Rawat\n\n\n\n\n\n\n\n\n\nNoteThe three scientific costs of sloppy naming\n\n\n\n\nInteroperability cost:\nNon-standard names break workflows across tools, languages, and collaborators.\nCollaboration cost:\nInconsistent naming forces others (including Future You) to reverse-engineer intent.\nReproducibility cost:\nWhen names drift, inputs and outputs become ambiguous, and results can no longer be traced back to their sources.\n\n\n\n\n\n\nmetadata to point to the correct data,\nscripts to operate predictably,\nand workflows to declare dependencies explicitly.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conventions and Style</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#naming-conventions-for-variables-and-files",
    "href": "chapters/naming_conventions.html#naming-conventions-for-variables-and-files",
    "title": "14  Conventions and Style",
    "section": "14.2 Naming conventions for variables and files",
    "text": "14.2 Naming conventions for variables and files\n\n\n\n14.2.1 What are “naming conventions”?\nThey are simply a consistent set of rules for naming:\n\nvariables (columns in your dataset)\nobjects in R (data frames, models, plots)\nfiles and folders (scripts, data, outputs)\n\n\n\n14.2.2 Three common naming styles\nYou will see these everywhere:\n\ncamelCase (meanDepth)\nPascalCase (MeanDepth)\nsnake_case (mean_depth)\n\nIn this course, we will default to snake_case for variables and files. This is strongly recommended for analysis scripts.\n\n\n\n\n\nFrom Allison Horst. Cartoon representations of common cases in coding. A snake screams “SCREAMING_SNAKE_CASE” into the face of a camel (wearing ear muffs) with “camelCase” written along its back. Vegetables on a skewer spell out “kebab-case” (words on a skewer). A mellow, happy looking snake has text “snake_case” along it.\n\n\n\n\n\n14.2.3 General guidelines (the “please don’t make future you angry” list)\nThese are simple rules that prevent a shocking number of problems:\n\n\n\n\n\n\n\n\n\nRule\nWhy it matters\nExample of poor form\n\n\n\n\nAvoid blank spaces\nSpaces complicate coding and break formulas, file paths, and scripting\nMean Depth, Site Name, Final Data.csv\n\n\nOmit special symbols like ?, $, *, +, #, (, ), -, /, }, {, |, &gt;, &lt;, etc.\nMany special characters are treated as operators or commands in R, so using them in names can break code or make it harder to work with.\nmass(g), count#, Y/N?, depth&gt;10\n\n\nUse _ as a separator\nUnderscores are safe, readable, and standard\nmean-depth, mean.depth, mean depth\n\n\nDo not begin names with numbers\nNames starting with numbers are invalid or confusing in R\n100m_depth, 2023_data\n\n\nMake names unique\nNon-unique names cause overwriting and ambiguity\ndata, data2, final, final_final\n\n\nBe consistent with case\nR is case-sensitive; inconsistency causes silent bugs\nDepth, depth, DEPTH in same project\n\n\nAvoid blank rows in data\nBlank rows can be misread as data or missing values\nEmpty rows between observations in CSV\n\n\nRemove comments from data files\nComments belong in scripts or metadata, not raw data\n# measured in June inside a CSV\n\n\nDefine and document NA values\nAmbiguous missing values lead to incorrect analyses\nMixing NA, 0, -999, . without explanation\n\n\nUse clear, documented date formats\nAmbiguous dates are easy to misinterpret\n03/04/21 (is this March 4 or April 3?)\n\n\n\n\n\n\n14.2.4 “Bad → Good” examples for variable names\nYour variable names should be: - readable at a glance - easy to type - consistent across files and projects\nHere are examples aligned with the tidyverse style guide (Hadley Wickham):\nA quick translation rule:\n\nUse units as suffixes: depth_cm, mass_g, distance_m (do not use parentheses around units)\nUse clear boolean names: yes_no or better: is_adult, has_nest\nPrefer meaning over brevity: percentile_50 beats p50 (unless p50 is a defined term in your field)",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conventions and Style</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#naming-conventions-for-folders-project-structure",
    "href": "chapters/naming_conventions.html#naming-conventions-for-folders-project-structure",
    "title": "14  Conventions and Style",
    "section": "14.3 Naming conventions for folders (project structure)",
    "text": "14.3 Naming conventions for folders (project structure)\n\n14.3.1 Why folder names matter\nA good folder structure makes it hard to lose track of: - raw versus processed data - inputs versus outputs - code versus results - files that you can re-create versus ones you cannot (i.e. that you should never overwrite)\nIn this course, we care about reproducibility, which means: - raw data is always treated as read-only - processed data is created by scripts, acting on raw data, and never manually edited - results are generated by scripts and never manually edited\n\n\n\n\n\n\nTipA folder structure that scales\n\n\n\n\n\nA simple, stable structure (example):\n\ndata/\n\nraw_data/ (never edited)\nprocessed_data/ (created by scripts)\n\nr_scripts/ (functions + helpers)\nanalysis/ (analysis note / exercises )\noutputs/\n\nfigures/\ntables/\n\n\n\n\n\nFolder naming rules (same spirit as variable naming): - lowercase (snake_case) - no spaces - prefer underscores - descriptive names over clever names",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conventions and Style</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#conventions-for-legible-and-consistent-r-code",
    "href": "chapters/naming_conventions.html#conventions-for-legible-and-consistent-r-code",
    "title": "14  Conventions and Style",
    "section": "14.4 Conventions for legible and consistent R code",
    "text": "14.4 Conventions for legible and consistent R code\n\n14.4.1 Why code style matters\nCode style is not about being fancy. It’s about making your thinking legible.\nGood style: - reduces bugs - improves peer review (including lab mates and future you) - makes it easier to modify models without breaking things\n\n\n\n\n\n\nNoteTwo extremely useful packages for checking R code formatting\n\n\n\n\nlintr checks your code for style problems (like a spellcheck for R code).\nstyler automatically formats your code to match a consistent style.\n\n\n\n\n\n14.4.2 “Bad → Good” examples for R code and files\nThese examples follow the tidyverse style guide (Hadley Wickham):\n\n\n\n\n\n\n\n\n\nBad\nWhy was it bad?\nGood\n\n\n\n\nFit models.r\nContains spaces; inconsistent casing; harder to reference in code\nfit_models.R\n\n\n##########\nNo semantic meaning; comments should describe intent, not fill space\n# extract elevation -----\n\n\nfindat\nVague name; gives no clue about content or structure\nfindat_mat\n\n\nfindat_mat\nRedundant and unclear; does not describe what the matrix contains\nmass_mat\n\n\n\n\nA few rules we will use repeatedly:\n\n\n14.4.3 Use nouns for objects and verbs for functions\n\nObjects (things): pufferi, pufferi_clean, model_gam\nFunctions (actions): clean_data(), fit_model(), plot_effects()\n\n\n\n14.4.4 Make “intermediate objects” obvious\nIf an object is temporary, label it like it is: - *_raw, *_clean, *_long, *_wide, *_summary\n\n\n14.4.5 Prefer readable code over clever code\n\n\n\n\n\n\n\n\nNoteOver-piped, confusing code\n\n\n\n\n\nsummary_df &lt;-\n  raw_df |&gt;\n  dplyr::filter(!is.na(count), year &gt;= 2015) |&gt;\n  dplyr::group_by(site, species) |&gt;\n  dplyr::summarise(\n    mean_count = mean(count),\n    sd_count   = sd(count),\n    n          = dplyr::n(),\n    .groups = \"drop\"\n  ) |&gt;\n  dplyr::filter(n &gt;= 5) |&gt;\n  dplyr::mutate(\n    cv = sd_count / mean_count,\n    log_mean = log(mean_count)\n  ) |&gt;\n  dplyr::arrange(dplyr::desc(log_mean))\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLonger but readable code\n\n\n\n\n\n# Step 1: keep valid observations from recent years\ndf_filtered &lt;- raw_df |&gt;\n  dplyr::filter(!is.na(count), year &gt;= 2015)\n\n# Step 2: summarize counts by site and species\ndf_summary &lt;- df_filtered |&gt;\n  dplyr::group_by(site, species) |&gt;\n  dplyr::summarise(\n    mean_count = mean(count),\n    sd_count   = sd(count),\n    n          = dplyr::n(),\n    .groups = \"drop\"\n  )\n\n# Step 3: retain well-sampled groups\ndf_sufficient &lt;- df_summary |&gt;\n  dplyr::filter(n &gt;= 5)\n\n# Step 4: compute derived metrics\ndf_metrics &lt;- df_sufficient |&gt;\n  dplyr::mutate(\n    cv       = sd_count / mean_count,\n    log_mean = log(mean_count)\n  )\n\n# Step 5: order results for inspection\nsummary_df &lt;- df_metrics |&gt;\n  dplyr::arrange(dplyr::desc(log_mean))\n\n\n\n\n\n\n\n14.4.6 A final note about reproducible analyses\nIn workflow tools like targets, names are not simply cosmetic. They define nodes in a dependency graph (i.e. the ways all your inputs, processes, and outputs are connected). If names are unclear, the graph is unclear. If the graph is unclear, the workflow is not reproducible.\n\n\n\n\n\n\nTipPractical guidance\n\n\n\n\nUse CSV/TSV when the dataset is small enough to load quickly and inspect easily.\nConsider .RDS when you want an R-native format that preserves object structure.\nConsider .parquet when files get big and you care about speed + file size.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conventions and Style</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#your-minimum-standard-checklist-use-this-before-you-submit-anything",
    "href": "chapters/naming_conventions.html#your-minimum-standard-checklist-use-this-before-you-submit-anything",
    "title": "14  Conventions and Style",
    "section": "14.5 Your “minimum standard” checklist (use this before you submit anything)",
    "text": "14.5 Your “minimum standard” checklist (use this before you submit anything)\n\n\n\n\n\n\nNoteNaming + style checklist\n\n\n\nData + variables\n\nColumn names are snake_case\nNo spaces or special characters in column names\nUnits are explicit where relevant (_m, _cm, _g, _s)\nMissing values are documented (what is NA, what is 0, what is “not observed”?)\nDates are in a clear, documented format (and the order is not ambiguous)\n\nFolders + files\n\nFolder names are lowercase and descriptive\nRaw data are not edited in place\nProcessed data are created by scripts (not by hand)\n\nR code\n\nObjects use nouns; functions use verbs\nScripts are readable (spacing, line breaks, consistent naming)\nYou can restart R and run the workflow without relying on “things in memory”",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conventions and Style</span>"
    ]
  },
  {
    "objectID": "chapters/naming_conventions.html#practice-task-recommended",
    "href": "chapters/naming_conventions.html#practice-task-recommended",
    "title": "14  Conventions and Style",
    "section": "14.6 Practice task (recommended)",
    "text": "14.6 Practice task (recommended)\n\nPick one of your current projects (or a past project).\nRename:\n\n5 confusing variables\n3 confusing files\n1 confusing folder\n\nMake one small style improvement:\n\nconsistent naming\nreadable comments\nbreak one long chunk into 2–3 meaningful steps\n\n\nThe goal is not perfection. The goal is to start practicing now.",
    "crumbs": [
      "Part I: Reproducible Analyses",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conventions and Style</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_intro.html",
    "href": "chapters/data_exploration_intro.html",
    "title": "15  Data Exploration",
    "section": "",
    "text": "15.1 Introduction\nConsider the following scenario. Being an expert in jackalope analytics, you are asked (by some really awesome colleagues) to analyze a dataset that you have never seen. You meet with your colleagues; they describe the data, their goals, and their timeline; you send a few appropriate follow-up questions. They answer. All seems good. You jump right into analysis. Two weeks (!) into a complex workflow, you discover two small details they “forgot” to mention: (1) half the data are questionable because, unbeknowst to them, a jackalope infiltrated the field crew and collected data for a month, and (2) several samples were contaminated with jackalope saliva.\nBecause you skipped the critical data exploration step, you just injected a suite of bad assumptions into everything downstream. The lesson is oddly simple: take your time and be systematic in data exploration. In this course, the goal is not to rush into running statistical models or jump to drawing strong inference, but rather to catch problems early – before they derail your modeling. Even if you collected the data yourself and feel like you know them inside and out, do not assume that you can or should bypass this critical step. Data Demons have great timing and even greater motivation to see you fail, especially near the end of a project or a degree.\nThis week, we purposefully slow the workflow down and practice exploring datasets so that problems like these show up early –long before they shape your model decisions or waste weeks of work. In the next pages, we will walk through core data exploration steps that should always come before formal modeling.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_intro.html#introduction",
    "href": "chapters/data_exploration_intro.html#introduction",
    "title": "15  Data Exploration",
    "section": "",
    "text": "In simple terms, what is data exploration? Data exploration is where you learn what your data can say — before you ask them (not tell them) to say it.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_intro.html#goals-for-this-section-week-3",
    "href": "chapters/data_exploration_intro.html#goals-for-this-section-week-3",
    "title": "15  Data Exploration",
    "section": "15.2 Goals for this section (Week 3)",
    "text": "15.2 Goals for this section (Week 3)\nBy the end of this week, you should be able to:\n\nDescribe some of the advantages and disadvantages of several types of data exploration approaches (before and after modeling)\nExamine your own dataset for outliers\nJustify the removal of outliers\nRealize the importance of taking your good ole’ sweet time with data exploration",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_intro.html#relevant-readings-related-to-data-exploration",
    "href": "chapters/data_exploration_intro.html#relevant-readings-related-to-data-exploration",
    "title": "15  Data Exploration",
    "section": "15.3 Relevant readings related to data exploration",
    "text": "15.3 Relevant readings related to data exploration\nThese are not required readings. These are simply some useful references for you.\n\nO’Hara & Kotze (2010) on why you should not log transform count data.\nBeltran & Tarwater (2024) on why you should not categorize some data.\nZuur et al. (2010) on some of the primary approaches for data exploration (what this page is based on)",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_intro.html#core-benefits-of-data-exploration",
    "href": "chapters/data_exploration_intro.html#core-benefits-of-data-exploration",
    "title": "15  Data Exploration",
    "section": "15.4 Core Benefits of Data Exploration",
    "text": "15.4 Core Benefits of Data Exploration\nLet us delve into the core advantages of a formal data exploration phase. Data exploration determines whether your data, assumptions, and planned analysis are scientifically defensible. Each section below represents a distinct utility of the exploration phase.\n\n\n\n\n\n\nTipReality-checking your mental model\n\n\n\n\n\nData exploration is where your expectations collide with the data you actually collected.\nAt this stage, you evaluate whether:\n\nThe data look the way you expected them to\nVariables behave in ways consistent with how they were measured\nAny patterns or anomalies that contradict your initial assumptions\n\nThis is where naïve assumptions die early –before they become permanent within your chosen modeling framework.\n\n\n\n\n\n\n\n\n\nTipAuditing measurement quality (not “finding patterns”)\n\n\n\n\n\nExploration is a measurement audit. It is not a fishing expedition to find interesting aspects of your data. Data exploration (ante hoc) lets you assess whether the numbers themselves are credible by asking three questions:\n\nAre there signs of systematic bias (e.g. floor/ceiling effects, truncation, observer effects)?\nHow much noise or variability is present (e.g. precision, repeatability)?\nDo any values indicate measurement failure/error rather than biological data-generating processes?\n\nThe central question at this stage is simple: Do these numbers deserve to be modeled?\n\n\n\n\n\n\n\n\n\nTipTurning metadata into something testable\n\n\n\n\n\nMetadata is not documentation-after-the-fact; it becomes operational during exploration. This is where you actively test assumptions such as:\n\nWhat does one row of the dataset actually represent?\nDoes the nested structure of the data (e.g. number of individuals in each of many sites)\nDo temporal and spatial units align with the research question?\n\nExploration exposes missing or contradictory metadata immediately.\n\n\n\n\n\n\n\n\n\nTipInforming how you design your models\n\n\n\n\n\nData exploration directly constrains your modeling options. It helps you determine:\n\nWhich covariates are redundant (e.g., PCA as a redundancy diagnostic)\nWhich transformations (e.g. scaling) are scientifically defensible\nWhich model families are plausible (and which are not)\n\nData exploration helps decide how the analysis should be done, so it cannot be skipped.\n\n\n\n\n\n\n\n\n\nTipBuilding scientific humility\n\n\n\n\n\nIn this course, data exploration is where you learn a few hard but important lessons:\n\nData are not perfect reflections of reality; they are messy results of how measurements were made\nFeeling confident that you know your data does not mean you are right\nRunning models without exploring your data first amounts to guesswork.\n\nData exploration is best understood as friendly peer review before you waste two weeks of your life.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_intro.html#summary-what-data-exploration-is-and-is-not",
    "href": "chapters/data_exploration_intro.html#summary-what-data-exploration-is-and-is-not",
    "title": "15  Data Exploration",
    "section": "15.5 Summary: What data exploration is and is not",
    "text": "15.5 Summary: What data exploration is and is not\nWhat is data exploration used for?\nData exploration exists to decide whether your data are fit for an analysis and whether that analysis is defensible. Use this phase to ask:\n\nDo these data behave like they were measured the way I think they were?\nAre there obvious data biases or limits to precision or accuracy?\nDo variables mean what their names suggest (i.e. following good nomenclature rules)?\nDo groups, measurement units, and temporal and spatial scales make scientific sense?\nDo any of the data already cause model failure?\n\nIf you cannot clearly explain what you learned during data exploration, you are not ready to move on to the data modeling phase.\n\nWhat Data Exploration is not\nData exploration should not be used for:\n\nFishing for significant results (i.e. omitting extreme values to change your model results)\nGenerating pretty plots that do not reflect model results\nRunning PCA because there are many variables\nDeleting outliers without understanding why they exist (e.g. not caring about data-generating processes)\nA formality you rush through to get to “the real analysis”\n\nIf exploration feels boring or unnecessary, you are either skipping it or not doing it well.\n\nIn the next section, we go over some approaches of a standard data exploration toolkit that will allow you understand the structure and limits of your dataset.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_toolkit.html",
    "href": "chapters/data_exploration_toolkit.html",
    "title": "16  Data Exploration Toolkit",
    "section": "",
    "text": "16.1 Overview",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Exploration Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_toolkit.html#overview",
    "href": "chapters/data_exploration_toolkit.html#overview",
    "title": "16  Data Exploration Toolkit",
    "section": "",
    "text": "Data exploration is not a single step to complete before analysis; it consists of a set of diagnostic tools applied iteratively across an analytical workflow. In this course, we distinguish –using very non-standard but arguably useful nomenclature– between two complementary phases of data exploration:\n\nAnte hoc data exploration: diagnostic checks performed before formal modeling, used to assess data structure, measurement quality, and basic data and model assumptions.\nPost hoc data exploration: diagnostic checks performed after initial models are fit and used to reassess data in light of model behavior (e.g., leverage, influence, residual structure, convergence failures, factor reduction, or model respecification).\n\nBoth phases are essential. Ante hoc exploration helps prevent obvious problems from being baked into models, while post hoc exploration helps identify more subtle issues that only become visible after a model has been fit.\nThis section introduces a coarse yet practical ante hoc Data Exploration Toolkit, a subset of exploratory tools that should always be considered before formal modeling begins. Post hoc diagnostics are introduced later in the course within the context of formal data modeling.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Exploration Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_toolkit.html#the-ante-hoc-data-exploration-toolkit",
    "href": "chapters/data_exploration_toolkit.html#the-ante-hoc-data-exploration-toolkit",
    "title": "16  Data Exploration Toolkit",
    "section": "16.2 The ante hoc Data Exploration Toolkit",
    "text": "16.2 The ante hoc Data Exploration Toolkit\nIn the R exercises used this course, we will repeatedly apply the following steps of the ante hoc Data Exploration Toolkit:\n\nTool #1: Assess potential outliers (univariate and multivariate)\nTool #2: Assess the presence of excess zeros\nTool #3: Explore interactions and structural relationships\nTool #4: Assess potential information redundancy among covariates (e.g., multicollinearity)\nTool #5: Decide whether covariate standardization is warranted (this could also be used during post hoc data exploration)\n\nThis toolkit is intentionally coarse and non-exhaustive. Its purpose is not to fix the data, but rather to identify critical and fatal issues when they are easiest to diagnose and least costly to address. Skipping these steps often leads to confusion or Paralysis of Analysis during modeling. Many of these are mistakes I have made myself over the years…and learned from the hard way.\nThis section focuses on the first three most important tools within this toolkit: assessing potential outliers, assessing potential excess of zero values, and assessing potential information redundancy among covariates. The first section –outliers– will be discussed on its own; for the sake of course timing, the other two will be discussed in the tutorial and later in the course.\n\n\n\n\n\n\nTipGLMMs and GAMMs: why homogeneity of variance is not required\n\n\n\n\n\nMost ecological data are far more complex than what simple Analysis of Variance (ANOVA) or linear model frameworks can handle, often involving non-Gaussian dependent variables, hierarchical structures (i.e. nested data), and uneven sampling. In this context, enforcing homogeneity of variance in the classical sense is not required; in face, it is appropriate.\nIn Generalized Linear Mixed Model (GLMMs) and Generalized Additive Mixed Models (GAMMs), variance is explicitly modeled through the choice of error distribution, link function, and hierarchical structure. As a result, heterogeneity of variance on the response scale is not a violation of assumptions; it is often an expected property of the data.\n\nTakeaway: In GLMMs and GAMMs, variance diagnostics evaluate model adequacy, not whether of not the data meet the homoscedasticity assumption a priori. In other words, you can skip that irrelevant section of Zuur et al. (2010).",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Exploration Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/data_exploration_toolkit.html#scope-and-next-steps",
    "href": "chapters/data_exploration_toolkit.html#scope-and-next-steps",
    "title": "16  Data Exploration Toolkit",
    "section": "16.3 Scope and Next Steps",
    "text": "16.3 Scope and Next Steps\nAt this ante hoc stage, the goal is to recognize potential problems, not to resolve them.\nIn the next sections, we tackle each one of the tools in the toolkit. After focusing on each of these, in turn, we present a hands-on tutorial (discussed in class) that walks through these diagnostics using real data, illustrating how rule-based flags arise and how they should be interpreted responsibly within an analytical workflow.\nAs a reminder, post hoc data exploration will be introduced later in the course, once models are fitted and can provide additional information about how the data and model interact.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Exploration Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/outliers.html",
    "href": "chapters/outliers.html",
    "title": "17  Tool 1: Outliers",
    "section": "",
    "text": "17.1 Tool #1: Assessing Potential Outliers\nThis section introduces the first core tool of ante hoc Data Exploration Toolkit: assessing potential outliers. The goal is not to justify a data point’s immediate removal, but rather to flag and understand why an observation is unusual before any modeling decisions are made. Throughout this section, outliers are treated as diagnostic signals rather than problems to be fixed,following a simple guiding principle:",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tool 1: Outliers</span>"
    ]
  },
  {
    "objectID": "chapters/outliers.html#tool-1-assessing-potential-outliers",
    "href": "chapters/outliers.html#tool-1-assessing-potential-outliers",
    "title": "17  Tool 1: Outliers",
    "section": "",
    "text": "Outlier rules guide scrutiny; they do not justify removal.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tool 1: Outliers</span>"
    ]
  },
  {
    "objectID": "chapters/outliers.html#what-is-an-outlier",
    "href": "chapters/outliers.html#what-is-an-outlier",
    "title": "17  Tool 1: Outliers",
    "section": "17.2 What is an outlier?",
    "text": "17.2 What is an outlier?\nA commonly cited definition is:\n\n“…an outlier is an observation that lies outside the overall pattern of a distribution.” (Moore & McCabe, 1999)\n\nThis definition is intentionally broad and inherently univariate. It tells us where an observation sits relative to others—but not why it is unusual, nor what should be done about it. Perhaps a more operational –but more vague– definition is that of Zuur et al. (2010):\n\n“…an observation that has a relatively large or small value compared to the majority of observations”",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tool 1: Outliers</span>"
    ]
  },
  {
    "objectID": "chapters/outliers.html#what-causes-outliers",
    "href": "chapters/outliers.html#what-causes-outliers",
    "title": "17  Tool 1: Outliers",
    "section": "17.3 What causes outliers?",
    "text": "17.3 What causes outliers?\nOutliers arise for many reasons (any number of data-generating processes), including, but not limited to:\n\nextreme relative to distributional assumptions\n\nmeasurement error\nerror during data entry\nextremely rare but biologically plausible (so called Black Swan events)",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tool 1: Outliers</span>"
    ]
  },
  {
    "objectID": "chapters/outliers.html#types-of-outliers",
    "href": "chapters/outliers.html#types-of-outliers",
    "title": "17  Tool 1: Outliers",
    "section": "17.4 Types of outliers",
    "text": "17.4 Types of outliers\nA useful first distinction to make in our discussion of outlier detection is whether an observation is unusual:\n\nIn a single variable (univariate outlier), or\n\nOnly when variables are considered jointly (multivariate outlier)\n\nThese are conceptually different problems and require different tools.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tool 1: Outliers</span>"
    ]
  },
  {
    "objectID": "chapters/outliers.html#univariate-outliers",
    "href": "chapters/outliers.html#univariate-outliers",
    "title": "17  Tool 1: Outliers",
    "section": "17.5 Univariate Outliers",
    "text": "17.5 Univariate Outliers\nUnivariate outliers occur along a single dimension (i.e. in a single variable). For example, a body size measurement that is much larger than most others. Such univariate outliers should never be removed automatically. Their role in ante hoc exploration is to flag observations that deserve closer inspection. So, how can we identify univariate outliers? Univariate outliers can be flagged using two approaches:\n\nStatistical rules: rules depend on assumptions and sample size\n\nExpert judgment: assessment that relies on domain knowledge\n\nBoth approaches have advantages and limitations. Statistical rules can be fragile with small samples, while expert judgment can be applied inconsistently over time and across contexts.\n\n17.5.1 Graphical approaches to univariate outliers\nBefore modeling, graphical inspection provides an extremely valuable first pass at outlier detection. All of these methods are subjective but effective for identifying values that warrant extra scrutiny. Because these myriad approaches essentially allow you to see the same dimensions in your dataset from slightly different perspective, we do not go all of the graphing types. Here are a few useful graphing types for your reference:\n\nCleveland dot plot\nStrip plot\nGeneralized density plot\nHeat map\na myriad others\n\n\n\n17.5.2 Tukey Box Plots\nTukey-style box plots are based on quantiles, not distributional assumptions. Traditional Tukey-style boxplots, which you will find in several R packages, have several core features:\n\nThe interquartile range (IQR), or the difference between the 75% precentile (Q3) and the 25% percentile (Q1):\n\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\n\nWhiskers extending to \\(1.5 \\times \\text{IQR}\\)\nPoints beyond this threshold flagged as outliers\n\nPoints beyond \\(3 \\times \\text{IQR}\\) sometimes flagged as extreme outliers\n\nThese cutoffs are conventional and not inferential. They provide a rule of thumb; they do not provide a statistical test. (You should see a theme: use outlier tests to flag values for extra scrutiny.)\n::: {callout-note collapse=true icon=false} ## Tukey-style boxplots and non-normal data: do not worry! Tukey-style boxplot whiskers are nonparametric; that is, they rely on quantiles (IQR), which are order statistics, and not distributional assumptions. Because outlier flagging at the lower bound is based on ranks, departures from normality are acceptable. :::\n\n\n\n\n17.5.3 Conditional Boxplots\nConditional boxplots extend the univariate, Tukey-style boxplot by stratifying the data by a grouping factor (e.g., site, treatment, time). These plots are especially useful for diagnosing heterogeneity across groups and identifying where flagged values originate. At first, the simplicity of these plots may give the impression that they are not useful, but you will quickly find that piecing together many of these visualizations can help uncover subtle issues with your data.\n\n\n\n\n\n\n\nConditional boxplot allowing the user to visualize potential extreme values within specific groups.\n\n\n\n\n\n\n\n17.5.4 Warnings about detecting univariate outliers from visual inspection\nRules of outlier detection must be interpreted very cautiously, especially if univariate datesets exhibit small sample sizes or skewed (i.e. non-normal) distributions. For example, data drawn from a Gamma distribution—bounded at zero and right-skewed—may naturally produce values flagged by Tukey’s rule even when no anomaly is present.\nUnivariate screening should therefore be viewed as a question-generating step, not an answer-generating one.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tool 1: Outliers</span>"
    ]
  },
  {
    "objectID": "chapters/outliers.html#multivariate-outliers",
    "href": "chapters/outliers.html#multivariate-outliers",
    "title": "17  Tool 1: Outliers",
    "section": "17.6 Multivariate Outliers",
    "text": "17.6 Multivariate Outliers\nMultivariate outliers occur when an observation is unusual in combination across variables, even if it appears unremarkable in any single variable. These are common in real datasets and often more consequential than univariate outliers. Because they depend on covariance structure (how one variable relates to another), multivariate outliers are invisible to univariate screening methods.\nConsider this example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Tukey-style boxplot did not flag outliers in either of the univariate dimensions. But, given the bivariate scatterplot at right, there is clearly a single value that seems to fall outside of the joint data distribution. This is why we need additional tools to help us flag potentially problematic values when dealing with multivariate data (which is probably the norm).",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tool 1: Outliers</span>"
    ]
  },
  {
    "objectID": "chapters/outliers.html#mahalanobis-distance-identifying-multivariate-outliers",
    "href": "chapters/outliers.html#mahalanobis-distance-identifying-multivariate-outliers",
    "title": "17  Tool 1: Outliers",
    "section": "17.7 Mahalanobis Distance: Identifying Multivariate Outliers",
    "text": "17.7 Mahalanobis Distance: Identifying Multivariate Outliers\nMost ecological and biological datasets are inherently multivariate. A point may fall near the center of each marginal distribution yet be extreme in joint space. One tool that we can use is Mahalanobis Distance, which is a standard tool for identifying multivariate outliers because it accounts for covariance among variables. Conceptually, it measures distance from the multivariate centroid.\n\n\n\n\n\n\nNoteWhat the Mahalanobis distance means (plain language)\n\n\n\nThe Mahalanobis distance tells us how unusual a data point is when we consider several variables together.\n\\[\nD^2 = (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\n\\]\nThink of it this way:\n\n\\(x\\) = the values for one observation (e.g., oxygen and temperature at one site)\n\n\\(\\mu\\) = the average values across all observations\n\n\\((x - \\mu)\\) = how far that observation is from the “typical” point\n\n\\(\\Sigma^{-1}\\) = a correction that accounts for different units and correlations between variables\n\n\\(D^2\\) = a single number summarizing how extreme the observation is given the relationships in the data\n\nA point can look reasonable on its own but still have a large Mahalanobis distance if it breaks the usual relationship between variables.\nThat’s why Mahalanobis distance is useful for finding multivariate (conditional) outliers, not just extreme values.\n\n\nIn practice, this can be implemented using existing R packages (e.g., stats::mahalanobis). For the dataset plotted above, the code here correctly flags the odd data point (complete, reproducible example):\n\nlibrary(tidyverse)\n\nset.seed(123)\nn &lt;- 120\n\nxy &lt;- tibble(\n  x = runif(n, min = 5.5, max = 8.5)\n) %&gt;%\n  mutate(\n    y = 14 * x - 20 + rnorm(n(), mean = 0, sd = 1)\n  ) %&gt;%\n  add_row(x = 7.0, y = 95.0) %&gt;%\n  mutate(is_conditional_outlier = row_number() == n())\n\n# --- Mahalanobis distance (stats::mahalanobis) ---\nX &lt;- as.matrix(xy %&gt;% select(x, y))\nmd &lt;- stats::mahalanobis(X, center = colMeans(X), cov = stats::cov(X))\n\n# Cutoff: chi-square quantile with df = 2 (x and y)\ncut &lt;- stats::qchisq(0.975, df = 2)\n\nxy &lt;- xy %&gt;%\n  mutate(\n    md = md,\n    md_outlier = md &gt; cut\n  )\n\n# Which rows are flagged?\nxy %&gt;% filter(md_outlier) %&gt;% select(x, y, md, md_outlier, is_conditional_outlier)\n\n# A tibble: 1 × 5\n      x     y    md md_outlier is_conditional_outlier\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;                 \n1     7    95  86.9 TRUE       TRUE                  \n\n\nAs with univariate methods, flagged points should prompt investigation—not automatic exclusion.\n\nIn the next section, we will apply the ante hoc Data Exploration Toolki to a real dataset. This example will showcase outlier detection but will also demonstrate how to apply some of the other tools in this Toolkit.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tool 1: Outliers</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_data_exploration.html",
    "href": "chapters/tutorial_data_exploration.html",
    "title": "18  Tutorial: Data Exploration",
    "section": "",
    "text": "18.1 Tutorial: Data Exploration\nIn the tutorial below, you will witness some common approaches to examine datasets for outliers and multicollinearity. Some of these have severe limitations, which will be explained. The primary takeaway from this initial approach is that there is not a single, optimal recipe for understanding all the issues with your data prior to analysis. The process of discovery is iterative across several parts of your workflow. So, just be patient with this first step.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Tutorial: Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_data_exploration.html#tutorial-data-exploration",
    "href": "chapters/tutorial_data_exploration.html#tutorial-data-exploration",
    "title": "18  Tutorial: Data Exploration",
    "section": "",
    "text": "NoteWhat to expect from future tutorials\n\n\n\n\n\nIn future weeks, you will be given a similar tutorial (with a very simple worked example) to work through on your own prior to class. In-class sessions will be used to answer questions pertaining to (1) the exercise itself and (2) the application of the tools and approaches to your own dataset.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Tutorial: Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_data_exploration.html#workspace-set-up",
    "href": "chapters/tutorial_data_exploration.html#workspace-set-up",
    "title": "18  Tutorial: Data Exploration",
    "section": "18.2 Workspace set-up",
    "text": "18.2 Workspace set-up\nYou are free to organize your R Project folder in whatever way works best for you, as this course imposes very little file-structure overhead. One simple option is to create a subfolder such as data/class_exercise_data (using the dir.create(\"data/class_exercise_data\") command to hold any example files used in class. If you need a refresher on project setup and organization, refer back to the Preparing Yourself section from Week 1.\nFirst, load the required packages by sourcing the file below. There a number of ways to do this (I have noticed several ways that different labs at UW do this, and that is more than acceptable). For your convenience, I have provided the following file (click to save; you may need to add the .r extension manually):\n\nload_packages.r\n\nPlace this file into a folder called class_r_scripts (just a recommendation!).\n\nsource(\"class_r_scripts/load_packages.r\")\n\nYou should include this sourcing step at the beginning of all independent R scripts. The exception is if and when you use a formal reproducible analysis workflow packages (e.g. the targets package) or you need Quarto, which requires that you load packages and their dependencies in another way. This will require removing the source function below.\n\n\n\n\n\n\nNoteThe tidyverse meta-package in R\n\n\n\nKeep in my that the tidyverse package in R is a meta package and contains many standalone R packages, including:\n\nggplot2: declarative construction of statistical graphics\ndplyr: relational and vectorized data manipulation operations\ntidyr: restructuring datasets into tidy (rectangular) form\nreadr: standardized, high-performance import of tabular data\npurrr: functional programming utilities for iterative workflows\ntibble: enhanced data-frame representation with explicit printing semantics\nstringr: consistent, vectorized string processing\nforcats: handling and releveling of categorical variables",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Tutorial: Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_data_exploration.html#our-dataset-impacts-of-mud-dwelling-jackalopes-on-dissolved-oxygen",
    "href": "chapters/tutorial_data_exploration.html#our-dataset-impacts-of-mud-dwelling-jackalopes-on-dissolved-oxygen",
    "title": "18  Tutorial: Data Exploration",
    "section": "18.3 Our dataset: Impacts of mud-dwelling jackalopes on dissolved oxygen",
    "text": "18.3 Our dataset: Impacts of mud-dwelling jackalopes on dissolved oxygen\n\n\nThese data for this exercise come from a totally absolutely real experiment on a newly discovered species of jackalope, Lepusantilocapra pelonates (“mud-dwelling jackalope”). Researchers observed that mudflats with higher densities of L. pelonate’s tended to have higher dissolved oxygen. At the same time, nearby forest salt licks provided salt that varied naturally across time and space, and L. pelonates were frequently observed foraging at them.\nTo test whether jackalope density and salt availability had causal effects on dissolved oxygen, the researchers constructed artificial mudflats and experimentally varied the number of L. pelonates (0–4 individuals) and salt availability (low, medium, high). Each unique combination was represented by a single mudflat, and dissolved oxygen was measured at each site. These are the data from that totally real experiment.\n\n\n\n\nTo do these exercises on your own, you will need the following files (simply click to save):\n\nlepusantilocapra_pelonates_experiment.csv\nlepusantilocapra_climate_data.csv\n\nPlace the data files into your class_exercise_data folder.\nLet us read in these data and save it to an object called d (for “data”). Yes, according to best practices for naming, this could have a more informative name. Because this is (a) a singular dataset for a simple analysis, and (b) a simple class example, we will use a shorthand to prevent some extra keystrokes. But just know this is technically very bad practice. Also, for the sake of streamlining this example, this file has no associated metadata (this will be shown later when we get to the modeling phase). Already, we are flawed programmers. Let us move forward and try to forget these incidents.\n\nd &lt;- read.csv(\"class_exercise_data/lepusantilocapra_pelonates_experiment.csv\")\n\nThis was a successful (and very simple) data ingestion. (Again, there are many ways to ingest data, so use whatever you are most comfortable with.)",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Tutorial: Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_data_exploration.html#applying-the-data-exploration-toolkit",
    "href": "chapters/tutorial_data_exploration.html#applying-the-data-exploration-toolkit",
    "title": "18  Tutorial: Data Exploration",
    "section": "18.4 Applying the Data Exploration Toolkit",
    "text": "18.4 Applying the Data Exploration Toolkit\nAs you recall, we have the following steps of the ante hoc Data Exploration Toolkit:\n\nTool #1: Assess potential outliers (univariate and multivariate)\nTool #2: Assess the presence of excess zeros\nTool #3: Explore interactions and structural relationships\nTool #4: Assess potential information redundancy among covariates (e.g., multicollinearity)\nTool #5: Decide whether covariate standardization is warranted\n\nThe good news is that we can generally combine the first three steps by simply looking at the overall structure of your dataset in several ways. These “ways” are non-prescriptive; you should continue to explore until you understand all the dimensions of your raw data. Let’s begin!",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Tutorial: Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_data_exploration.html#basic-examination-of-broad-scale-data-structure",
    "href": "chapters/tutorial_data_exploration.html#basic-examination-of-broad-scale-data-structure",
    "title": "18  Tutorial: Data Exploration",
    "section": "18.5 Basic examination of broad-scale data structure",
    "text": "18.5 Basic examination of broad-scale data structure\nFirst, we need to check the data types that were imported; this is a critical first step, as this can let you immediately detect any red flags. Though the str() (or the glimpse() function) is good for initial checking:\n\nstr(d)\n\n'data.frame':   45 obs. of  3 variables:\n $ number_jackalope: int  0 0 0 1 1 1 2 2 2 3 ...\n $ salt_treatment  : chr  \"1_low\" \"1_low\" \"1_low\" \"1_low\" ...\n $ oxygen_ppm      : num  0.67 0.97 0.97 2.67 1.37 4.35 3.23 3 2.17 3.23 ...\n\n\nWe note that number_jackalope is an integer. We could certainly keep it like this, but, given how few categories there are (and the fact that this was an experimental condition), we decide that it is best to convert it to a factor.\n\nd &lt;- d |&gt;\n  mutate(number_jackalope = factor(number_jackalope )) |&gt;\n  mutate(salt_treatment = factor(salt_treatment))\n\n\n\n\n\n\n\nWarningDigression about base versus magrittr/tidyverse pipes\n\n\n\n\n\nNote that the native pipe operator |&gt; was used in the above tidy code snippit.\n\n\n\n\n\n\n\n\nFeature\n|&gt; (base R)\n%&gt;% (magrittr / tidyverse)\n\n\n\n\nIntroduced\nR 4.1.0 (base R)\nEarlier (via **magrittr**, widely adopted by tidyverse)\n\n\nRequires package\nNo\nYes (library(magrittr) or library(tidyverse))\n\n\nDefault recommendation\nPreferred going forward\nLegacy / tidyverse-centric\n\n\nPlaceholder support\nNo implicit placeholder\nSupports . placeholder\n\n\nExample placeholder use\nNot possible\ndf %&gt;% mutate(x = . + 1)\n\n\nAnonymous functions\nUses \\(x) syntax\nUses . or {} blocks\n\n\nExample\nx |&gt; mean()\nx %&gt;% mean()\n\n\nReadability (simple pipelines)\nVery clean\nVery clean\n\n\nReadability (complex pipelines)\nCan become verbose\nOften clearer due to .\n\n\nCompatibility with older R\nRequires R ≥ 4.1\nWorks in older R\n\n\nDebugging / tooling\nExcellent (native)\nExcellent\n\n\nPerformance\nSlightly faster (base)\nSlight overhead (negligible)\n\n\nTeaching clarity\nEmphasizes base R\n️ Adds extra abstraction\n\n\nFuture-proofing\nYes\n️ Likely to persist but not expand\n\n\nWorks with tidyverse verbs\nYes\nYes\n\n\n\n\n\n\nYou can then run str(d) or glimpse() again to see this change.",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Tutorial: Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_data_exploration.html#data-structure-visualizing-variation-within-and-among-groups",
    "href": "chapters/tutorial_data_exploration.html#data-structure-visualizing-variation-within-and-among-groups",
    "title": "18  Tutorial: Data Exploration",
    "section": "18.6 Data structure: visualizing variation within and among groups",
    "text": "18.6 Data structure: visualizing variation within and among groups\nLet us now examine how the treatments and the response variable are structured and if there are any statistical outliers. There are many approaches to this, and most of these formal outlier tests (Grubb’s test, Dixon’s Q test, and Rosner’s test) are based on the condition of dat normality. As such, they have context-dependent performance; that is, they fail at low sample sizes and have variable ability to detect outliers dependent on sample size. If you want to use these tests despite these weaknesses, you can find them in the EnvStats and outliers packages.\nIt is important at this step to avoid making any inferences about the relationships. As far as you, the responsible data analyst, are concerned, there are no relationships. You are simply looking for anything odd in the dataset: spurious datapoints, data outside the range of data (though you could use the range() function to see this), etc. Let us conduct a simple examination using one tabular and six graphical approaches:\n\nStep 1: Simple summary\nStep 2: Boxplot (Tukey-style)\nStep 3: Violin plot\nStep 4: Scatterplot\nStep 5: Density plot\nStep 6: Heatmap\nStep 7: Interaction plot\n\n\n18.6.1 Simple summary and examination of data structure\n\nsummary(d)\n\n number_jackalope  salt_treatment   oxygen_ppm    \n 0:9              1_low   :15     Min.   : 0.670  \n 1:9              2_medium:15     1st Qu.: 3.000  \n 2:9              3_high  :15     Median : 5.370  \n 3:9                              Mean   : 7.456  \n 4:9                              3rd Qu.:12.300  \n                                  Max.   :19.560  \n\n\nThis returns information about range, median value, and sample size within groups. Use this first to discover any major issues.\nYou can then graphically view aspects of your raw dataset. So, let us create a few plots using the ggplot2 package. Please refer to the ggplot2 Cheat Sheet (right click for download) for more information about this.\n\n\n\n\n\n\nTipA recipe for a ggplot2 visualization\n\n\n\n\n\nUse the ggplot2 Cheat Sheet (right click for download) and find the appropriately useful arguments to replace the angle bracketed (&lt; &gt;) sections (that is, remove the angle brackets in your final code).\n\ndata |&gt; \n  ggplot(\n    aes(x = &lt;x_variable&gt;, y = &lt;y_variable&gt;, &lt;optional_aesthetic&gt; = &lt;variable&gt;)\n  ) +\n  geom_&lt;geometry&gt;(&lt;geometry_options&gt;) + \n  labs(\n    title = \"&lt;plot title&gt;\",\n    x = \"&lt;x-axis label&gt;\",\n    y = \"&lt;y-axis label&gt;\",\n    &lt;other_labels&gt;\n    ) +\n  theme_&lt;theme_name&gt;() + \n  &lt;optional_additional_layers&gt;\n\nThis follows the Grammar of Graphics (GoG) approach introduced by Leland Wilkinson in 1999. All the verbiage (e.g. facets, aesthetics, geometry) was then used as the basis for the first ggplot version by Hadley Wickham in 2007 (first book published in 2009; now available online). Wilkinson’s book on graphical theory is a great read and a great example of how to develop a set or irreducible guidelines for a seemingly complex system.\n\n\n\n\n\n18.6.2 Boxplot (Tukey-style)\nFor Step #1 of our Data Exploration Toolkit, we can start the process of identifying potential univariate outliers by using a Tukey-style boxplot for visual inspection. The following code takes our dataset (d) and then sends it forward to the ggplot function. As you read on the Cheat Sheet (linked above) and the brief recipe in the callout box above, you need to set what is called an “aesthetic” (abbreviated to “aes” in the function) that contains what you want to be your x and y variables. You can also set the fill color to vary by salt_treatments too. Then, the next lines add some axis and graph labels and set a theme (i.e. changes colors of the whole plotting area).\n\nd |&gt; \n  ggplot(aes(x = salt_treatment, y = oxygen_ppm, fill = salt_treatment)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot by salt treatment\", x = \"salt_treatment\", y = \"oxygen_ppm\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n18.6.3 Cleveland Dot Plot\nThis is another simple way of visualizing potential outlier. Fancy name; unfancy visualization.\n\nd |&gt;\n  ggplot(aes(x = oxygen_ppm, y = salt_treatment)) +\n  geom_point(alpha = 0.6) +\n  labs(\n    title = \"Cleveland dot plot by salt treatment\",\n    x = \"oxygen_ppm\",\n    y = \"salt_treatment\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow, what can be said about potential outliers? In our boxplot, we see two black dots each group that seem to be outside of the normal range of those groups. These are potential outliers. You should go back and examine these datapoints to see if there is anything strange about them. But be aware of how Tukey-style boxplots calculate outliers; it can be misleading if you do not understand Tukey’s original approach. Let us also examine the boxplots by different L. pelonates densities (i.e. the jackalope density treatments):\n\nd |&gt; \n  ggplot(aes(x = number_jackalope, y = oxygen_ppm, fill = number_jackalope)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot by number of jackalope\", x = \"number_jackalope\", y = \"oxygen_ppm\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe Tukey boxplot is not showing any black “outlier” dots in the groups. So, using this approach, there do not seem to be any structural outliers. You can repeat this process on every variable in your dataset. It is generally effective for flagging major issues (anomalous values, etc.). Most likely, however, this approach will flag values that end up being perfectly reasonable. That is more than acceptable. At the very worst, you have put your eyes on the dataset a few more times.\n\n\n18.6.4 Violin plot\nA violin plot visualizes distributions using a mirrored density plot. As such, the default is to extend the density profile past the data boundaries. This feature can be suppressed by setting the trim argument in the geom_violin to FALSE.\n\nggplot(d, aes(x = number_jackalope, y = oxygen_ppm, fill = salt_treatment)) +\n  geom_violin(trim = FALSE) +\n  labs(title = \"distribution of oxygen ppm by # jackalope and salt treatment\",\n       x = \"number of jackalopes\",\n       y = \"oxygen ppm\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBecause violin plots do not explicitly show outliers, they may be worse in some ways than Tukey-style boxplots for visual data exploration. Furthermore, violin plots may be harder to interpret than boxplot, may be space-inefficient (in terms of Edward Tufte’s well-known argument for reduced non-data ink in visualizations), and may smooth over important details of the data (i.e. it’s difficult to visually assess the location of the 25th and 75th percentiles, if that is something you are accustomed to). Violin plots also can be misleading, especially with small sample sizes. Use them, but think critically about what information they are providing.\n\n\n18.6.5 Scatter plot\nDo not overlook the utility of simple scatterplot. A scatterplot is useful for seeing the raw relationship between a predictor and a response across treatments. It helps you assess overall trends, clustering, and potential outliers while keeping individual observations visible.\n\nggplot(d, aes(x = number_jackalope, y = oxygen_ppm, color = salt_treatment, shape = salt_treatment)) +\n  geom_jitter(width = 0.1, height = 0) +\n  labs(title = \"oxygen ppm scatter plot by # jackalope and salt treatment\",\n       x = \"# jackalope\",\n       y = \"oxygen ppm\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n18.6.6 Density plot\nA density plot is very useful for quickly comparing the overall distribution of a response variable within or between groups. It helps you see differences in spread, skewness, and overlap between treatments that may not be obvious from summary statistics alone.\n\nggplot(d, aes(x = oxygen_ppm, fill = salt_treatment)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"density of oxygen ppm by salt treatment\",\n       x = \"oxygen ppm\",\n       y = \"density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor our example, this approach gives us a good idea of the relative spread of each of salt_treatment response. We can also produce the same plot, but we could split it by jackalope treatment instead. (In this case, it’s a bit messy.)\n\nggplot(d, aes(x = oxygen_ppm, fill = number_jackalope)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"density of oxygen ppm by # jackalope\",\n       x = \"oxygen ppm\",\n       y = \"density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n18.6.7 Heatmap\n\nd |&gt;\n  group_by(number_jackalope, salt_treatment) |&gt;\n  summarise(mean_oxygen = mean(oxygen_ppm), .groups = \"drop\") |&gt;\n  ggplot(aes(x = number_jackalope, y = salt_treatment, fill = mean_oxygen)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  labs(title = \"heatmap: mean oxygen ppm by # jackalope and salt treatment\",\n       x = \"number of jackalope\",\n       y = \"salt treatment\",\n       fill = \"mean oxygen ppm\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n18.6.8 Interaction plot (Step #3)\nStep #3 of our Data Exploration Toolkit is to examine our interaction structure. A simple interaction plot is useful for getting a quick, visual sense of whether the relationship between a predictor and a response might change across levels of another factor. By faceting the same boxplot across salt treatments (i.e. creating “small multiples” sensu Edward Tufte), you can more easily compare both the typical oxygen levels and the spread of the data for each jackalope count within each treatment. This does not test for an interaction, but it is a fairly good reality check for overall patterns –such as shifts in medians or changes in variance– that might justify including an interaction term later on.\n\nggplot(d, aes(x = number_jackalope, y = oxygen_ppm)) +\n  geom_boxplot() +\n  facet_wrap(~salt_treatment) +\n  labs(title = \"Oxygen ppm by number of jackalopes across salt treatments\",\n       x = \"# jackalope\",\n       y = \"oxygen ppm\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis is not particularly useful for detecting data anomalies, but it could be useful for helping you understand where data may be sparse (i.e. low sample sizes) or where there may be missing combinations of treatments (which can lead to lack of model convergence).",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Tutorial: Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_data_exploration.html#inspecting-for-multicollinearity-step-4",
    "href": "chapters/tutorial_data_exploration.html#inspecting-for-multicollinearity-step-4",
    "title": "18  Tutorial: Data Exploration",
    "section": "18.7 Inspecting for multicollinearity (Step #4)",
    "text": "18.7 Inspecting for multicollinearity (Step #4)\nThe jackalopeologists then conducted a preliminary quantification of the climate in which L. pelonates was found. They used remote sensors to measure humidity, precipitation, temperature, temperature variability, and heating degree days. We don’t think we can include all the variables in our statistical model, so we want to first inspect if any of the variables are correlated (i.e. they tell us redundant information). Let’s read in these climate data and save it to an object called “climate”:\n\nclimate &lt;- import(\"class_exercise_data/lepusantilocapra_climate_data.csv\")\n\nAnd then look at the structure of the data:\n\nstr(climate)\n\n'data.frame':   37 obs. of  5 variables:\n $ humidity        : int  1 1 3 1 1 0 2 0 3 0 ...\n $ precipitation   : int  692 874 852 990 650 741 1301 368 1148 669 ...\n $ temperature     : num  94 100 106 112 116 ...\n $ temp_variation  : num  8.2 8.3 8.6 9.2 8.95 9.1 9.35 9.45 10.4 9.4 ...\n $ heating_deg_days: num  2710 2514 2334 2156 2048 ...\n\n\nNormally, we would have the associated metadata so that you could see the units for each of these variables. For this example, the units are not important.\nWe will proceed to visually examine multicollinearity (correlation) between these climate metrics, but we will also include–as a basic measure of strength of any correlation– the Pearson correlation metric. Let’s use a pairwise plot to visually inspect these. Note that the correlation coefficient (which varies between 0-1) is on the opposite diagonal from the graph.\n\nclimate |&gt;\n  ggpairs(\n    upper = list(continuous = \"cor\"),\n    lower = list(continuous = \"smooth\"),\n    diag  = list(continuous = \"barDiag\")\n  )\n\n\n\n\n\n\n\n# \"upper\" = Show Pearson correlation coefficients in the upper panel\n# \"lower\" = Optionally show scatterplots with smoothing in the lower panel\n# \"diag\"  = Optionally show histograms on the diagonal\n\nThe GGally::ggpairs function markedly improves upon similar functionality of base R. What do we see? We see that:\n\nhumidity and precipitation are strongly and positively correlated (r = 0.648)\ntemperature and temperature variation are strongly and positively correlated (r = 0.584)\ntemperature and heating degree days are strongly and negatively correlated (r = -0.998)\ntemperature variation and heating degree days are strongly and negatively correlated (r = -0.588)\n\nSo, what to do, what to do, what to do? The next part is one solution to this issue. We clearly have some variables that might be providing redundant information. This could impact our modeling (specifically inflating the variance of a given regression coefficient). We can explicitly examine this using Variance Inflation Factors (VIFs), which are calculated for each term in a model. Note that there are options for VIFs applied to both continuous and categorical variables. Let us do this for continuous variables for these climate data. Most available functions in R require regression models; however, there are some approaches–such as the usdm package–that require only data frames of multiple variables. So, let us use that.:\n\nvif_results &lt;- usdm::vif(climate)\nvif_results\n\n         Variables        VIF\n1         humidity   1.774591\n2    precipitation   1.764116\n3      temperature 244.453457\n4   temp_variation   1.555291\n5 heating_deg_days 246.129937\n\n\nThis is a clear example that two variables, temperature and heating_deg_days, are highly correlated with a number of other variables. That is, they potentially provide redundant information.\nIf variables are negatively correlated, consider retaining them both. From there, you can decide to:\n\nEliminate one of the positively correlated variables from our analysis.\nConduct a factor-reduction analysis to create a new variable that captures the correlated variation between correlated variables.\n\nThe former sounds lazy and arbitrary. Let us proceed with a factor reduction approach. For this, one of the most widely accepted approaches is a principal components analysis (PCA). There are many related factor reduction methods out there as well that may work better for non-normal data, etc., but PCA generally works fairly well.\nOur first step is to conduct a PCA without specifying how many new components (new variables) should be generated.\n\npca_results &lt;- stats::princomp(\n  ~ humidity + precipitation + temperature + temp_variation + heating_deg_days,\n  data = climate,\n  cor = TRUE\n) # using the namespace operator\n\nNote that the above code does not work with pipes. This behavior happens sometimes with tidy.\nLet us now examine the results output, specifically the standard deviation values (i.e. “Comp.1”, etc.). Your first step is square the standard deviations and then use this eigenvalue to decide how many new components (i.e. new variables) to retain. Values &gt;1 indicate that the new principal component accounts for more variance than the original variables; this is the critically important part of PCA.\n\npca_results$sdev ^ 2  # this squares the standard deviations\n\n     Comp.1      Comp.2      Comp.3      Comp.4      Comp.5 \n2.688766244 1.452196831 0.507234299 0.349760919 0.002041707 \n\n\nIt looks like the first two components work well. Now, we want to force a PCA to contain just two components, and we want those two variables to be as independent as follows. We therefore conduct what is called a rotated PCA to achieve maximum information of each component. Note that we are now using the principal function found in the psych package:\n\npca_results_rotated &lt;- psych::principal(climate, nfactors = 2, rotate = \"varimax\", covar = FALSE) # using a namespace operator\npca_results_rotated$loadings\n\n\nLoadings:\n                 RC1    RC2   \nhumidity         -0.124  0.892\nprecipitation            0.905\ntemperature       0.944 -0.196\ntemp_variation    0.799       \nheating_deg_days -0.945  0.196\n\n                 RC1   RC2\nSS loadings    2.444 1.697\nProportion Var 0.489 0.339\nCumulative Var 0.489 0.828\n\n\nLet’s dissect this. First, look at the loadings table. These show the linear correlations between the rotated principal components (RC1 and RC2) and the original climate variables. Therefore, RC1 is strongly correlated with temperature, temperature variation, and heating degree days, whereas RC2 is correlated with humidity and precipitation.\nYou can see this generally reflected when you plot PCA axes of variation. A safe default is to biplot the princomp() object:\n\nbiplot(pca_results)\n\n\n\n\n\n\n\n\nOne advantage of PCA/ICA or other factor reduction methods is that you can use expert knowledge to decide on the number of resulting axes. Certainly, in the end, these may not capture much total variation. This flexibility may improve interpretability.\nWe can then create two new rotated scores for each observation (row). But what are these new values? These are simply new continous values that capture the variation attributable to the set of variables that correlated with each rotated component (PC1, PC2, etc.).\nThe statistical convention in most fields is to accept all rotated components that sum to &gt;80% of cumulative explained variation. You can output these new rotated scores by running the following code and then inputting these as new columns in your dataset. Then, run the model with each of these new proxy variables as new factors in your model(s):\n\npca_results_rotated$scores\n\n            RC1         RC2\n1  -2.297743319 -0.93979095\n2  -1.615079146 -0.56215540\n3  -0.742103374  0.43524344\n4   0.168491793 -0.07924539\n5   0.175054065 -0.81591440\n6   0.319233549 -1.07322356\n7  -0.151659949  1.05198229\n8  -1.153591365 -1.67740452\n9   1.731717862  1.53557338\n10  1.531908063 -1.13298910\n11 -0.710144408 -0.01812957\n12 -0.835747218  0.30459876\n13  0.642890086 -0.30633196\n14 -0.163440060  0.94065321\n15 -0.005344451 -0.47011057\n16  0.593689628  0.62454354\n17 -0.537374119 -0.19831498\n18  1.575800238  0.43229319\n19  1.236151237  0.89397481\n20  0.837959614 -1.25849702\n21  0.815647774 -0.81482061\n22 -0.001440188  0.48095893\n23  0.684009759  0.72280254\n24  0.679808842  0.25259784\n25 -0.882679835 -0.23250232\n26  0.367779897  1.47725934\n27 -0.619318068 -1.21317733\n28 -0.983554574 -0.13256898\n29 -0.027524574 -0.59730235\n30  1.368900971 -0.86595054\n31  1.536240251 -0.64932937\n32  0.688559658 -0.20506311\n33 -0.105285111 -0.35740682\n34 -1.659241971 -0.15264062\n35 -0.749029761  0.32033636\n36 -0.444050160  0.60008159\n37 -1.269491637  3.67997024\n\n\nEnd of tutorial",
    "crumbs": [
      "Part II: Chaos to Columns",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Tutorial: Data Exploration</span>"
    ]
  },
  {
    "objectID": "chapters/statistical_aphorisms.html",
    "href": "chapters/statistical_aphorisms.html",
    "title": "19  Statistical Philosophy",
    "section": "",
    "text": "19.1 Some larger-scale advice before moving into modeling\nMore than any single method, R (or Python) package, or complex statistical model, the ultimate goal of this course is for you to develop a personal and operational statistical philosophy. This term is often thrown around, so we define a statistical philosophy as:\nIn other words, a good statistical philosophy is not a set of rules; it does not tell you exactly what to do in every situation. Instead, it functions to reduce bias, prevent analytical wandering, promote clarity, encourage efficiency. Most importantly, a good statistical philosophy evolves with you, as you learn more. I will offer a set of guiding principles and aphorisms early in the course. You should modify them, reject them, or replace them entirely. What matters is that you have something guiding your choices. This is how you become consistent, thoughtful, and credible as a scientist rather than someone who simply runs analyses.\nWhat follows are two components. First, I describe four statistical aphorisms. They are intentionally broad. Again, think of them as guardrails rather than rules. Then, I explain what I mean by analytical workflow, which supports an operational statistical philosophy.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Philosophy</span>"
    ]
  },
  {
    "objectID": "chapters/statistical_aphorisms.html#some-larger-scale-advice-before-moving-into-modeling",
    "href": "chapters/statistical_aphorisms.html#some-larger-scale-advice-before-moving-into-modeling",
    "title": "19  Statistical Philosophy",
    "section": "",
    "text": "A set of principles that consistently guides how you think about data, models, uncertainty, and decisions.\n\n\n\n\n\n\n\n\nTipKeep your eyes out!\n\n\n\n\n\nLook and listen to your graduate student peers, postdoctoral researchers, and faculty who you interact with and pay close attention to the different statistical perspectives that exist. It’s a fascinating amount of variation! One good example is how different practitioners interpret the output from multimodel (AIC, BIC, or DIC) comparisons. Whenever you hear a different perspective, ask the researcher about how the philosophy that guides their inferential decisions.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Philosophy</span>"
    ]
  },
  {
    "objectID": "chapters/statistical_aphorisms.html#four-statistical-aphorisms",
    "href": "chapters/statistical_aphorisms.html#four-statistical-aphorisms",
    "title": "19  Statistical Philosophy",
    "section": "19.2 Four Statistical Aphorisms",
    "text": "19.2 Four Statistical Aphorisms\n\n\n\n\n\n\nNoteAphorism #1: Accept imperfection\n\n\n\n\n\nBeware perfection. In data analysis, one can be paralyzed by by the moving target of the perfect solution. But what does perfection really mean in practice? Does it mean that an analysis looks complex enough to impress your colleagues or instructors, even if it is does not match the scope or intent of your questions (or, worse yet, is blatantly incorrect)? Is it an analysis that works flawlessly the first time, right out of the box? Or is it one that is quickly built and deployed but also hides critical assumptions or produces error-filled predictions?\nThe danger in seeking statistical (or, more generally, scientific) perfection lies in waiting for an ideal solution that may never arrive, or may arrive too late. In doing so, your and your collaborators’ progress stalls, opportunities are missed, and, perhaps most importantly, learning is sorely delayed. This idea is captured poetically in the words of Robert Watson-Watt, the inventor of radar:\n\nGive them the third best to go on with; the second best comes too late, the best never comes.\n\nWatson-Watt’s advice should remind us that, in dynamic and complex fields such as radar development or our own messy statistical modeling, timely action often outweighs our vision of perfection. A solution that is simply good enough today allows you to:\n\nLearn through deployment: Practical application often reveals insights that you cannot anticipate.\nIterate and improve: Real-world feedback reveals issues far faster than endless refinement in isolation!\nTransparently communicate analytical limitations: Collaborators and stakeholders can better understand assumptions, uncertainties, and the scope of your work.\n\nIn short (as as you know), perfection is often the enemy of good progress. By purposefully embracing imperfection, you will be able to better learn, iterate, and produce work that is timely, practical, and ultimately more impactful.\n\n\n\n\n\n\n\n\n\nNoteAphorism #2: Embrace complexity.\n\n\n\n\n\nThere is a reason why my lab is called the Behavioral Complexity Lab. Ecological and biological systems are wonderfully complex, thrillingly messy, and often only partially observed. Pretending that such complexities do not exist does not make your analysis stronger or more robust; it just makes the analysis less honest and transparent. Actively embracing complexity does not mean building the most complicated model you can imagine. It means acknowledging that the process generating your data is rarely simple and that oversimplification might be very costly.\nAt the same time, complexity must be earned. Adding complex model structure, more parameters, or more nested hierarchical layers should be motivated by expert knowledge of biology (at any level) or study design, not by the existence of a published paper, an established method, or a online tutorial. Tis better to manage complexity rather than avoid it (mainly because complexity is all around us). Good analysts learn how to represent complexity in ways that are interpretable, testable, and aligned with the question being asked.\n\n\n\n\n\n\n\n\n\nNoteAphorism #3: Beware of Armadillo Burrows.\n\n\n\n\n\nThis is based on a real sign I saw on a hiking trail in southern Brazil, and I thought it was a good metaphorical reminder to not fall into someone else’s statistical trap!\nConsider a statistical approach of a senior colleague or advisor that they deem the correct or optimal approach (without justification)? One of the fastest ways to absolutely derail an otherwise good analysis is to blindly inherit and use a statistical framework without understanding why it was chosen in the first place. If you blindly accepted it, what is to say that the previous scientist did not also do the same, ad infinitum? Methods might be copied or carried over from published papers, lab traditions, or online examples without careful thought. Every statistical choice embeds assumptions about scale, error, independence, and causality. When you adopt someone else’s approach wholesale, you also adopt all of their assumptions, whether or not they apply to your data. So, please be cautious.\nThe assignments in this course will repeatedly (ad nauseum) ask you to justify your modeling decisions clearly. And your analysis decisions should align with your question, your data, and your scientific goals.\n\n\n\n\n\n\n\n\n\nNoteAphorism #4: Beware paralysis of analysis.\n\n\n\n\n\nThere is a point at which conducting more data exploration, running more (and different) models, and sifting through more diagnostics stop improving understanding and start delaying decisions. This is paralysis of analysis, and it is insidious. It often shows up as endless model tweaking, perpetual searching for the perfect model structure, or reluctance to commit to an inferential path (which can lead to never-ending arguments with co-authors),\nA good analytical workflow balances scientific rigor with momentum. Certainly, you may end up with data that are near-perfect and easy to work with. You also may use models that are well-matched to your data constraints without you ever lifting an analytical finger. But this is rare. You should aim for models that are good enough to answer the question at hand, given the data you have, within the constraints you face. This does not mean lowering your standards, especially when faced with looming deadlines. This simply means recognizing diminishing returns and knowing when to move forward. Science advances through iteration and improvement, not achieving perfection on a first attempt.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Philosophy</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html",
    "href": "chapters/intro_modeling.html",
    "title": "20  Introduction to Models",
    "section": "",
    "text": "20.1 From data to data-generating processes\nEvery measurement reflects two central components of a system, which will be the focus of this short chapter:\nOnce you start thinking about data in this way, it perhaps becomes clearer that not all variation in a given dataset carries the same meaning. Some variation reflects real structure in the system that we actually care about. Some has to do with the natural limits of the data themselves. Other variation comes from randomness, measurement limits, or processes that we did not explicitly account for.\nSo, with that, let us delve into the two fundamental components of set of measurements.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Models</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html#from-data-to-data-generating-processes",
    "href": "chapters/intro_modeling.html#from-data-to-data-generating-processes",
    "title": "20  Introduction to Models",
    "section": "",
    "text": "In this chapter, we make a necessary transition: from practical handling of data to thinking conceptually about data-generating processes. By now, you have entered your data into a spreadsheet (or some other organized data table). Now, it is time to modify this “data = spreadsheet” thinking and consider that data are outcomes of often complex processes. That is, data are not simply abstract numbers.\nAs a thought exercise, consider a set of mass values for a jackalope. The body mass of marsh-water guzzling jackalopes (Lepus antilocaprahydropicus) varies because individuals differ in body size and condition. Depending on recent food capture or hydration state, mass may fluctuate within individuals over very short time scales, driven by rapid changes to a complex digestive and metabolic machinery. Additional variation also arises during measurement due to instrument precision, calibration drift, and movement or posture on the scale. In sum, there are a myriad processes leading to a simple measurement.\n\n\n\n\n\n\nComponent #1: Systematic structure (signal): patterns deriving from mechanisms and constraints\n\nComponent #2: Unmeasured variation (noise): variability generated by unidentified mechanisms, measurement constraints, and nmeasured stochastic processes\n\n\nThese two components are co-products of the same data-generating processes.\n\n\n\n\n\n\n\n\nNoteA thought about ecology being a discipline of controlling noise\n\n\n\n\n\nThe order in which Signal and Noise are discussed below does not reflect my view of their relative importance. In fact, for modeling purposes, I argue that the majority of ecologists’ work –rightly so, I think– is focused on coping with noise. To convince you of this, take any scientific paper and compare the length of the sections dedicated to hypotheses and methods. We use calibrated instruments; we carefully design well-replicated and controlled experiments; we test and re-test interobserver reliability; we control for difference is detectability (or other biases). What remains after dealing with all the noise is the answer to an often simple question (hypothesis).",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Models</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html#component-1-systematic-structure-signal",
    "href": "chapters/intro_modeling.html#component-1-systematic-structure-signal",
    "title": "20  Introduction to Models",
    "section": "20.2 Component #1: Systematic structure (signal)",
    "text": "20.2 Component #1: Systematic structure (signal)\nWhen we collect data, we are not merely cataloging patterns or associations. I would argue that few scientists have been driven by the search for simple associations independent of causes. If data are outcomes of data-generating processes, then those processes must, by definition, include causes. In ecology and field biology, we routinely speak in causal language:\n\nFood availability influences body condition\nTemperature affects metabolic rate\nPredators reduces nest success\nGroundwater discharge stabilizes stream temperature\n\n\n\nIn our work, we are observing the results of interacting forces operating in real systems. Those forces operate directionally (in time and space). Something influences something else; that something influences yet another something. And so forth. And despite the often misinterpreted adage “Correlation does not equal causation,” ecologists rarely stop at correlation. Associations are taken almost as cues to hypothesize about the mechanisms that generated them.\nEven when we fully describe a simple univariate distribution, we are implicitly asserting something about a set of generative processes: that events occur independently, that variability scales with the mean (in some fashion), that measurements fluctuate around a central tendency (in some fashion). In a regression model (e.g. GLM, GLMM), the “signal” is simply the structured component of the data-generating process — the part we attribute to directional influences of predictors. The model does not create causation; it formalizes assumptions how we hypothesize about how different putative causes may produce observable patterns. For example, a recent –and very well-written– paper (abstract at right) by our own University of Wyoming colleagues clearly demonstrates this causal mindset when describing the results on an analysis.\n\n\n\n\nFurthermore, many of the primary theories in biology and ecology are inherently causally structured (as they are based on many sets of observational and experimental studies):\n\n\n\n\n\n\n\n\n\ntheory\nyear(s)\ncore causal mechanism\n\n\n\n\nEvolution by Natural Selection\n1859\nHeritable variation causes differential survival and reproduction\n\n\nCompetitive Exclusion Principle\n1934\nLimiting resources cause competitive displacement\n\n\nOptimal Foraging Theory\n1966\nFitness maximization drives adaptive foraging decisions\n\n\nIsland Biogeography Theory\n1967\nIsolation and area determine immigration and extinction rates\n\n\nIntermediate Disturbance Hypothesis\n1978\nDisturbance prevents competitive exclusion and maintains diversity\n\n\nModern Coexistence Theory\n1990s\nNiche differences stabilize species coexistence\n\n\nNeutral Theory of Biodiversity\n2001\nDemographic stochasticity drives community composition\n\n\n\n\n\n20.2.1 Identifying causal language\nWe often know causal words when we see them. When we are assessing the inferences (associational or causal) made in scientific papers, our first step is to identify if associational or causal language is being employed.\nFor a bit of fun, read the statements below, decide if the statement makes claim about associations or causes, and then click to expand the box.\n\n\n\n\n\n\n\n\nTipElevated temperature induces heat-shock gene expression\n\n\n\n\n\nAnswer: Causal\nWhy: The verb induces asserts a directional mechanism from temperature to gene expression.\n\n\n\n\n\n\n\n\n\nTipForaging duration and prey density are negatively associated in seabirds\n\n\n\n\n\nAnswer: Associational\nWhy: The statement reports a relationship without asserting a mechanism or direction of effect.\n\n\n\n\n\n\n\n\n\nTipForest fragmentation reduces nesting success\n\n\n\n\n\nAnswer: Causal\nWhy: The phrase reduces explicitly claims that fragmentation produces a change in nesting success.\n\n\n\n\n\n\n\n\n\nTipHeat-shock gene expression is associated with ambient temperature\n\n\n\n\n\nAnswer: Associational\nWhy: The wording describes a correlation pattern without implying intervention or mechanism.\n\n\n\n\n\n\n\n\n\nTipNitrogen availability causes faster leaf expansion\n\n\n\n\n\nAnswer: Causal\nWhy: The verb causes signals a direct causal claim about nitrogen’s effect on growth.\n\n\n\n\n\n\n\n\n\n\n\n\nTipMetabolic rate and activity level are associated across mammals\n\n\n\n\n\nAnswer: Associational\nWhy: The statement summarizes a cross-species pattern without claiming direction or mechanism.\n\n\n\n\n\n\n\n\n\nTipBird species richness is higher in forested landscapes\n\n\n\n\n\nAnswer: Associational\nWhy: This reports a pattern across landscapes but does not assert that forest cover drives richness.\n\n\n\n\n\n\n\n\n\nTipHigher prey density shortens foraging trips\n\n\n\n\n\nAnswer: Causal\nWhy: The verb shortens implies that prey density actively changes foraging behavior.\n\n\n\n\n\n\n\n\n\nTipIncreased activity elevates metabolic rate\n\n\n\n\n\nAnswer: Causal\nWhy: The statement claims a physiological mechanism linking activity to metabolism.\n\n\n\n\n\n\n\n\n\nTipTree growth and temperature are associated after accounting for elevation\n\n\n\n\n\nAnswer: Associational\nWhy: Conditioning on elevation defines a conditional association without asserting causation.\n\n\n\n\n\nImportantly, the use of causal language in scientific subfield has greatly increased. Some interesting readings are: *  *",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Models</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html#component-2-unmeasured-variation-noise",
    "href": "chapters/intro_modeling.html#component-2-unmeasured-variation-noise",
    "title": "20  Introduction to Models",
    "section": "20.3 Component #2: Unmeasured Variation (Noise)",
    "text": "20.3 Component #2: Unmeasured Variation (Noise)\nThe second component of any set of measurements is unmeasured variation, often called noise. Unfortunately, the word noise has morphed into a sort of statistical perjorative that we equate with annoying, irrelevant, or cumbersome. So, why is noise so important? It is important because a model of the world is incomplete if it explains only the mean of a signal but ignores how variability behaves around that mean. Explaining the average outcome is only half the story. We must also understand why observations deviate from that average (or other measure of central tendency).\nIn other words, noise is not a nuisance obscuring an otherwise pristine biological or ecological signal. It is not static layered on top of clear understanding. At its core, noise refers to the myriad additional processes operating in concert with the systematic structure of the data we have chosen to model. These processes are very real, and often scientifically interesting. Noise can arise from:\n\nIndividual variation (heterogeneity)\n\nEnvironmental fluctuations\n\nMeasurement error (accuracy and precision)\n\nUnmeasured drivers\n\nStochastic processes\n\nAt first glance, this list may feel extremely unsettling. It suggests that much of the world lies beyond direct and easy measurement. But to a scientist, this diversity of variation represents opportunity. Each source of variability reflects some underlying mechanism. Each pattern of deviation welcomes new explanation.\nTo be clear, noise is not something we subtract away after modeling the “real” part of the system. Like systematic structure, it has structure of its own. Different mechanisms generate different kinds of variability. Those recognizable and stereotypical patterns of variability are what we call distributions.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Models</span>"
    ]
  },
  {
    "objectID": "chapters/intro_modeling.html#data-distributions",
    "href": "chapters/intro_modeling.html#data-distributions",
    "title": "20  Introduction to Models",
    "section": "20.4 Data Distributions",
    "text": "20.4 Data Distributions\nIf variability has structure, we need a precise language to describe it. In statistics, that language is the probability distribution. So, what is a distribution?\n\nA distribution is an explicit statement about how randomness originates in the data-generating process.\n\nA data distribution specifies:\n\nWhat values are possible (constraints)\n\nHow variability scales with the mean\n\nWhether outcomes are discrete or continuous\n\nWhether extreme outcomes are rare or common\n\nThe key insight is this: when we choose a distribution, we are choosing a mechanistic description of how variability arises. That is, in the simple process of applying a distribution to a set of measurements, we are applying a mental causal model to the data, such as:\n\nCount data: We assume that variance generally scales with the mean\n\nProportion data: We acknowledge constraints on the data (bounded between 0 and 1)\n\nOnce we begin looking at distributions as mental models, it becomes clear that different distributions imply different stories about how data were generated. The Normal distribution reflects additive variation around a central tendency. Count distributions arise when outcomes reflect the accumulation of discrete events. Other distributions emerge when values are bounded, strictly positive, or shaped by multiplicative processes (such as the Gamma distribution). We are acknowledging that the features of the dataset are caused by some process or processes.\nIn what follows, I introduce several commonly used distributions and the types of data-generating processes they can represent. This is by no means an exhaustive list. As you read, focus less on their mathematical formulas and more on the assumptions they encode about randomness, constraints, and mechanism.\n\n20.4.1 Gaussian (Normal) Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nType: Continuous\nSupport: \\(-\\infty &lt; x &lt; \\infty\\)\nVariance/mean relationship:\n\nVariance is constant\n\nDoes not depend on the mean\n\n\n\n\n\n\n\nNoteEcological examples\n\n\n\n\n\n\nMeasurement error in body mass\n\nDaily deviation in stream temperature from a seasonal mean\n\n\n\n\n\n\n\n\n20.4.2 Gamma Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nType: Continuous\nSupport: \\(0 &lt; x &lt; \\infty\\)\nVariance/mean relationship:\n\nVariance proportional to μ² (square of the mean)\nVariability increases faster than mean\n\n\n\n\n\n\n\nNoteEcological examples\n\n\n\n\n\n\nBiomass accumulation\n\nTime until an event (e.g., flowering)\n\nRainfall (with a slight adjustment for zero values)\n\n\n\n\n\n\n\n\n20.4.3 Poisson Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nType: Discrete\nSupport: \\(x \\in \\{0,1,2,\\dots\\}\\) (x is an element of…)\nVariance/mean relationship:\n\nVariance equals the mean\n\nVariability increases linearly with μ (mean)\n\n\n\n\n\n\n\nNoteEcological examples\n\n\n\n\n\n\nNumber of nests in a plot\n\nNumber of insects captured per trap\n\n\n\n\n\n\n\n\n20.4.4 Bernoulli Distribution (a type of binomial distribution)\n\n\n\n\n\n\n\n\n\n\n\n\nType: Discrete\nSupport: \\(x \\in \\{0,1\\}\\), where 1 = success and 0 = failure\nVariance/mean relationship:\n\nVariance = \\(p (1-p)\\)\n\nConstrained by bounds 0 and n\n\n\n\n\n\n\n\nNoteEcological examples\n\n\n\n\n\n\nNumber of successful nests out of total attempts\n\nGermination success out of planted seeds\n\n\n\n\n\n\n\n\n20.4.5 Beta Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nType: Continuous\nSupport: \\(0 &lt; x &lt; 1\\)\nVariance/mean relationship:\n\nVariance depends on both mean and precision\n\nConstrained between 0 and 1 (but does not include 0 or 1)\n\n\n\n\n\n\n\nNoteEcological examples\n\n\n\n\n\n\nProportion of canopy cover\n\nProportion of habitat occupied\n\n\n\n\n\n\n\n\n\n20.4.6 Negative Binomial Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nType: Discrete\nSupport: \\(x \\in \\{0,1,2,\\dots\\}\\) (x is an element of…)\nVariance/mean relationship:\n\nVariance = \\(μ + μ²/k\\) (where k is a dispersion parameter)\nVariance exceeds mean\n\n\n\n\n\n\n\nNoteEcological examples\n\n\n\n\n\n\nAggregated insect counts\n\nOverdispersed species abundance\n\n\n\n\n\n\n\n\n20.4.7 Zero-Inflated Poisson Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nType: Discrete\nSupport: \\(x \\in \\{0,1,2,\\dots\\}\\) (x is an element of…)\nVariance/mean relationship:\n\nExtra zeros inflate variance\n\nVariance greater than Poisson expectation\n\n\n\n\n\n\n\nNoteEcological examples\n\n\n\n\n\n\nSpecies absent from many sampling sites\n\nTraps with structural zeros\n\n\n\n\n\n\nNext, will will introduce Generalized Linear Models so can understand their distinct advantages over other, older approaches.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Models</span>"
    ]
  },
  {
    "objectID": "chapters/intro_glms.html",
    "href": "chapters/intro_glms.html",
    "title": "21  GLM Introduction",
    "section": "",
    "text": "21.1 Separating signal from noise\nOur task as scientists is to learn how to separate the patterns that carry meaningful information (signal) from the variation that does not (noise). This is why we use all of these silly models in the first place. So, what exactly are models?",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro_glms.html#separating-signal-from-noise",
    "href": "chapters/intro_glms.html#separating-signal-from-noise",
    "title": "21  GLM Introduction",
    "section": "",
    "text": "Models are hypotheses about how the data were generated.\n\n\n21.1.1 Comparing GLMs to vintage models (ANOVAs and t-tests)\nLet us begin this introduction to our primary modeling framework (the GLM) by comparing GLMs to some familiar statistical approaches.\n\n\nHere, we provide some simulation code that will (hopefully) convince you that GLMs are pretty much the same kinds of models that you were exposed to in your introductory statistics courses back in the day. As you will quickly see, however, GLMs are highly flexible and will allow you to extract the maximum amount of signal from your noisy data.\nWe will simulate a fake dataset that has two groups (1 and 2). The values for each group follow normal distributions with a standard deviation of 1. The groups differ in their mean values, however, by one unit.\n\nset.seed(100) # this is a random seed that ensures example reproducibility\nnval &lt;- 50 # Generate some fake data\ngroup1 &lt;- rnorm(nval, mean=5, sd=1)\ngroup2 &lt;- rnorm(nval, mean=6, sd=1)\nd &lt;- data.frame(id=rep(c(\"group1\", \"group2\"), each=nval), \n  y=c(group1, group2))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, using three models (t-test, ANOVA, and GLM), let us test the hypothesis that these two groups differ. Remember that t-tests and ANOVAs are linear models that assume, among other things, that both the signal and the noise follow a Gaussian/Normal distribution. Note that we need to convert the ANOVA’s F-stat to a t-statistic for comparability. You can see the code for this multi-model comparison in this callout box:\n\n\n\n\n\n\nNoteR code for inter-model comparison\n\n\n\n\n\n\nttest.mod &lt;- t.test(y ~ id, data=d) # t-test\nanova.mod &lt;- aov(y ~ id, data=d) # ANOVA\nglm.mod &lt;- glm(y ~ id, data=d) # GLM\n\nttest.t &lt;- abs(as.numeric(ttest.mod$statistic))\nanova.f &lt;- summary(anova.mod)[[1]]$F[1]\nanova.t &lt;- sqrt(anova.f) # t-stat = sqrt(F-stat)\nglm.t &lt;- coefficients(summary(glm.mod))[2, 3]\n\nThen, we compare all the test statistics (t-values), from which the p-values are derived.\n\n\n[1] 4.119985 4.119985 4.119985\n\n\n\n\n\nYou can see that all of the t-values are exactly the same (t = 4.119985). Fantastic! Now you can sit back and silently reconsider your desire to use a t-test or ANOVA (or anything similar). Let us say that we are not abandoning t-tests and ANOVAs; instead, we are outgrowing them. The table below shows a number of other advantages that GLMs have.\n\n\n\n\n\n\n\n\n\n\nFunctionality / Capability\nt-test\nANOVA\nGLM\n\n\n\n\nUnbalanced sample\n✅\n✅\n✅\n\n\nCompare more than two groups\n\n✅\n✅\n\n\nHandle continuous predictors\n\n\n✅\n\n\nHandle non-Normal distributions\n\n\n✅\n\n\nComplex hierarchical random effects\n\n\n✅\n\n\nUse link functions (log, logit, etc.)\n\n\n✅\n\n\nPredicted values from model output\n\n\n✅\n\n\nNonlinear extensions\n\n\n✅\n\n\nUnder current development\n\n\n✅\n\n\n\n\nHopefully, this short exercise convinces you that GLMs and their extensions (Generalized Linear –and Additive– Mixed Models) no longer need to be viewed as “advanced statistics”; they can simply be used as a formal and unified language for ecological thinking.\n\n\n21.1.2 Setting the stage for GLMs\nMost textbook treatments of GLMs start off examining a model with Gaussian (normally distributed) errors. The Gaussian models have core assumptions that are likely familiar and comfortable to most of you, including:\n\nThe response variable is continuous\nThe residual variance is constant (homoscedastic)\nThe error distribution is symmetric\nThe outcome can take any real value (including negatives)\n\nIn many classroom situations, that works beautifully. If we are modeling body mass of adults (why adults?), temperature, or some other roughly symmetric continuous variable, the Gaussian/normal distribution is often appropriate. However, the Gaussian/normal distribution is often the exception rather than the rule because:\n\nMost ecological data are not normally distributed.\n\nWhy? As ecologists, we regularly collect data on proportions, rates, skewed measurements, zero-heavy processes, and, perhaps most commonly, counts like:\n\nNumber of individuals observed\n\nNumber of detections on a recorder\n\nNumber of nests in a plot\n\nNumber of disease cases\n\nFor example, consider counts of events. Count distributions –often best approximated by a Poisson distribution– have their own features, including:\n\nValues cannot be negative.\nValues are discrete (not continuous)\nThey often have variances that equal or increase with their mean\nThey are frequently right-skewed\n\nA Gaussian model, on the other hand, ignores almost everything about how most ecological data are generated. Therefore, in this section, we will first explore the Poisson GLM, which is designed specifically for count data. It:\n\nRestricts predictions to non-negative values.\nAllows variance to increase with the mean.\n\nFurthermore, using a Poisson GLM allows you to see the internal architecture of a GLM. It is a bit more difficult to see how a GLM works when using a Gaussian distribution. In a Poisson GLM, the GLM components are clearly delineated:\n\nThe error distribution describes the noise.\nThe linear predictor describes the signal.\nThe link function connects the expected value of the response to the linear predictor\n\nBecause those parts are visibly distinct, the Poisson model makes it much easier to understand what a GLM is actually doing.\n\n\n\n\n\n\nTipA reminder about the Greek alphabet\n\n\n\n\n\n\n\n\nGreek Symbol\nName\n\n\n\n\n\\(\\mu\\)\nmu\n\n\n\\(\\eta\\)\neta\n\n\n\\(\\beta\\)\nbeta\n\n\n\\(\\alpha\\)\nalpha\n\n\n\\(\\theta\\)\ntheta\n\n\n\n{style=“thead tr {background-color: #DCEEFF;}”}",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro_glms.html#the-poisson-glm-model-specification",
    "href": "chapters/intro_glms.html#the-poisson-glm-model-specification",
    "title": "21  GLM Introduction",
    "section": "21.2 The Poisson GLM: Model Specification",
    "text": "21.2 The Poisson GLM: Model Specification\nAt first glance, especially if you have had no formal mathematical training, the formal notation of a GLM can appear very intimidating. Symbols such as \\(Y_i\\), \\(\\mu_i\\), and \\(\\eta_i\\) may give the impression that the model is mathematically difficult to the average scientist. In reality, these equations simply provide an efficient way to describe simple concepts about data and model structure. Fundamentally, a relatively small set of equations can describe:\n\nHow observations in a dataset vary\nHow predictors influence expected responses\nHow effects are linked to observable data\n\nWriting the mathematical form of a model really does helps demystify it. At least, that will be my attempt in this course. Each component of how we mathematically formalize a GLM corresponds to one part of the data-generating process.\nFirst, we begin with our observations (our dataset). Using each observation \\(i\\) and sample size \\(n\\), we define our dataset as: \\(i = 1, \\dots, n\\):\nWe then set up the three core components of our GLM:\n\n21.2.1 Random Component (Noise)\nThe random component –the unmeasured structure of your model– is formalized as:\n\\(Y_i \\sim \\text{Poisson}(\\mu_i)\\), where:\n\n\\(Y_i\\): observed count for observation \\(i\\)\n\\(\\mu_i\\): expected (mean) count for observation \\(i\\)\n\nRecall that a Poisson distribution has a mean and variance that are equal: the expected count is \\(\\mu_i\\), and the variability around that expectation is also \\(\\mu_i\\). This means that as the expected number of events (counts) increases, the spread of the data increases proportionally, so larger means naturally are associated with greater variability.\n\\(E(Y_i) = \\mu_i\\) \\(\\text{Var}(Y_i) = \\mu_i\\)\nThe notation \\(Y_i \\sim \\text{Poisson}(\\mu_i)\\) simply states:\n\nThe observed count \\(Y_i\\) is a random draw from a Poisson distribution with mean \\(\\mu_i\\).\n\nThere is nothing mystical in this expression. It formalizes three important ideas:\n\nThe outcome is a count (non-negative integers only).\nThe mean and variance are equal.\nVariability scales with the expected value.\n\nBy writing the distribution explicitly, we clarify:\n\nWhat values are possible\nHow uncertainty behaves\nHow variance depends on the mean\n\nThe equation prevents hidden assumptions. Without it, readers might assume constant variance or continuous outcomes. The formal notation makes those assumptions transparent. So, I strongly encourage you to take some time and get comfortable writing out your models in this way. These points will be reiterated below in the link function section.\n\n\n\n21.2.2 Systematic Component (Signal)\n\\(\\eta_i = \\beta_0 + \\beta_1 X_i\\)\nDefinitions\n\n\\(\\eta_i\\): linear predictor\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): slope parameter\n\\(X_i\\): predictor value for observation \\(i\\)\n\nThis expression is simply a linear equation. It states:\n\nThe structured component of the model is a straight-line function of the predictor.\n\nThere is no randomness here. If \\(X_i\\) and the coefficients are known, \\(\\eta_i\\) is determined exactly. The equation clarifies:\n\nHow predictors are assumed to influence outcomes.\nThat effects are additive on the linear predictor scale.\nThat the relationship is directional and structured.\n\nWithout writing this formally, it is easy to obscure how predictors fold into the model. The mathematical form adds clarity about what is signal and what is noise.\n\n\n\n21.2.3 Link Function\nThe final component of a GLM is the link function, and its role is more conceptual than technical. It connects the structured part of the model (the signal) to the distribution that governs randomness (the noise).\nIn a GLM, the variance is defined as a function of the mean; this is general to any GLM. Recall that, in a Poisson model:\n\\(E(Y_i) = \\mu_i\\) \\(\\text{Var}(Y_i) = \\mu_i\\)\nThis tells us something very cool: once the mean \\(\\mu_i\\) is determined, the amount of variability is determined as well. The expected value and the noise structure are mathematically tied together. The link function is simply the bridge between signal and noise. It takes the output of the linear predictor (the signal) and converts it into the mean of the response distribution. Because the variance depends on that mean, the signal indirectly determines the scale of the noise.\nPut another way, we first build a straight-line equation:\n\\(\\eta_i = \\beta_0 + \\beta_1 X_i\\)\nWe then transform that linear result to obtain a valid mean:\n\\(\\mu_i = \\exp(\\eta_i)\\)\nThat exponentiation is all the link function is:\n\na rule that converts the output of the linear predictor into a proper mean for the distribution. Since the variance depends on the mean, this transformation also determines how much noise we expect.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro_glms.html#summary-the-compact-form-of-a-poisson-glm",
    "href": "chapters/intro_glms.html#summary-the-compact-form-of-a-poisson-glm",
    "title": "21  GLM Introduction",
    "section": "21.3 Summary: the compact form of a Poisson GLM",
    "text": "21.3 Summary: the compact form of a Poisson GLM\nLet us put this all together in a single structure that could be included in a scientific paper:\n\\(Y_i \\sim \\text{Poisson}(\\mu_i)\\)\n\\(\\log(\\mu_i) = \\beta_0 + \\beta_1 X_i\\)\nThis compact form contains the entire model. When broken apart, it describes:\n\nThe distribution of variability (the Poisson portion)\nThe structured influence of predictors (the line equation)\nThe transformation connecting structure to expectation (the log operator)\n\nThe mathematical notation is far more concise than what we sometimes see in the articles’ descriptions of statistical analyses. The concise form communicates all of the components of the model in a straightforward way. The value of writing them formally is not useless abstraction; it is clarity about how our decisions are translated into strong inference.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro_glms.html#testing-yourself-thinking-about-signal-and-noise-in-a-totally-real-dataset",
    "href": "chapters/intro_glms.html#testing-yourself-thinking-about-signal-and-noise-in-a-totally-real-dataset",
    "title": "21  GLM Introduction",
    "section": "21.4 Testing yourself: thinking about signal and noise in a totally real dataset",
    "text": "21.4 Testing yourself: thinking about signal and noise in a totally real dataset\nFor this conceptual self-test, we will use a dataset that was collected from a population of Lepusantilocapra pufferi (puffer jackalope) starting in the mid-1970s.\nYou will need the following file (simply click to save):\n\nlepusantilocapra_pufferi_abundance.csv\n\nThe data include the following variables:\n\nsite: where the individuals were sampled\nabund: number of individuals detected at each location\ndens: density of individuals at each sampling location\nmean_depth: mean depth, in meters of the sampling location\nyear: year of the study\nperiod: 1=before and 2= after introduction of Pelicanus pufferivorus (puffer-jackalope pelican)\nx_km: longitude (decimal degrees) of sampling location\ny_km: latitude (decimal degrees) of sampling location\nswept_area: size in square meters of the sampling location (effort)\n\nFamiliarize yourself with these data (especially mean_depth, dens, and abund) and then think about the following questions. Click to expand each box for the answer.\n\n\n\n\n\n\nTipIf mean depth is a true driver, what happens when we exclude it from the model?\n\n\n\n\n\nAnswer:\nIts effect does not disappear from the world. The effect is moved into the noise (residual). It may also distort the effects of any correlated covariates (but we will discuss this more later).\n\n\n\n\n\n\n\n\n\n\nTipIf residual variance drops after adding mean depth, what changed?\n\n\n\n\n\nAnswer:\nWe improved our model’s alignment with the data-generating process (which included mean depth). Nature did not become less noisy.\n\n\n\n\n\n\n\n\n\n\nTipIs residual variance (noise) a feature of the system/world or our model?\n\n\n\n\n\nAnswer:\nThis is sort of a trick question. Residual variance most proximately reflects what the model itself fails to explain; it is simply a mathematical way that we are making sense of the world.\n\n\n\n\n\n\n\n\n\n\nTipIf mean depth causally affects abundance and is omitted from the model, is that just extra noise?\n\n\n\n\n\nAnswer:\nNo. This means that the model has been misspecified. It does not only increase residual variance; it also creates systematic bias.\n\n\n\n\n\n\n\n\n\n\nTipWhen mean depth moves from residual to systematic, are we changing the ecology?\n\n\n\n\n\nAnswer:\nNo. The ecology stays the same. We are changing our description of it by moving the explained variation from residual (unexplained) to systematic (explained).",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_glms.html",
    "href": "chapters/tutorial_glms.html",
    "title": "22  Tutorial: GLMs",
    "section": "",
    "text": "22.1 Introduction\nFor this first example, we will use the dataset that you were briefly introduced to in the previous section. The dataset was collected from a population of Lepusantilocapra pufferi (puffer jackalope) starting in the mid-1970s.\nYou will need the following file (simply click to save):\nAs a reminder, the data include the following variables:\nYou are tasked with examining/testing the relationship between total Lepusantilocapra pufferi abundance and mean depth for each time period (before/after introduction of puffer-jackalope pelican). Note that this example purposefully starts with a simple –and inaccurate– model to illustrate the process of building a better, more explanatory model.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Tutorial: GLMs</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_glms.html#introduction",
    "href": "chapters/tutorial_glms.html#introduction",
    "title": "22  Tutorial: GLMs",
    "section": "",
    "text": "In this tutorial, we introduce Generalized Linear Models (GLMs), starting with how to construct a Poisson GLM. Every type of GLM –and all of their extensions like GLMM and GAMMs– separates:\n\nSignal: linear predictor (why the mean changes)\n\nNoise: distribution (how observations vary around that mean)\nLink: how the structured signal (the mean) maps to the response variable\n\nThis separation is the real power of the GLM framework: signal and noise are model-dependent, and what is treated as systematic structure in one formulation may be treated as variability (noise) in another. It provides flexibility to test a myriad ecological data-generating hypotheses.\n\n\n\n\n\n\n\nlepusantilocapra_pufferi_abundance.csv\n\n\n\nsite: where the individuals were sampled\nabund: number of individuals detected at each location\ndens: density of individuals at each sampling location\nmean_depth: mean depth, in meters of the sampling location\nyear: year of the study\nperiod: 1=before and 2= after introduction of Pelicanus pufferivorus (puffer-jackalope pelican)\nx_km: longitude (decimal degrees) of sampling location\ny_km: latitude (decimal degrees) of sampling location\nswept_area: size in square meters of the sampling location (effort)",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Tutorial: GLMs</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_glms.html#set-up-workspace",
    "href": "chapters/tutorial_glms.html#set-up-workspace",
    "title": "22  Tutorial: GLMs",
    "section": "22.2 Set up workspace",
    "text": "22.2 Set up workspace\n\nsource(\"class_r_scripts/load_packages.r\")",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Tutorial: GLMs</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_glms.html#generalized-linear-model-gaussian-error-distribution",
    "href": "chapters/tutorial_glms.html#generalized-linear-model-gaussian-error-distribution",
    "title": "22  Tutorial: GLMs",
    "section": "22.3 Generalized Linear Model (Gaussian error distribution)",
    "text": "22.3 Generalized Linear Model (Gaussian error distribution)\nRead in the Lorax pufferi dataset and examine its structure. Remember that you can also use the glimpse function here.\n\npufferi &lt;- read.csv(\"class_exercise_data/lepusantilocapra_pufferi_abundance.csv\")\nstr(pufferi) # examine data structure\n\n'data.frame':   146 obs. of  9 variables:\n $ site      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ abund     : int  76 161 39 410 177 695 352 674 624 736 ...\n $ dens      : num  0.00207 0.00352 0.000981 0.008039 0.005933 ...\n $ mean_depth: int  804 808 809 848 853 960 977 982 985 986 ...\n $ year      : int  1978 2001 2001 1979 2002 1980 1981 1979 1982 1980 ...\n $ period    : int  1 2 2 1 2 1 1 1 1 1 ...\n $ x_km      : num  98.8 76.8 103.8 91.5 107.1 ...\n $ y_km      : num  -57.5 178.6 -50.1 146.4 -37.1 ...\n $ swept_area: num  36710 45741 39775 51000 29831 ...\n\n\nNote: Though the readr::read_csv function is commonly used, the initial warning messages upon import may throw off users who are unfamiliar with what it is doing. The function does provide quite a bit more flexibility in defining column types, but we do not need it for this example. So, here, we are using the base R function read.csv. You should feel free to use any data import function that you prefer.\nLet us explore the data a bit to see what we are up against. Obviously, a formal exploratory data analysis requires quite a bit more detailed examination, but this should serve us well right now.\n\np &lt;- ggplot2::ggplot(pufferi, aes(x = mean_depth, y = abund)) +\n  geom_point(color = \"black\", size = 3) +\n  facet_wrap(~ period, ncol = 1, nrow = 2) +\n  labs(x = \"mean depth (km)\", y = \"number of _L. pufferi_\") +\n  theme_bw()\np\n\n\n\n\n\n\n\n\nFrom this two-panel scatterplot, we observe (but do not yet find statistical support for) a general decline in L. pufferi abundance with mean depth. Of course, our desire to see a pattern might be influenced by a general decrease in variation in the number of L. pufferi as mean depth increases. So, let us change this informal notion about what the data show to something more scientific. Specifically, we need a framework that separates effectively separates the signal from the underlying noise.\nLet us develop our first GLM. There are a variety of packages and functions for GLMs and their more complicated and powerful extensions. In this course, we will try to use the packages/functions that (1) have similar model syntax (even though used in Bayesian approaches), and (2) are used in the majority of published work of the last few years.\nLet us run a GLM without considering the distribution of our response variable. We use the mass::glm function, a versatile model function for non-clustered data (i.e. data that has no hierarchical structure like multiple samples collected for each individual, or multiple students from multiple schools from multiple school districts).\n\nbase_model &lt;- glm(formula = abund ~ mean_depth, data = pufferi)\n\nThis model structure will be very similar across all the modeling functions we discuss in the course. This is a very standard formula structure in R. This structure simply means tht abundance is modeled as a function of mean_depth. That is, the response variable (abund) is on the left side of the tilde (~) and the predictors are on the right side of the tilde.\nLet us view the summary output of this model in two ways: First, use the summary function to review the summary of the model object you just created (base_model). For many model objects in R, the summary function yields the same result.\n\nsummary(base_model) \n\n\nCall:\nglm(formula = abund ~ mean_depth, data = pufferi)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 451.39068   33.28310  13.562  &lt; 2e-16 ***\nmean_depth   -0.09758    0.01232  -7.918 5.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 34192.14)\n\n    Null deviance: 7067483  on 145  degrees of freedom\nResidual deviance: 4923668  on 144  degrees of freedom\nAIC: 1942.5\n\nNumber of Fisher Scoring iterations: 2\n\n\nResults in this table can sometimes be a bit daunting if you are not familiar with regression. A relatively newcomer to the R package scene is the broom package. You can use the broom package to make the model output more human-friendly. This also saves you –to some extent– from learning how to extract specific output components from model objects from different packages. And the broom functions will certainly save you a huge amount of time outputting model results, residuals, and predicted values.\nFirst, just examine the effect sizes.\n\n# Tidy model coefficients\ntidy_glm &lt;- tidy(base_model, conf.int = TRUE)\nprint(tidy_glm)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 451.       33.3        13.6  1.63e-27  386.     517.    \n2 mean_depth   -0.0976    0.0123     -7.92 5.92e-13   -0.122   -0.0734\n\n\nThen, you can check out the model fit. This is just for your information; we will do this in depth when we discuss how to report your results. But it is avdantageous to build some strong habits now! And I also want to give you some tools, so you can start adding them to your ever-improving analytical workflow!\n\n# Glance at model-level statistics\nglance_glm &lt;- glance(base_model)\nprint(glance_glm)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1      7067483.     145  -968. 1943. 1951. 4923668.         144   146\n\n\nNext, we can calculate model residuals and then add these values to (i.e. “augment”) our original dataset.\n\n# Augment the original data with predictions and residuals\naugment_glm &lt;- augment(base_model)\nprint(augment_glm)\n\n# A tibble: 146 × 8\n   abund mean_depth .fitted  .resid   .hat .sigma    .cooksd .std.resid\n   &lt;int&gt;      &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1    76        804    373. -297.   0.0181   184. 0.0243        -1.62  \n 2   161        808    373. -212.   0.0181   185. 0.0123        -1.15  \n 3    39        809    372. -333.   0.0181   183. 0.0305        -1.82  \n 4   410        848    369.   41.4  0.0175   186. 0.000454       0.226 \n 5   177        853    368. -191.   0.0175   185. 0.00966       -1.04  \n 6   695        960    358.  337.   0.0160   183. 0.0276         1.84  \n 7   352        977    356.   -4.05 0.0158   186. 0.00000393    -0.0221\n 8   674        982    356.  318.   0.0158   184. 0.0241         1.74  \n 9   624        985    355.  269.   0.0157   184. 0.0171         1.46  \n10   736        986    355.  381.   0.0157   183. 0.0344         2.08  \n# ℹ 136 more rows\n\n\nThe results (from summary() or broom::tidy functions) show a significant effect of mean_death. As an aside, it might not be the best to use the term effect, as this technically implies a causal structure. But it is common usage, so we will stick to it. Just be cautious of using wording that does not match with your hypotheses and predictions.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Tutorial: GLMs</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_glms.html#generalized-linear-model-poisson-error-distribution",
    "href": "chapters/tutorial_glms.html#generalized-linear-model-poisson-error-distribution",
    "title": "22  Tutorial: GLMs",
    "section": "23.1 Generalized Linear Model (Poisson error distribution)",
    "text": "23.1 Generalized Linear Model (Poisson error distribution)\nLooking more closely at the data, we realize that our variable for abundance is simple count data (integer). Make a histogram:\n\nhist(pufferi$abund, breaks=100)\n\n\n\n\n\n\n\n\nThe distribution also ranges from zero to infinity (right-skewed). So, let us try a Poisson error distribution with the default link function; this is often a good first guess for count data.\n\nbase_model_pois &lt;- glm(abund ~ mean_depth, \n  data = pufferi, \n  family = poisson(link = \"log\"))\n\nHere we are using the log link function, the preferred one for the Poisson distribution. Now, check your model residuals:\n\nsim_res &lt;- simulateResiduals(base_model_pois)\n\nWe can then plot this simulation:\n\nplot(sim_res)\n\n\n\n\n\n\n\n\nUh, oh. You know that you’ve correctly identified a distribution that fits your data boundaries. But, clearly, there are still patterns in these residuals that should not be present. So there are other issues going on as well. Perhaps the issues are related to model misspecification. Believe it or not, despite the horrific simulated residuals, you are now one step closer to building a successful GLM.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Tutorial: GLMs</span>"
    ]
  },
  {
    "objectID": "chapters/tutorial_glms.html#section",
    "href": "chapters/tutorial_glms.html#section",
    "title": "22  Tutorial: GLMs",
    "section": "24.1 ",
    "text": "24.1 \nThe problems with residuals remain. This means that this really is a case of model misspecification, so let us think about what other factors we should include. Note that the model names could possibly be improved. As you review the models, think critically about each one means. In all models, we include the offset variable.\n\nmod_nb_offset_01 &lt;- glm.nb(abund ~ offset(log_sa), \n  data = pufferi)\nmod_nb_offset_02 &lt;- glm.nb(abund ~ mean_depth + offset(log_sa), \n  data = pufferi)\nmod_nb_offset_03 &lt;- glm.nb(abund ~ period + offset(log_sa), \n  data = pufferi)\nmod_nb_offset_04 &lt;- glm.nb(abund ~ year + offset(log_sa), \n  data = pufferi)\nmod_nb_offset_05 &lt;- glm.nb(abund ~\n  mean_depth + period + offset(log_sa), \n  data = pufferi)\nmod_nb_offset_06 &lt;- glm.nb(abund ~\n  mean_depth + year + offset(log_sa), \n  data = pufferi)\nmod_nb_offset_07 &lt;- glm.nb(abund ~\n  period + year + offset(log_sa), \n  data = pufferi)\nmod_nb_offset_08 &lt;- glm.nb(abund ~\n  mean_depth * period + year + offset(log_sa), \n  data = pufferi)\nmod_nb_offset_09 &lt;- glm.nb(abund ~\n  mean_depth * year + period + offset(log_sa), \n  data = pufferi)\nmod_nb_offset_10 &lt;- glm.nb(abund ~\n  mean_depth + year * period + offset(log_sa), \n  data = pufferi)\n\nNote that these models test all two-way combinations (including interactions). A sort of null hypothesis —although some would disagree philosophically– is the first model (mod_nb_offset_01), as it contains only the offset variable. Let us calculate AIC and then sort the output in ascending order of AIC (where lowest AIC is the best-supported model). Note that this is bonus material right now.\n\naic_df &lt;- AIC(\n  mod_nb_offset_01,\n  mod_nb_offset_02,\n  mod_nb_offset_03,\n  mod_nb_offset_04,\n  mod_nb_offset_05,\n  mod_nb_offset_06,\n  mod_nb_offset_07,\n  mod_nb_offset_08,\n  mod_nb_offset_09,\n  mod_nb_offset_10\n  )\naic_df &lt;- aic_df |&gt;\n  arrange(AIC)\n\nLet us check those residuals of the top model. There are better ways of doing this, but let us be simple for now.\n\nsim_res &lt;- simulateResiduals(mod_nb_offset_10)\nplot(sim_res)\n\n\n\n\n\n\n\n\nThese look great so far, but let’s check the residuals against the predictors in the model.\n\nplotResiduals(sim_res, pufferi$mean_depth)\n\nqu = 0.75, log(sigma) = -2.701421 : outer Newton did not converge fully.\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L,\n: Fitting terminated with step failure - check results carefully\n\n\n\n\n\n\n\n\n\nThis looks great!\n\nplotResiduals(sim_res, pufferi$period)\n\n\n\n\n\n\n\n\nOK, we’re starting to get the picture here that we’ve done something up.\n\nplotResiduals(sim_res, pufferi$year)\n\n\n\n\n\n\n\n\nThis was the final check. We’ve specified the model correctly, and we have confirmed that there is no residual structure.\nWe have successfully formulated a Generalized Linear Model!",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Tutorial: GLMs</span>"
    ]
  },
  {
    "objectID": "chapters/glm_partial_association.html",
    "href": "chapters/glm_partial_association.html",
    "title": "23  Tutorial: Partial Associations",
    "section": "",
    "text": "23.1 How a GLM works (for partial associations)",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Tutorial: Partial Associations</span>"
    ]
  },
  {
    "objectID": "chapters/glm_partial_association.html#how-a-glm-works-for-partial-associations",
    "href": "chapters/glm_partial_association.html#how-a-glm-works-for-partial-associations",
    "title": "23  Tutorial: Partial Associations",
    "section": "",
    "text": "A key strength of a GLM is that it can estimate the association between a predictor and an outcome while simultaneously accounting for other variables in the model; that is, it can quantify partial associations rather than just simple bivariate relationships. It helps to remember that when a model includes multiple predictors, its coefficients describe partial associations, which are defined with respect to:\n\nhow variation in one predictor relates to the response after accounting for other predictors in the model\n\nAlthough R (and other great software) estimates these quantities automatically (i.e. behind-the-scenes), it can be helpful to unpack what “controlling for” actually means. This can help demystify the GLM (or even general linear regression) framework. In this brief tutorial, we strip the GLM down and explain how partial associations are calculated by the model.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Tutorial: Partial Associations</span>"
    ]
  },
  {
    "objectID": "chapters/glm_partial_association.html#residualization",
    "href": "chapters/glm_partial_association.html#residualization",
    "title": "23  Tutorial: Partial Associations",
    "section": "23.2 Residualization",
    "text": "23.2 Residualization\nOne intuitive way to do this is through residualization. By successively removing the variation explained by a covariate, we can isolate any remaining structure in the data (using residuals) and then examine how that relates to the response variable. The sequence below reproduces –almost exactly– the coefficient for a single predictor in a GLM.\n\n\n\n\n\n\nNoteFrisch–Waugh–Lovell Theorem (Residualization)\n\n\n\nIn a multiple regression, the coefficient of a predictor \\(X_i\\) equals the slope obtained by regressing the residuals of \\(Y\\) (after removing all other predictors) on the residuals of \\(X_i\\) (after removing those same predictors).",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Tutorial: Partial Associations</span>"
    ]
  },
  {
    "objectID": "chapters/glm_partial_association.html#residualization-two-model-predictors",
    "href": "chapters/glm_partial_association.html#residualization-two-model-predictors",
    "title": "23  Tutorial: Partial Associations",
    "section": "23.3 Residualization: Two model predictors",
    "text": "23.3 Residualization: Two model predictors\nFirst, we begin with the previously examined dataset of Lepusantilocapra pufferi (puffer jackalope). Again, you will need to set up with R space, including loading R packages and reading in the example dataset.\n\nsource(\"class_r_scripts/load_packages.r\")\n\nAnd you will need the following file (simply click to save):\n\nlepusantilocapra_pufferi_abundance.csv\n\nRead these data into R:\n\npufferi &lt;- read.csv(\"class_exercise_data/lepusantilocapra_pufferi_abundance.csv\")\nstr(pufferi) # examine data structure\n\n'data.frame':   146 obs. of  9 variables:\n $ site      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ abund     : int  76 161 39 410 177 695 352 674 624 736 ...\n $ dens      : num  0.00207 0.00352 0.000981 0.008039 0.005933 ...\n $ mean_depth: int  804 808 809 848 853 960 977 982 985 986 ...\n $ year      : int  1978 2001 2001 1979 2002 1980 1981 1979 1982 1980 ...\n $ period    : int  1 2 2 1 2 1 1 1 1 1 ...\n $ x_km      : num  98.8 76.8 103.8 91.5 107.1 ...\n $ y_km      : num  -57.5 178.6 -50.1 146.4 -37.1 ...\n $ swept_area: num  36710 45741 39775 51000 29831 ...\n\n\nWe will adopt a model good enough for a short example (even though we established that this is a poorly specified model):\nThe goal of this exercise is to estimate the partial association between mean_depth and abund. That is, we are trying to estimate the effect of mean_depth on abund after accounting for dens.\nThere are three steps for this example. To go through the steps, click on each of the callout boxes below to work through the process of estimating partial effects in a GLM. For the sake of clarity, I only include two covariates. Also note that, for the sake of clarity, I use a lengthy object name that does not use snake_case.\n\n\n\n\n\n\nNoteStep 1: Remove the effect of dens from mean_depth\n\n\n\n\n\nFirst, regress mean_depth on dens (the two predictors, in this case) and calculate model residuals.\n\nmod_DEPTH_on_DENS &lt;- glm(mean_depth ~ dens, data = pufferi)\ndepth_resid &lt;- resid(mod_DEPTH_on_DENS)\n\nThe residuals from this model represent the component of mean_depth that cannot be explained by dens. This answers the question: How does mean_depth vary once dens has been held constant?\n\n\n\n\n\n\n\n\n\nNoteStep 2: Remove the effect of dens from abund\n\n\n\n\n\nNext, regress abund on dens and retain the residuals.\n\nmod_ABUND_on_DENS &lt;- glm(abund ~ dens, data = pufferi)\nabund_resid &lt;- resid(mod_ABUND_on_DENS)\n\nThese residuals represent variation in abund that is not associated with dens. By now, you should see the general pattern of this residualization approach.\n\n\n\n\n\n\n\n\n\nNoteStep 3: Estimate the partial association\n\n\n\n\n\nFinally, regress the residuals of abund (Step 2) onto the residuals of mean_depth (Step 1).\nYou can run the summary() function to view the model output. The slope from this regression quantifies the association between mean_depth and abund after dens has been accounted for. This value will be very close to the coefficient for mean_depth in the full multivariable model:\n\n\n\nCall:\nglm(formula = abund ~ mean_depth + dens, data = pufferi)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.858e+01  1.926e+01   2.003   0.0471 *  \nmean_depth  -3.730e-04  5.833e-03  -0.064   0.9491    \ndens         3.787e+04  1.318e+03  28.735   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5082.682)\n\n    Null deviance: 7067483  on 145  degrees of freedom\nResidual deviance:  726824  on 143  degrees of freedom\nAIC: 1665.2\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nCall:\nglm(formula = abund_resid ~ depth_resid, data = pufferi)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  1.009e-13  5.880e+00   0.000    1.000\ndepth_resid -3.730e-04  5.812e-03  -0.064    0.949\n\n(Dispersion parameter for gaussian family taken to be 5047.386)\n\n    Null deviance: 726844  on 145  degrees of freedom\nResidual deviance: 726824  on 144  degrees of freedom\nAIC: 1663.2\n\nNumber of Fisher Scoring iterations: 2\n\n\nIndeed, you see that the effect size is -3.730e-04 in both cases. So, we have successful reconstructed how GLMs can examine partial associations between predictors and outcome variables.\n\n\n\n\n\n\n\n\n\nNoteRepeat this logic for dens\n\n\n\nYou could do the same thing for dens and find its partial association with abund after controlling for mean_depth.\n\n# residual of focal, controlling for non-focal\nmod_DENS_on_DEPTH &lt;- glm(dens ~ mean_depth, data = pufferi)\ndens_resid &lt;- resid(mod_DENS_on_DEPTH)\n\n# residual of effect of non-focal predictor\nmod_ABUND_on_DEPTH &lt;- glm(abund ~ mean_depth, data = pufferi)\nabund_resid &lt;- resid(mod_ABUND_on_DEPTH)\n\n# overall residual effect\nmod_resid_dens &lt;- glm(abund_resid ~ dens_resid, data = pufferi)\n\n\n\n\n23.3.1 Plotting all the residuals\n\n\n\nggplot(data.frame(dens_resid = dens_resid,\n                  abund_resid = abund_resid),\n       aes(x = dens_resid, y = abund_resid)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(\n    x = \"residuals of `dens`\",\n    y = \"residuals of `abund`\",\n    title = \"partial association: `dens` on `abund`\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nggplot(data.frame(depth_resid = depth_resid,\n                  abund_resid = abund_resid),\n       aes(x = depth_resid, y = abund_resid)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(\n    x = \"residuals of `mean_depth`\",\n    y = \"residuals of `abund`\",\n    title = \"partial association: `mean_depth` on `abund`\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nExamine the summary of modbase to compare the coefficients for mean_depth and dens to the variation in each of these plots. You should immediately notice the one with less residual structure (more clustered around the regression line) has the larger effect size.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Tutorial: Partial Associations</span>"
    ]
  },
  {
    "objectID": "chapters/glm_partial_association.html#residualization-two-model-predictors-1",
    "href": "chapters/glm_partial_association.html#residualization-two-model-predictors-1",
    "title": "23  Tutorial: Partial Associations",
    "section": "23.4 Residualization: Two model predictors",
    "text": "23.4 Residualization: Two model predictors\nThe above example clearly illustrates how residualization works for a model with two predictors. But what about a model with &gt;2 predictors? We will simply add one of the other potential predictors (period) to our base model from above. Again, this is a rather nonsensical formulation:\n\n\n\nCall:\nglm(formula = abund ~ mean_depth + dens, data = pufferi)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.858e+01  1.926e+01   2.003   0.0471 *  \nmean_depth  -3.730e-04  5.833e-03  -0.064   0.9491    \ndens         3.787e+04  1.318e+03  28.735   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5082.682)\n\n    Null deviance: 7067483  on 145  degrees of freedom\nResidual deviance:  726824  on 143  degrees of freedom\nAIC: 1665.2\n\nNumber of Fisher Scoring iterations: 2\n\n\nLet us go through the three steps again.\n\n\n\n\n\n\nNoteStep 1: Remove the effect of dens from mean_depth\n\n\n\n\n\nFirst, regress mean_depth on dens (the two predictors, in this case) and calculate model residuals.\n\nmod_DEPTH_on_DENS &lt;- glm(mean_depth ~ dens, data = pufferi)\ndepth_resid &lt;- resid(mod_DEPTH_on_DENS)\n\nThe residuals from this model represent the component of mean_depth that cannot be explained by dens. This answers the question: How does mean_depth vary once dens has been held constant?\n\n\n\n\n\n\n\n\n\nNoteStep 2: Remove the effect of dens from abund\n\n\n\n\n\nNext, regress abund on dens and retain the residuals.\n\nmod_ABUND_on_DENS &lt;- glm(abund ~ dens, data = pufferi)\nabund_resid &lt;- resid(mod_ABUND_on_DENS)\n\nThese residuals represent variation in abund that is not associated with dens. By now, you should see the general pattern of this residualization approach.\n\n\n\n\n\n\n\n\n\nNoteStep 3: Estimate the partial association\n\n\n\n\n\nFinally, regress the residuals of abund (Step 2) onto the residuals of mean_depth (Step 1).\nYou can run the summary() function to view the model output. The slope from this regression quantifies the association between mean_depth and abund after dens has been accounted for. This value will be very close to the coefficient for mean_depth in the full multivariable model:\n\n\n\nCall:\nglm(formula = abund ~ mean_depth + dens, data = pufferi)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.858e+01  1.926e+01   2.003   0.0471 *  \nmean_depth  -3.730e-04  5.833e-03  -0.064   0.9491    \ndens         3.787e+04  1.318e+03  28.735   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5082.682)\n\n    Null deviance: 7067483  on 145  degrees of freedom\nResidual deviance:  726824  on 143  degrees of freedom\nAIC: 1665.2\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nCall:\nglm(formula = abund_resid ~ depth_resid, data = pufferi)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  1.009e-13  5.880e+00   0.000    1.000\ndepth_resid -3.730e-04  5.812e-03  -0.064    0.949\n\n(Dispersion parameter for gaussian family taken to be 5047.386)\n\n    Null deviance: 726844  on 145  degrees of freedom\nResidual deviance: 726824  on 144  degrees of freedom\nAIC: 1663.2\n\nNumber of Fisher Scoring iterations: 2\n\n\nIndeed, you see that the effect size is -3.730e-04 in both cases. So, we have successful reconstructed how GLMs can examine partial associations between predictors and outcome variables.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Tutorial: Partial Associations</span>"
    ]
  },
  {
    "objectID": "chapters/glm_partial_association.html#residualization-more-than-two-model-predictors",
    "href": "chapters/glm_partial_association.html#residualization-more-than-two-model-predictors",
    "title": "23  Tutorial: Partial Associations",
    "section": "23.5 Residualization: More than two model predictors",
    "text": "23.5 Residualization: More than two model predictors\nFor more than two model predictors, the number of steps increases. But, arguably, the complexity does not. The logic is still the same:\n\nA predictor’s coefficient equals the relationship between the parts of the outcome and that predictor that remain after removing the influence of all other variables (residuals!)\n\nHere is a new model with a third predictor (period) added:\n\nmod_full &lt;- glm(abund ~ mean_depth + dens + period, data = pufferi)\nsummary(mod_full)\n\n\nCall:\nglm(formula = abund ~ mean_depth + dens + period, data = pufferi)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.345e+01  2.821e+01   1.895   0.0601 .  \nmean_depth  -9.281e-04  5.893e-03  -0.158   0.8751    \ndens         3.764e+04  1.359e+03  27.702   &lt;2e-16 ***\nperiod      -9.265e+00  1.282e+01  -0.723   0.4710    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5099.716)\n\n    Null deviance: 7067483  on 145  degrees of freedom\nResidual deviance:  724160  on 142  degrees of freedom\nAIC: 1666.7\n\nNumber of Fisher Scoring iterations: 2\n\n\nBelow is the way in which we would calculate –by residualization– the partial association coefficient between one predictor (mean_depth) and abund (controlling for dens and period). You can apply the same process to the other two predictors. Click on each callout box to expand.\n\n\n\n\n\n\nNoteStep 1: Remove the effect of dens and period from mean_depth\n\n\n\n\n\n\nmod_DEPTH_on_OTHERS &lt;- glm(mean_depth ~ dens + period, data = pufferi)\ndepth_resid &lt;- resid(mod_DEPTH_on_OTHERS)\n\n\n\n\n\n\n\n\n\n\nNoteStep 2: Remove dens and period from abund\n\n\n\n\n\n\nmod_ABUND_on_OTHERS &lt;- glm(abund ~ dens + period, data = pufferi)\nabund_resid_depth &lt;- resid(mod_ABUND_on_OTHERS)\n\n\n\n\n\n\n\n\n\n\nNoteStep 3: Estimate the partial association\n\n\n\n\n\nFor this step, we regress the residuals of abund (controlling for the non-focal predictors; Step 2) onto the residuals of mean_depth (controlling for the non-focal predictors; Step 1).\n\nmod_partial_depth &lt;- glm(abund_resid_depth ~ depth_resid)\nsummary(mod_partial_depth)\n\n\nCall:\nglm(formula = abund_resid_depth ~ depth_resid)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  1.046e-13  5.869e+00   0.000    1.000\ndepth_resid -9.281e-04  5.852e-03  -0.159    0.874\n\n(Dispersion parameter for gaussian family taken to be 5028.886)\n\n    Null deviance: 724286  on 145  degrees of freedom\nResidual deviance: 724160  on 144  degrees of freedom\nAIC: 1662.7\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe slope equals the mean_depth coefficient in mod_full (-9.281e-04).",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Tutorial: Partial Associations</span>"
    ]
  },
  {
    "objectID": "chapters/glm_partial_association.html#summary",
    "href": "chapters/glm_partial_association.html#summary",
    "title": "23  Tutorial: Partial Associations",
    "section": "23.6 Summary",
    "text": "23.6 Summary\nThe goal of this tutorial was to demystify the mathematics of regression so that it feels like a transparent tool rather than a black box. We showed that a partial coefficient is simply the relationship between the leftover variation in a predictor and the leftover variation in the response after accounting for other variables. It really is that simple.",
    "crumbs": [
      "Part III: Columns to Curves",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Tutorial: Partial Associations</span>"
    ]
  }
]